{"title":"Github copilot for R","markdown":{"headingText":"Github copilot for R","containsRefs":false,"markdown":"\n**Time:** 11:30-12:00pm\n\nI'll show you how you can most effectively use github copilot to plan, code and write up your data analysis and modelling.\n\n**Software requirements:** VScode with R and github copilot license + extension for copilot.\n\nGithub Copilot calls itself an 'AI programming assistant' or an 'AI pair programmer'. I'll refer to it as an 'LLM coding assistant' or just 'Assistant'.\n\nAssistants add a layer of software between you and the LLM. The software is doing some hidden interpretation of what you want to do, as well as trying to save costs. For instance, for most assistants we often don't get to control (or even see) the system message, the temperature or the number of output tokens. The assistant is also guessing context to include in the prompt, so it can automatically give the LLM more context. At the same time it is managing the LLM's context window and trying to save on costs. \n\nThere is no generic name for this type of software (the field is moving to fast to have standardized names). So I'll refer to them Assistants. In this bucket I'll also put chatGPT, Claude, Roo Code, Cline and others. Note that Github Copilot (which I'll call copilot for short) is different to the 'Copilot' assistant that is on the web and in the Teams app. \n\nThis software is also called 'chatbots', however, I prefer assistants as the tasks they can do are much broader than just chatting. \n\n::: {.tip}\n**Tip:** You'll get the most of out Github Copilot if you use Visual Studio Code as your development environment (rather than RStudio). Setting up VScode with R can be a bit fiddly, check out my [my installation instructions](https://www.seascapemodels.org/rstats/2025/02/07/setting-up-vscode-r-cline.html) if you have trouble. Web searching advice is also a good idea if you are stuck. Its worth the effort. \n:::\n\nCopilot It is developing rapidly, so it is quite likely that when you read this there will be changes and new features. \n\nIn this section I'll focus on showing the main ways you can use copilot. Just be aware the implementation may change in future. \n\nWe'll look at: \n\n- Overview VScode for those that are new to this software\n- Best practices for setting up your project directory \n- Inline code editing\n- Ask mode\n- Edit mode\n- Agent mode \n\n## Inline code editing\n\nThis chapter explores techniques for using GitHub Copilot's inline code editing capabilities to enhance your R programming workflow. \n\n### 1. Code completion\n\nThis is only option supported in Rstudio (last time I checked). \n\nAssuming you have github copilot set-up you just need to start a new R script (remember to keep it organized and give it a useful name) and start typing. You'll see suggested code completions appear in grey. Hit `tab` to complete them. \n\nLet's read in the benthic site data and fish counts: \n\n```{r, eval=FALSE}\nlibrary(tidyverse)\nlibrary(readr)\n\ndat <- read_csv(url(\"https://raw.githubusercontent.com/cbrown5/R-llm-workshop/refs/heads/main/resources/fish-coral-cover-sites.csv\"))\n\nhead(dat)\nsummary(dat)\n```\n\nNow try create a ggplot of `secchi` (a measure of water clarity, higher values mean clearer water) and `pres.topa` (count of topa, the bumphead parrotfish). Start typing `gg` and see what happens. \n\nYou should a recommendation for a ggplot. But it won't know the variable names. \n\n::: {.tip}\n**Tip:** Sometimes GC gets stuck in a loop and keeps recommending the same line. To break it out of the loop try typing something new.  \n:::\n\n### 2. Using comments \n\nThe code completion is using your script and all open scripts in VScode to predict your next line of code. It won't know the variable names unless you've provided that. One way is to include them in the readme.md file and have that open, another is to use comments in the active script (which tends to work more reliably), e.g.\n\n```\n# Make a point plot of secchi against pres.topa\ngg...\n```\n\nShould get you the write ggplot. Using variable names in your prompts is more precise and will help the LLM guess the right names. \n\nYou could also try putting key variable names in comments at the top of your script. \n\nAnother way to use autocomplete is not to write R at all, just to write comments and fix the R code. Try templating a series of plots like:\n\n```\n# Make a point plot of secchi against pres.topa with a stat_smooth\n\n# Plot logged (two categories) and pres.topa as a boxplot\n\n# Plot CB_cover (branching coral cover) against secchi\n```\n\nNow go back through and click under each line to get the suggestions. \n\nThis strategy is great in data wrangling workflows. As a simple example try make this grouped summary using comments only: \n\n```{r, eval=FALSE}\ndat %>%\n    group_by(logged) %>%\n    summarize(mean_topa = mean(pres.topa), \n                mean_CB = mean(CB_cover))\n```\n\nTo make this I might write this series of comments: \n\n```\n    #group dat by logged \n    #summarize pres.topa and CB_cover\n```\n\nIf the variable names are documented above you can ofter be lazier and less precise with variable names here. \n\n### 3. Code completion settings\n\nClick the octocat in the bottom right corner of VScode to fine-tune the settings. You can enable/disable code completions (sometimes they are annoying e.g. when writing a workshop!). \n\nYou can also enable 'next edit suggestions'. These are useful if editing an exisiting file. e.g. if you misspelt 'sechi' then updated it in one place, it will suggest updates through the script. Hit tab to move through these. \n\nThe box will also tell you if indexing is available. Indexing allows AI to search your code faster. \n\n### 4. Inline code generation\n\nIn VScode you can also access an inline chat box with cmd/cntrl-i. This chat can chat as well as edit code. \n\nYou can click anywhere and active this. I find it most useful though to select a section of code and then hit cmd/cntrl-i. \n\nThis is most useful to \n- Add new code\n- Explain code\n- Fix bugs \n- Add tests \n\nTry select some of your code (e.g. a ggplot) and ask it to explain what the code does. \n\nNow try select one of your plots and ask for some style changes (e.g. theme, colours, axes label sizes etc...). \n\nNow add a bug into one of your plots. See if the inline chatbox can fix the bug. \n\n#### Prompt shortcuts\n\nUse the `/` to bring up a list of prompt shortcuts. The most useful in R are `/explain`, `/fix`, `/tests`. Try select some code then use these to see what happens. \n\n## Planning your analysis with Ask mode\n\n### Stages of analysis \n\nThere are overall decisions you need to make when developing your analysis:\n\n1. What types of statistics to use. \n2. How to implement those statistics in R code. \n\nIts worthwhile separting these two decisions, as they are different issues. One is a science question, the other is a programming question. \n\nWhen using Assistants its also worthwhile using different chat sessions to try and find answers. \n\n### Ask mode \n\nAsk mode helps you plan analysis and implementation, using context from your project. \n\nIn VScode click the 'octocat' symbol that should be at the top towards the right. This will open the chat window. \n\nThe chat panel will appear down the bottom of this new sidebar. Confirm that the chatbot is currently set to 'Ask' mode. \n\nYour current file will automatically be included as context for the prompt. You can drag and drop any other files here as well. \n\nStart by asking the chatbot for guidance on a statistical analysis. We are interested in how the abundance of Topa relates to coral cover. For instance you could ask:\n\n```\nHow can I test the relationship between pres.topa and CB_cover?\n```\n\nEvaluate the quality of its response and we will discuss. \n\n### The jagged frontier of LLM progress\n\nLLMs were created to write text. But it soon became apparent that they excel at writing programming code in many different languages. \n\nSince then AI companies have been optimising their training and development for coding and logic. \n\nThere are a series of standardized tests that are used to compare quality of LLMs. Common evaluation tests are the SWE benchmark which looks at the ability of LLMs to autonomously create bug fixes. Current models get about [50% resolution on this benchmark](https://www.swebench.com/). \n\nTheir progress on math and logic is a bit more controversial. It seems like some of the math benchmarks (like AIME annual tests for top 5% highschool students) [are saturated as LLMs are scoring close to 100% on these tests.](https://epoch.ai/frontiermath/the-benchmark). So newer tests of unsolved maths problems are being developed. \n\nHowever, others are finding that the ability of [LLMs on math and logic are overstated](https://garymarcus.substack.com/p/reports-of-llms-mastering-math-have), perhaps because the LLMs have been trained on the questions and the answers. Its also clear that AI companies have a strong financial incentive to find ways (real and otherwise) of improving on the benchmarks. Are the moment there is tough competition to be 'industry leaders' and grab market share with impressive results on benchmarks. \n\nEither way, it does seem that the current areas of progress are programming, math and logic. \n\nEvaluations on statistics and the R software are less common. \n\nThe limited evaluations of LLMs on their ability to identify the correct statistical procedure are less impressive than other benchmarks. [An evaluation (published 2025) of several models, including GPT-4 as the most up-to-date model](https://arxiv.org/abs/2406.07815), found accuracy at suggesting the correct statistical test of between 8% and 90%. \n\nIn general LLMs were good at choosing descriptive statistics (accuracy of up to 90% for GPT-4). Whereas when choosing inferential tests accuracy was much less impressive - GPT-4 scored between 20% and 43% accuracy on questions for which a contingency table was the correct answer. \n\nThe results also indicate the improvements that can be gained through better prompts (i.e. doubling in accuracy for GPT 4). \n\nThe lesson is two-fold. Just because LLMs excel at some tasks doesn't mean they will excel at others. Second, good prompting strategies pay off. \n\nFor us in the niche R world there is also another lesson. The LLMs should be good at helping us implement analyses (ie write the R code). However, they are less reliable as statisticians who can guide us on the scientific question of what type of analysis to do. \n\n### How to prompt for better statistical advice\n\nThe limited number of evaluations of LLMs for statistics have found the biggest improvements for prompts that:\n\n- Include domain knowledge in the prompt\n- Include data or summary data in the prompt\n- Combine domain knowledge with CoT (but CoT on its own doesn't help)\n\nIn addition, larger and more up-to-date models tend to be better. e.g. try Claude 4.0 over GPT-mini.  \n\n#### What LLMs don't do that real statisticians do... \n\nIf you consult a human statistician they'll usually ask you lots of questions. LLMs, in contrast, will tend to just give you an answer, whether or not they have enough context. \n\nSay you asked me the same question you had in your LLM prompt like \"how do see if fish are related to coral\". There's no way I'd jump in with an answer with so little information. But the LLM will. \n\nSo be aware of this shortcoming and come to prompting pre-prepared with the context it will need to give you a better answer. \n\n\n```{r echo=FALSE, message=FALSE,warning=FALSE}\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\n# Create sample data\ntime <- seq(1, 10)\n\n# Human consultant data - starts with questions, transitions to more answers with a curved pattern\nhuman_questions <- c(8, 7.5, 6.7, 5.6, 4.4, 3.2, 2.1, 1.4, 1.1, 1)\nhuman_answers <- c(1, 1.8, 2.7, 3.9, 5.2, 6.4, 7.3, 7.8, 8, 8.2)\n\n# AI assistant data - consistently gives more answers than asks questions (with slight wobble)\nai_questions <- c(2.2, 1.8, 2.1, 2.3, 1.9, 2.2, 1.8, 2.1, 1.9, 2.2)\nai_answers <- c(7.1, 7.3, 6.8, 7.2, 6.9, 7.4, 7, 7.2, 6.8, 7.1)\n\n# Combine data\ndf <- data.frame(\n  time = rep(time, 4),\n  number = c(human_questions, human_answers, ai_questions, ai_answers),\n  type = rep(c(rep(\"Questions\", 10), rep(\"Answers\", 10)), 2),\n  source = c(rep(\"Human consultant\", 20), rep(\"AI assistant\", 20))\n)\n\n# Create plot\nggplot(df, aes(x = time, y = number, color = type, group = type)) +\n  geom_line(size = 1.2) +\n  facet_wrap(~ source, ncol = 2) +\n  labs(x = \"Time in conversation\", \n       y = \"Number of \\n questions and answers\",\n       color = \"\") +\n  theme_bw(base_size = 14) +\n  scale_color_manual(values = c(\"Questions\" = \"#E69F00\", \"Answers\" = \"#009E73\")) +\n  theme(\n    legend.position = \"top\",\n    strip.text = element_text(size = 14, face = \"bold\"),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    strip.background = element_rect(fill = \"white\")\n  )\n\n```\n\n**Figure 1 From Brown and Spillias in review** Comparison of how an experienced human statistical consultant would structure a conversation compared to a typical prompt chain with an AI assistant (figure 1). The human consultant will usually ask more questions than provide answers at the start of a conversation, then switch to providing more answers once they understand the context of the study. An AI assistant will tend to be constant in the number of questions it asks, unless explictly prompted to ask questions rather than provide answers. This means it provides answers without first gathering appropriate context. \n\n#### Guidelines for prompting for statistical advice\n\n**Attach domain knowledge** Try to find quality written advice from recognized researchers to include in your prompts. \n\n**Always provide context on the data** For instance, the model will give better advice for the prompt above if we tell it that `pres.topa` is integer counts (it will probably then recommend poisson GLM straight away). Likewise, if your replicates are different sites, tell that to the model so it has the opportunity to recommend approaches that are appropriate for spatial analysis. \n\n**Attach data to your prompts** You can attach the whole dataset if its in plain text (e.g. csv). Or write a `summary()` and/or `head()` to file and attach that. \n\n**Combine the above approaches with Chain of Thought** Just add 'use Chain of Thought reasoning' to your prompt. Its that easy. \n\n**Double-up on chain of thought with self evaluation** After the initial suggest try prompts like \"are you sure?\", \"Take a deep breath, count to ten and think deeply\", \"Evaluate the quality of the options on a 1-5 scale\". \n\n::: {.tip}\n**Tip:** Make a library of reference material for your prompting. If you see vignettes, blogs, or supplemental sections of papers that explain an analysis well, save them as text files to use in prompts. \n:::\n\n\n#### Improving our initial prompt by attaching data\n\nRecall our initial prompt was: \n\n```\nHow can I statistically test the relationship between pres.topa and CB_cover?\n```\n\nTry some of the strategies above (make a new prompt by clicking the  + button) and compare the quality of advice. \n\nFor instance, you can save a data summary like this: \n\n```{r, eval=FALSE}\nwrite_csv(head(dat), \"resources/head-site-level-data.csv\")\n```\n\nThen drag and drop it into the ask window and add something like:\n\n```\nHow can I statistically test the relationship between pres.topa and CB_cover? Here are the first 6 rows of data\n```\n\n#### Improving our initial prompt by attaching domain knowledge\n\nYou can further improve the response by attaching a trusted resource. e.g. [save this webpage on count models for ecology](https://environmentalcomputing.net/statistics/glms/glm-2/) to your computer. Then you can  attach the html file. That turned out to be a bit slow to compute (file too large?). Would be better if we had in plain text (e.g. copy and paste the text to a file, or use an extraction tool to extract text from the html).\n\nIf you installed the websearch tool (which will likely become default in future) then you could add a prompt like this:\n\n```\nHow can I statistically test the relationship between pres.topa and CB_cover? Here are the first 6 rows of data. pres.topa is my response and it is count data. Use @websearch to find robust recommendations for ecologists to analyse count data before proceeding with your recomemndations. \n```\n\nThat worked well for me. I then followed up with:\n\n```\nGreat. Evaluate the robustness of each suggestoin on a 1-10 scale\n```\n\nAnd it gave me a nice summary suggesting to try overdispersion models first (which is a good suggestion). \n\nThe absolute best practice would be to give the assistant all the context for your study and observational design. Let's see how doing that can work in our favour when planning implementation. \n\n### Planning implementation \n\nThe other main way to use Ask mode is for help in implementing an analysis. Many of our workflows are complex and involve multiple data wrangling steps. \n\nTo get the best out of GC I recommend creating a detailed README.md file with project context. Let's try that and use it to plan our project. \n\nSave the [README.md that his here to a local file](https://github.com/cbrown5/R-llm-workshop/tree/main/resources/benthic-analysis). (Remember that we are going to be using this as a prompt, so read it first). \n\nNow you can attach it (or open it then click new chat). Given all the context you've provided you can just write something simple like: \n\n```\nHelp me plan R code to implement this analysis. \n```\n\nOr\n```\nHelp me plan the workflow and scripts to implement this analysis\n```\n\nI did this. It suggested both code (that looked approximatley correct) and the directory structure, sticking to my guideline in the readme about being modular. \n\nYou should iterative with Ask mode to if there are any refinements you want. \n\nLet's move onto edit mode to see how to put this plan to action. \n\n## Creating your code with Edit mode\n\nEdit mode will edit files for you. The best way to learn how is to just see it in action. \n\nOpen the Chat panel and click the 'Ask' button, then select 'Edit'. \n\n### Adding a plan to the readme\n\nOpen the README.md. Then type this prompt: \n\n```\nHelp me plan the implementation of this project. Add the plan to the ## Steps section\n```\n\nClick 'Keep' if you like what it did. Or you can suggest improvements. Alternatively, accept it for now and then edit it afterwards. \n\n::: {.tip}\n**Tip:** Sometimes you can't go back once copilot has made edits to a file. So its good practice to use git and commmit changes before and after editing. \n:::\n\n#### Working through your plan \n\nOnce you're happy with the plan, you can get copilot to implement it. You can continue the current chat, or start a new chat to do this (depending on the length of the task). \n\nNow step through, asking copilot to create each file as you. \n\nAt this point everyone's answers will diverge, as there is an element of randomness to the LLM's responses. We will compare as a class to see if everyone gets to a similar analysis and answer. \n\n::: {.tip}\n**Tip:** We are using the readme.md is copilot's memory. This means the assitant always has the context it needs across different chat sessions (where it would otherwise forget). So its important to keep the readme updated. Its also useful to help you remember if you come back to the project some months or years later. \n:::\n\n#### Why so much code? \n\nCopilot is designed as a programming assistant. We don't know its system message, but given the main market for this software is professional programmers, we can guess it has a strong emphasis on programming robust code. \n\nYou might notice that copilot tend to 'over-engineer' your R scripts. For instance, it has a tendancy to make an `if` statement to check if each new package needs installing, before loading it. \n\nIf you don't like this style you can add a statement to the readme asking it to keep implementation simple. \n\n### Workflows and tips for edit mode \n\nRemember its an assistant, its not doing the project for you. So you need to make sure it stays on track. Left unattended (if you just accept, accept, accept without reading) it can go down rabbit holes. Sometimes it creates superfluous analyses or even incorret statistics. \n\nSo here's how I recommend you use it: \n\n- Use git for version control so you can go back in to older versions. \n- Read the suggested edits before accepting\n- Keep the readme.md updated and keep attaching it to your prompts. This will help keep it focused on the tasks that matter\n- Use a two-step approach to identifying the statistical tests first, then implementing them as R code second. If you conflate these tasks you risk letting copilot guide the stats and getting it wrong. \n- You can use it to help implement multiple different types of statistical tests for experimenting. If you do this, I just suggest you still use a two-step approach: plan a list of stats options first, then get copilot to implement them so you can compare results. \n\nNEVER edit the file while copilot is working! To edit files it uses string matching to locate the position to insert the edits. If you change the file it may not find the correct place to insert the new code. \n\n::: {.tip}\n**Tip:** LLMs will tend to suggest the most obvious statistical analyses. If you want to innovate creative new types of analyses you need to work a bit harder. One way to do this is to mix up your prompts to try and get cross-disciplinary pollination. For instance, you could ask it: \"Suggest methods I could use for this analysis, taking inspiriation from different disciplines such as medicine, psychology and climate research\".\n:::\n\n#### Suggested workflow for new analyses\n\nHere's a workflow I've found works well if I'm doing an analysis that is new to means\n\n1. Read the literature to identify the appropriate analysis for the research question and data. \n\n2. Once I've narrowed down the options I look for useful domain knowledge: vignettes, manuals or blogs that have suitable R examples. \n\n3. Start a new folder, setting up the directory and readme as descriped in this workshop. \n\n4. Use copilot to implement the analysis, attaching data summaries and the domain knowledge to get the best prompts. \n\n#### Suggested workflow for analyses I know well\n\nMuch the same as above, just less planning and you don't need to search the literature because you know what you want to do. If you save useful domain knowledge when you see it you will also have the documents on hand to support the assistant. \n\n## Automated workflows with Agent mode\n\nAgents are LLMs that have tools that allow them to work autonomously. In effect they review the results of tool use (such as writing code and running code), then respond to those results. \n\nIn Copilot's chat window you can set it to 'Agent' mode to enable these features. \n\nAfter each tool use copilot will ask you to confirm the changes and the next action. At that point you can review its changes, make edits, or continue chatting to suggest refinements. \n\n![](https://code.visualstudio.com/assets/blogs/2025/02/24/diagram.png)\n\nAgent mode has access to the terminal, so it will be using the terminal application to run scripts it creates. We'll demonstrate in class so you can understand what its doing. \n\n**Image:** Agent mode from https://code.visualstudio.com\n\nYou can also just accept every suggestion without reading it, also called 'vibe coding'. However, I don't recommend doing that, especially when you are starting out. You need to get a feel of how much direction it needs and problems it might create. Without human intervention the algorithms have a tendency to go off task: \n\n```{r eval = TRUE, echo=FALSE}\nlibrary(ggplot2)\nlibrary(grid)\nlibrary(gridExtra)\n\n# Create data for the workflows, now flowing vertically\nworkflow1 <- data.frame(\n    x = rep(1, 100),\n    y = seq(0, 10, length.out = 100)\n)\n\n# Add checkin points\ncheckins <- data.frame(\n    x = rep(1, 5),\n    y = seq(1, 9, by = 2)\n)\n\n# Create divergent workflow\nworkflow2 <- data.frame(\n    x = 1 + 0.2 * (seq(0, 10, length.out = 100) - 3)^2 / 10,\n    y = seq(0, 10, length.out = 100)\n)\nworkflow2$x[1:31] <- 1\n\n# Create the plot\np <- ggplot() +\n    # Draw workflows\n    geom_path(data = workflow1, aes(x = x, y = y), \n                        color = \"blue\", size = 1, arrow = arrow(length = unit(0.3, \"cm\"))) +\n    geom_path(data = workflow2, aes(x = x, y = y), \n                        color = \"red\", size = 1,  arrow = arrow(length = unit(0.3, \"cm\"))) +\n    # Add checkin points\n    geom_point(data = checkins, aes(x = x, y = y), \n                         color = \"blue\", size = 3) +\n    # Add start and end points (now at top and bottom)\n    annotate(\"text\", x = 1, y = -0.3, label = \"A (Start)\", size = 8) +\n    annotate(\"text\", x = 1, y = 11, label = \"B (Goal)\", size = 8) +\n    \n    # Add labels\n    annotate(\"text\", x = 0.7, y = 5, label = \"Workflow with checkins\", color = \"blue\", size = 7, angle = 90) +\n    annotate(\"text\", x = 2.5, y = 7, label = \"Divergent workflow\\n(no checkins)\", color = \"red\", size = 7) +\n    \n    # Theme adjustments\n    theme_minimal() +\n    theme(\n        axis.title = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank()\n    ) +\n    ylim(-0.5, 11) +\n    xlim(0.5, 3.5)\n\np\n```\n\nHave a readme with clear steps that you attach as a prompt is also helpful for Agent mode. It helps it stay on topic. \n\nAgent mode also allows installation of additional tools, which we'll explore later. \n\n### Exploring agent mode \n\nLet's explore Agent mode's features through some analysis. \n\n#### Motivating example Bayesian time-series analysis \n\nWe'll develop time-series models to forecast rock lobster (*Jasus edwardsii*) abundance from annual diver surveys. I've provided you with summary data. If you want to use this data in your research it is freely available and the original should be downloaded from [the AODN portal](https://portal.aodn.org.au/). \n\nWe'll use the [INLA package](https://www.r-inla.org/) for our time-series models. We'll fit it for the first part of the data, then we'll forecast to the last part. In this way we can test the model's predictions against data that is independent of model fitting (validation). \n\n[The example is based on my study where I asked how accurately we can forecast species abundance change in dynamic environment](https://www.biorxiv.org/content/10.1101/2025.01.23.634630v1.abstract). In a rapidly changing environment the models we fit to historical data may no longer make accurate predictions to future, novel, environments. So our current models may overstate the future predictability of ecosystems. \n\nIn short, the environments we want to predict to in the future have no analogue in contemporary data. This may make accurate prediction more challenging. \n\nTo explore this idea I developed a new way of validating time-series. I deliberately designed validations that forced the model fitting to be to older data and the forecasting and accuracy evaluation to be on contemporary data. As such, if the environment has changed the parameters the model has learned from the historical data will no longer be relevant in the contemporary environment. \n\nWe found the new method gave much more pessimistic estimates of model accuracy for species that undergo rapid changes. Whereas for species that have resisted environmental change the new method gave comparable results to traditional methods of validation. \n\nIn today's workshop we'll look at the first step, which is how to fit a model and make forecasts. \n\nWe chose Bayesian models with INLA because have several advantages over alternatives:\n\n1.  Allow for complex heirarchical models in a familiar GLMM framework - we have structuring by time and sites to consider\n\n2.  Are computationally fast to run - convenient if you are re-running the model to do cross validation. \n\n3. Automatically handles gaps in time-series - Our data has a gap in 2003 when funding for monitoring wasn't available\n\n4. Straightforward to model non-normal data - we are using counts. \n\nWe'll use INLA to fit auto-regressive order 1 (AR1) models to rock lobster abundance, with a negative binomial distribution. We'll also use INLA to make forecasts.\n\nAnother nice thing about INLA for us is that it has an unusual way of implementing predictions. This tends to trip-up copilot, so we'll see how to overcome that challenge and get copilot to write correct code. \n\n#### Set-up your project\n\nSet-up a new project, including creating a readme following the structure we used before. \n\nHere's the link to the data:\n\n```{r}\nlibrary(tidyverse)\ndat <- readr::read_csv(url(\"https://raw.githubusercontent.com/cbrown5/R-llm-workshop/refs/heads/main/resources/ATRC-RLS-jasus-edwardsii-maria-island.csv\"))\n```\n\n[You can see the readme.md I used to get started here](https://github.com/cbrown5/R-llm-workshop/tree/main/resources/forecasting-with-inla). I encourage you to write your own to get a feel for how it works and develop your own style. \n\n#### Prompts I used\n\nOnce I had the folder and readme set-up here's the series of prompts I used. I encourage you to explore making your own. \nI used Claude 4.0 as the model option. I've found that GPT occaisonally makes errors with tool use or stuffs up text matching when editing files (meaning it inserts text in the wrong place). \n\nI started a new chat session between each of these prompts. This helps manage the context window. I'm relying on updating the readme.md so Copilot has memory (and I get it to update that). \n\n```\nStart by documenting the directory structure in the readme.md\n```\n\n```\nI'd be most pleased if you can undertake to perform steps 1-2. Document the data variables in the readme when you are done. \n```\n\n::: {.tip}\n**Tip:** There's no 'optimal' prompt, only better prompts. Sometimes the best way to write is the way you are most comfortable writing. You'll get more out of your brain that way and copilot will end up performing the same. \n:::\n\n```\nAhoy you salty sea dog, we've scrubbed down steps 1 and 2, time for you to raise the sail on step 3!\n```\n\n(Ok so that last prompt definitely doesn't follow the guidelines of being super clear, but I was bored and it seemed to work ok)\n\nIt wrote some nice code for step 3, but had some problems with model convergence. At this point I intervened manually and edited the model myself. I didn't really want it deciding the model structure for me, as I knew what I wanted (below is the model I used FYI). That fixed it and I got it to document the changes then started a new chat. \n\nNote that the Agent changed the default fitting algorithm, which I wasn't pleased with. So always important to check the details. \n\n```\nsimple_model_formula <- total_lobsters ~ 1 + \n  protection_status +\n  f(site_numeric, model = \"iid\") +\n  f(year, model = \"ar1\", hyper = ar1_prior)\n\nar1_model <- inla(\n  formula = simple_model_formula,\n  data = train_data,\n  family = \"nbinomial\",  # Use negative binomial for count data\n  control.predictor = list(compute = TRUE),\n  control.compute = list(\n    dic = TRUE, \n    waic = TRUE,\n    cpo = FALSE,  # Disable CPO to help convergence\n    config = FALSE\n  ),\n  verbose = FALSE\n)\n```\n\nAfter fixing the model and updated the readme, here's the next step: \n\n```\nAlright cobber, take you best shot at step 4\n```\n\nThat worked, which actually I was expecting it not to work based on prior experience. [INLA does predictions as part of model fitting, so you can't `predict(model1)` like you can with other packages.](https://www.r-inla.org/faq#h.821k2r53fvx3). I've found that often trips up copilot when it tries to predict directly from the model object. It might be that Claude 4.0 (only came out as I was writing this) now 'knows' not to make that mistake. \n\nI tried again with Claude 3.5 (older version) to see if I could fool that one. However, it avoided the problem by writing a custom fitting function (which would need careful checking). \n\nAnyway, the lesson was meant to be to show you how to solve these types of problems by attaching domain knowledge like the FAQ linked above. \n\nCopilot agent did have some problems running Rscript on my computer (used to source R files from terminal). So I added this line to the readme to help it: When using Rscript from terminal be sure to put the script in \"\", e.g. `Rscript \"Scripts/script1.R\"`\n\n```\nJust step 5 left to go, make me some nice plots using the types of colours that Wes Anderson would choose\n```\n#### Writing up the project?\n\nYou can keep going from here if you like and get agent mode to write up the results it found as an Rmd file. It will use the tables it generates to (hopefully) make accurate interpretations. Pretty soon Copilot will also have vision capabilities (currently available in preview mode as of 2025-05-27). This means it will be able to interpret the figures it creates as well. We'll see that in action when we look at Roo Code in a bit. \n\nIf you do that, as always, don't take anything for granted. Make sure you check everything and understand the results yourself. \n\n#### Custom intstructions\n\nFor heavy agent use you may want to set-up custom instructions. These apply to all prompts in a project. e.g. you could set preference for ggplot2, or tell it how to use Rscript to avoid terminal errors [See here for instructions](https://docs.github.com/en/copilot/customizing-copilot/adding-repository-custom-instructions-for-github-copilot). \n\n### Summary\n\nAgent mode can really accelerate your workflow development. But there are some risks. It can also go off track or write excessive amounts of code (over-engineering). Best practices for using Agent mode include:\n\n- Separate science questions (what stats) from implementation stats (what code)\n- Understand the stats you want to do, don't just rely on copilot to get it right\n- Checking what it does at is does it, so you can keep it on track\n- Giving strong guidelines e.g. through a project readme file. \n- Keeping the readme updated to guide copilot\n- Report AI use and how it was used in your publications\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"06-github-copilot.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","bibliography":["book.bib"],"theme":"simplex"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"06-github-copilot.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"bibliography":["book.bib"],"documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}},"epub":{"identifier":{"display-name":"ePub","target-format":"epub","base-format":"epub"},"execute":{"fig-width":5,"fig-height":4,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"epub","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":false,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"default-image-extension":"png","html-math-method":"mathml","to":"epub","output-file":"06-github-copilot.epub"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"bibliography":["book.bib"],"cover-image":"cover.png"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf","epub"]}