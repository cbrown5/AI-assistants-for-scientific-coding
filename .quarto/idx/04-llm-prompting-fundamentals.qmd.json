{"title":"LLM prompting fundamentals","markdown":{"headingText":"LLM prompting fundamentals","containsRefs":false,"markdown":"\n*Start of practical material*\n\nWe'll cover LLM prompting theory through practical examples. This section will give you a deep understanding of how chatbots work and teach you some advanced prompting skills. By understanding these technical foundations, you'll be better equipped to leverage LLMs effectively for your R programming and data analysis tasks, and to troubleshoot when you're not getting the results you expect.\n\n**Software requirements:** VScode with R or Rstudio. API key, ideally to openrouter. R (best with ellmer) or Python.\n\n## Setup authorisation\n\nGet your API key, see @sec-apikeys and then @sec-ellmer for connecting that to Ellmer. \n\nIf you are using Python, save your API key in a .env file in your project directory, like this: \n`OPENROUTER_API_KEY=my-key-here \n\n## Understanding how LLMs work\n\nLarge Language Models (LLMs) operate by predicting the next token in a sequence, one token at a time. To understand how this works in practice, we'll call an API directly through computer code. \n\nBy accessing the API in this way we get as close to the raw LLM as we are able. Later on we will use 'coding assistants' (e.g. copilot) which put another layer of software between you and the LLM. \n\nTry to get it to complete a sentence: \n\n```{r eval=TRUE, echo=FALSE}\nknitr::opts_chunk$set(eval = FALSE)\n```\n\n::: {.panel-tabset group=\"language\"}\n\n### R - Ellmer\n```{r}\nlibrary(ellmer)\n\n# Initialize a chat with Claude\nchat <- chat_openrouter(\n  system_prompt = \"You are a helpful assistant.\",\n  model = \"anthropic/claude-3.5-haiku\",\n  api_args = list(max_tokens = 50)\n)\nchat$chat(\"Ecologists like to eat \")\n```\n\n### R\n```{r}\nlibrary(jsonlite)\nlibrary(httr)\nopenrouter_api_key <- Sys.getenv(\"OPENROUTER_API_KEY\")\nresponse <- POST(\n    url = \"https://openrouter.ai/api/v1/chat/completions\",\n    add_headers(\n      \"Content-Type\" = \"application/json\",\n      \"Authorization\" = paste(\"Bearer\", openrouter_api_key)\n    ),\n    body = toJSON(list(\n      model = \"anthropic/claude-3.5-haiku\",\n      messages = list(\n        list(\n          role = \"system\",\n          content = \"You are a helpful assistant.\"\n        ),  \n        list(\n          role = \"user\",\n          content = \"Ecologists like to eat \"\n        )\n      )\n    ), auto_unbox = TRUE),\n    encode = \"raw\"\n  )\nr3 <- fromJSON(content(response, \"text\"))\nr3$choices$message$content\n```\n\n### Python\n```{python}\nimport requests\nimport json\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\napi_key = os.getenv(\"OPENROUTER_API_KEY\")\nmodel = \"anthropic/claude-3.5-haiku\"\nmessage = \"Ecologists like to eat \"\n\nresponse = requests.post(\n  url=\"https://openrouter.ai/api/v1/chat/completions\",\n  headers={\n    \"Authorization\": f\"Bearer {api_key}\",\n    \"Content-Type\": \"application/json\"\n  },\n  data=json.dumps({\n    \"model\": model,\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\",\n      },\n      {\n        \"role\": \"user\",\n        \"content\": message,\n      }\n    ]\n  })\n)\ndata = response.json()\ncontent = data['choices'][0]['message']['content']\ncontent\n```\n:::\n\nNotice that the model doesn't do what we intend, which is complete the sentence. LLMs have a built in command to be an assistant. Let's use the 'system prompt' to provide it with strong directions. \n\n::: {.tip}\n**Tip:** The system prompt sets the overall context for a chat. It is meant to be a stronger directive than the user prompt. In most chat interfaces (e.g. copilot) you are interacting with the user prompt. The provider has provided the system prompt, [here's the system prompt for the chat interface version of anthropic (Claude)](https://www.reddit.com/r/ClaudeAI/comments/1ixapi4/here_is_claude_sonnet_37_full_system_prompt/)\n:::\n\n\n::: {.panel-tabset group=\"language\"}\n\n### R - Ellmer\n```{r}\nchat <- chat_openrouter(\n  system_prompt = \"Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides\",\n  model = \"anthropic/claude-3.5-haiku\",\n  api_args = list(max_tokens = 50)\n)\nchat$chat(\"Ecologists like to eat \")\n```\n\n### R\n```{r}\nlibrary(jsonlite)\nlibrary(httr)\nopenrouter_api_key <- Sys.getenv(\"OPENROUTER_API_KEY\")\nresponse <- POST(\n    url = \"https://openrouter.ai/api/v1/chat/completions\",\n    add_headers(\n      \"Content-Type\" = \"application/json\",\n      \"Authorization\" = paste(\"Bearer\", openrouter_api_key)\n    ),\n    body = toJSON(list(\n      model = \"anthropic/claude-3.5-haiku\",\n      messages = list(\n        list(\n          role = \"system\",\n          content = \"Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides\"\n        ),  \n        list(\n          role = \"user\",\n          content = \"Ecologists like to eat \"\n        )\n      ),\n      max_tokens = 50\n    ), auto_unbox = TRUE),\n    encode = \"raw\"\n  )\nr3 <- fromJSON(content(response, \"text\"))\nr3$choices$message$content\n```\n\n### Python\n```{python}\nimport requests\nimport json\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\napi_key = os.getenv(\"OPENROUTER_API_KEY\")\nmodel = \"anthropic/claude-3.5-haiku\"\nmessage = \"Ecologists like to eat \"\nsystem_prompt = \"Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides\"\n\nresponse = requests.post(\n  url=\"https://openrouter.ai/api/v1/chat/completions\",\n  headers={\n    \"Authorization\": f\"Bearer {api_key}\",\n    \"Content-Type\": \"application/json\"\n  },\n  data=json.dumps({\n    \"model\": model,\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": system_prompt,\n      },\n      {\n        \"role\": \"user\",\n        \"content\": message,\n      },\n    \"max_tokens\": 50\n    ]\n  })\n)\ndata = response.json()\ncontent = data['choices'][0]['message']['content']\n```\n:::\n\n::: {.tip}\n**Tip:** It is generally more effective to tell the LLM **what to do** rather than **what not to do** (just like people!). \n:::\n\n### Controlling randomness of the responess\n\nThe \"temperature\" parameter controls the randomness of token predictions. Lower temperatures (closer to 0) make the model more deterministic, while higher temperatures (closer to 2) make it more creative and unpredictable.\n\nEven at 0 there is still a small amount of randomness. We can also try to make a response more deterministic with the 'top_k' parameter. Top K forces the model to choose among a smaller set of tokens, setting it to 1 will give the most predictable results\n\nLet's compare responses with different temperatures and top K settings:\n\n::: {.panel-tabset group=\"language\"}\n\n### R - Ellmer \n\n```{r temperature-comparison, eval=FALSE}\n# Create chats with different temperature settings\nchat_temp <- chat_openrouter(\n          system_prompt = \"Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides\",\n        model = \"anthropic/claude-3.5-haiku\",\n        api_args = list(max_tokens = 50, temperature = 0, top_k = 1)\n    )\n\nchat_temp$chat(\"Marine ecologists like to eat \")\n\n```\n\n### R\n```{r}\nlibrary(jsonlite)\nlibrary(httr)\nopenrouter_api_key <- Sys.getenv(\"OPENROUTER_API_KEY\")\nresponse <- POST(\n    url = \"https://openrouter.ai/api/v1/chat/completions\",\n    add_headers(\n      \"Content-Type\" = \"application/json\",\n      \"Authorization\" = paste(\"Bearer\", openrouter_api_key)\n    ),\n    body = toJSON(list(\n      model = \"anthropic/claude-3.5-haiku\",\n      messages = list(\n        list(\n          role = \"system\",\n          content = \"Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides\"\n        ),  \n        list(\n          role = \"user\",\n          content = \"Marine ecologists like to eat \"\n        )\n      ),\n      max_tokens = 50,\n      temperature = 0,\n      top_k = 1\n    ), auto_unbox = TRUE),\n    encode = \"raw\"\n  )\nr3 <- fromJSON(content(response, \"text\"))\nr3$choices$message$content\n```\n\n### Python\n```{python}\nimport requests\nimport json\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\napi_key = os.getenv(\"OPENROUTER_API_KEY\")\nmodel = \"anthropic/claude-3.5-haiku\"\nmessage = \"Marine ecologists like to eat \"\nsystem_prompt = \"Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides\"\n\nresponse = requests.post(\n  url=\"https://openrouter.ai/api/v1/chat/completions\",\n  headers={\n    \"Authorization\": f\"Bearer {api_key}\",\n    \"Content-Type\": \"application/json\"\n  },\n  data=json.dumps({\n    \"model\": model,\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": system_prompt,\n      },\n      {\n        \"role\": \"user\",\n        \"content\": message,\n      }\n    ],\n    \"max_tokens\": 50,\n    \"temperature\": 0,\n    \"top_k\": 1\n  })\n)\ndata = response.json()\ncontent = data['choices'][0]['message']['content']\ncontent\n```\n:::\n\nNow try again but set the temperature parameter higher, say to 2 and remove the top_k line, or set it to a high number like 1000\n\nAt low temperatures and top K values, you'll notice the model consistently produces similar \"safe\" completions that focus on the most probable next tokens. As temperature increases, the responses become more varied and potentially more creative, but possibly less coherent.\n\n::: {.tip}\nThere are quite a few different parameters you can play with to control how an LLM responds. The [names and meanings of these are all documented in the Openrouter docs](https://openrouter.ai/docs/api-reference/parameters). \n:::\n\n### Navigating Openrouter\n\nI created a lot of the code for this tutorial by browsing the Openrouter page to see what is possible. \n\nThere are two key pages. \n\nThe [Models](https://openrouter.ai/models) page lists all the models available. If you have some idea of what you want you can filter this list. Click the page icon to copy the string for the model id. You can use this to replace the string in ` model = \"anthropic/claude-3.5-haiku\"` if you want to use a different model. \n\nClick the model's name to get more information about it. \n\nA single model may be available through multiple providers (servers), these will also be listed here. Openrouter automatically picks the provider for you. Their docs tell you how to fix the provider if you want to default to a specific choice (e.g. if you are concerned about some providers for data security of ethical reasons, more on this later). \n\nClick a provider then the drop-down arrow to see what parameter choices are available for this model (note not all providers support all parameters for a model). \n\nThe [second important page is the docs page](https://openrouter.ai/docs/quickstart). This has all the information on using the openrouter API. There are lost of code examples, usually in Python. You can use an AI assistant to translate these to R if you are using R. \n\nFor example, there is a long list of parameters that various LLMs support. If you want to try any of these just add them using the same syntax as we did for the temperature parameter. Note that not all LLMs support all parameters. To find out what parameters a model supports go to the model's provider page and [expand a specific provider to see a list of Supported Parameters](https://openrouter.ai/docs/api-reference/parameters). \n\n#### API errors.\n\nSometimes you won't get a response instead you'll get an HTTP error. After you send your response you may get an answer something like `Error: 402`. \n\n[All the error codes are listed here](https://openrouter.ai/docs/api-reference/errors). Some common errors codes: \n\n**400**\nThere's an error in your code. \n\n**401** \nAPI key is invalid (did you copy and paste it correctly? Does it need \"\" around it?)\n\n**402** \nA 402 means you've run out of API credits and need to put more money on your openrouter account against that API key. \n\n**408** \nTime out error, your request might be too big for the speed of your internet connection. \n\n**429** \nWow you are really a power user! You've been rate limited, meaning you're calling the LLM too much, too quickly. Just wait a bit and try again (or add pauses in your code to slow it down if you are doing automated repeat requests). \n\n### Comparing model complexity\n\nDifferent models have different capabilities based on their size, training data, and architecture. \n\nFor example `anthropic/claude-3.5-haiku` has many fewer parameters than `anthropic/claude-sonnet-4`. This means that the latter model is more complex and can handle more nuanced tasks. However, haiku is significantly cheaper to run. Haiku is 80c per million input tokens vs $3 for Sonnet. Output tokens are $4 vs $15\n\nTask for you: use the [Openrouter models page](https://openrouter.ai/models) to find a different model to try. \n\nHint try some of the Chinese models like Kimi instead of the American models.\n\n### Understanding context windows\n\nLLMs have a limited \"context window\" - the amount of text they can consider when generating a response. This affects their ability to maintain coherence over long conversations. For most LLMs this is about 100-200K tokens, which includes input and output. However, Google's models have up to 1 million + tokens. \n\nWe'll come back to the context window when we explore more advanced tools with longer prompts. These simple prompts don't come close to using up the context window. \n\n## DIY stats bot \n\nLet's put together what we've learnt so far and built our own chatbot. I've provided you with a detailed system prompt that implements a chat bot that specialises in helping with statistics. First, we read the bot markdown file from github, then we can use it in our chat session. \n\n::: {.panel-tabset group=\"language\"}\n\n### R - Ellmer\n```{r}\nstats_bot <- readr::read_file(url(\"https://raw.githubusercontent.com/cbrown5/AI-assistants-for-scientific-coding/refs/heads/main/resources/DIY-stats-bot-system.md\"))\n\nchat_stats <- chat_openrouter(\n  system_prompt = stats_bot,\n  model = \"anthropic/claude-sonnet-4\",\n  api_args = list(max_tokens = 5000)\n)\n\nchat$chat(\"Who are you?\")\n\n```\n\n### R\n```{r}\nlibrary(jsonlite)\nlibrary(httr)\nopenrouter_api_key <- Sys.getenv(\"OPENROUTER_API_KEY\")\nstats_bot <- readLines(\"https://raw.githubusercontent.com/cbrown5/AI-assistants-for-scientific-coding/refs/heads/main/resources/DIY-stats-bot-system.md\")\nresponse <- POST(\n    url = \"https://openrouter.ai/api/v1/chat/completions\",\n    add_headers(\n      \"Content-Type\" = \"application/json\",\n      \"Authorization\" = paste(\"Bearer\", openrouter_api_key)\n    ),\n    body = toJSON(list(\n      model = \"anthropic/claude-sonnet-4\",\n      messages = list(\n        list(\n          role = \"system\",\n          content = paste(stats_bot, collapse=\"\\n\")\n        ),\n        list(\n          role = \"user\",\n          content = \"Who are you?\"\n        )\n      ),\n      max_tokens = 5000\n    ), auto_unbox = TRUE),\n    encode = \"raw\"\n  )\nr3 <- fromJSON(content(response, \"text\"))\nr3$choices$message$content\n```\n\n### Python\n```{python}\nimport requests\nimport json\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\napi_key = os.getenv(\"OPENROUTER_API_KEY\")\nmodel = \"anthropic/claude-sonnet-4\"\nimport requests\nstats_bot = requests.get(\"https://raw.githubusercontent.com/cbrown5/AI-assistants-for-scientific-coding/refs/heads/main/resources/DIY-stats-bot-system.md\").text\n\nresponse = requests.post(\n  url=\"https://openrouter.ai/api/v1/chat/completions\",\n  headers={\n    \"Authorization\": f\"Bearer {api_key}\",\n    \"Content-Type\": \"application/json\"\n  },\n  data=json.dumps({\n    \"model\": model,\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": stats_bot,\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Who are you?\",\n      }\n    ],\n    \"max_tokens\": 5000\n  })\n)\ndata = response.json()\ncontent = data['choices'][0]['message']['content']\n```\n:::\n\nTry asking it some different statistical questions and note how it responds. \n\n::: {.tip}\n**Tip:** How many of you started using \"DIY-stats-bot-system.md\" without first reading it? Did you find the easter egg in my prompt? For security you should ALWAYS read prompts before you start running them through LLM chats. We'll see later that LLMs can be given 'tools' which allow them to run code on your computer. Its easy to see how a malicious prompt could mis-use these tools. We'll cover security later. \n:::\n\n### Multi-turn conversation\n\nYou might want to have a conversation with your chat bot to help you problem solve. This is easy with Ellmer and a bit fiddly with python or base R. I'm only going to cover Ellmer here. \n\n::: {.panel-tabset group=\"language\"}\n\n### R - Ellmer\n\n```{r}\nstats_bot <- readr::read_file(url(\"https://raw.githubusercontent.com/cbrown5/AI-assistants-for-scientific-coding/refs/heads/main/resources/DIY-stats-bot-system.md\"))\n\nchat_stats <- chat_openrouter(\n  system_prompt = stats_bot,\n  model = \"anthropic/claude-sonnet-4\",\n  api_args = list(max_tokens = 5000)\n)\n\nlive_console(chat_stats)\n# live_browser(chat_stats)\n```\n\n`live_console` let's you chat in the terminal. `live_browser` opens up a browser window that is hosted locally on your computer. It feels the most like chatGPT. \n\n### R \n\nI suggest using ellmer for multi-turn conversations. If Ellmer doesn't work, don't worry we'll do more on this with github copilot in a moment. \n\n### Python \n\nThere are some packages available to help with multi-turn conversation. For example the [chat-cli](https://deepwiki.com/potofo/openrouter-chat-cli/1-overview). These take some time to set-up so we won't go over them here, read the docs for more information. \n\n:::\n\n::: {.tip}\n**Tip:** LLMs performance is better if you put everything in an initial single questions, rather than a multi-turn conversation. [Responses to a problem split into multiple questions are less accurate than responses to problem that is put up front in the initial prompt](https://arxiv.org/abs/2505.06120). This rule applies across many domains from logic to coding. I like to think of it as mansplaining, the LLMs have (sub-concioulsy?) been designed to prefer being mansplained over having a conversation. We'll look more at this later. \n:::\n\n### Improving the stats bot\n\nMake a local copy of the stats bot system prompt and try editing it. You can use your coding skills to write the stats bot to file, or just go here and copy the text: [https://raw.githubusercontent.com/cbrown5/AI-assistants-for-scientific-coding/refs/heads/main/resources/DIY-stats-bot-system.md](https://raw.githubusercontent.com/cbrown5/AI-assistants-for-scientific-coding/refs/heads/main/resources/DIY-stats-bot-system.md). \n\nTry different commands within it and see how your chat bot responds (you'll have to open a new chat object each time). \n\nHere's some ideas. \n\n- Try making a chat bot that is a verhment Bayesian that abhors frequentist statistics.   \n- You could provide it with more mode-specific instructions. For instance, try to get the chatbot to suggest appropriate figures for verifying statistical models. \n- Try different temperatures. \n- Add your own easter egg. \n\n::: {.tip}\n**Tip:** Adjectives, CAPITALS, `*markdown*` formatting can all help create emphasis so that your model more closely follows your commands. I used 'abhors' and 'verhment' above on purpose. \n:::\n\n\n### Tools\n\nTools like Copilot Agent mode then go a step further and send the results of step 5 back to the LLM, which then interprets the results and the loop continues (sometimes with and sometimes without direct user approval). \n\nIf you want to go further with making your own tools, then I suggest you check out `ellmer` package. It supports tool creation in a structured way. For instance, I made a tool that allows an [LLM to download and save ocean data to your computer](https://ellmer.tidyverse.org/articles/tool-calling.html).  \n\n## Reflection on prompting fundamentals \n\nTo recap, the basic workflow an agent follows is: \n\n1. Set-up a system prompt with detailed instructions for how the LLM should format responses\n2. User asks a question that is sent to the LLM\n3. LLM responds and sends response back to user\n4. Software on user's computer attempts to parse and act on the response according to pre-determined rules\n5. User's computers enacts the commands in the response and provides results to user\n\nThe key things I hoped you learnt from this lesson are: Basic LLM jargon, including tokens, temperature, API access and different LLM models. \n\nNow you understand the basics, let's get into Github Copilot.\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["style.css"],"output-file":"04-llm-prompting-fundamentals.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","bibliography":["book.bib"],"theme":"simplex"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"04-llm-prompting-fundamentals.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"bibliography":["book.bib"],"documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}},"epub":{"identifier":{"display-name":"ePub","target-format":"epub","base-format":"epub"},"execute":{"fig-width":5,"fig-height":4,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"epub","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":false,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"default-image-extension":"png","html-math-method":"mathml","to":"epub","output-file":"04-llm-prompting-fundamentals.epub"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"bibliography":["book.bib"],"cover-image":"cover.png"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf","epub"]}