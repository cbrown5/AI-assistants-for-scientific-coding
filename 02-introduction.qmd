# Introduction to LLMs for R

**Time:** 9-10am

In this presentation I'll cover how LLMs work, best practices prompt engineering, software, applications for R users and ethics.

This chapter provides an overview of:

- How Large Language Models (LLMs) function and their capabilities
- Best practices for prompt engineering when working with R
- Software options available for R users to interact with LLMs (coding assistants)
- Practical applications of LLMs for R programming and data analysis
- Ethical considerations when using LLMs for scientific work


We'll explore how LLMs can enhance your R workflow, from code generation to data analysis assistance, while maintaining scientific rigor and reproducibility.

[Slides for presentation (on a google drive)](https://docs.google.com/presentation/d/1BYfAjU4NPaIQ9CNSRlIVZJpJGof3moS9/edit?usp=drive_link&ouid=107596646496267980935&rtpof=true&sd=true)


## The jagged frontier of LLM progress

LLMs were created to write text. But it soon became apparent that they excel at writing programming code in many different languages. 

Since then AI companies have been optimising their training and development for coding and logic. 

There are a series of standardized tests that are used to compare quality of LLMs. Common evaluation tests are the SWE benchmark which looks at the ability of LLMs to autonomously create bug fixes. Current models get about [50% resolution on this benchmark](https://www.swebench.com/). 

Their progress on math and logic is a bit more controversial. It seems like some of the math benchmarks (like AIME annual tests for top 5% highschool students) [are saturated as LLMs are scoring close to 100% on these tests.](https://epoch.ai/frontiermath/the-benchmark). So newer tests of unsolved maths problems are being developed. 

However, others are finding that the ability of [LLMs on math and logic are overstated](https://garymarcus.substack.com/p/reports-of-llms-mastering-math-have), perhaps because the LLMs have been trained on the questions and the answers. Its also clear that AI companies have a strong financial incentive to find ways (real and otherwise) of improving on the benchmarks. Are the moment there is tough competition to be 'industry leaders' and grab market share with impressive results on benchmarks. 

Either way, it does seem that the current areas of progress are programming, math and logic. 

Evaluations on statistics and the R software are less common. 

The limited evaluations of LLMs on their ability to identify the correct statistical procedure are less impressive than other benchmarks. [An evaluation (published 2025) of several models, including GPT-4 as the most up-to-date model](https://arxiv.org/abs/2406.07815), found accuracy at suggesting the correct statistical test of between 8% and 90%. 

In general LLMs were good at choosing descriptive statistics (accuracy of up to 90% for GPT-4). Whereas when choosing inferential tests accuracy was much less impressive - GPT-4 scored between 20% and 43% accuracy on questions for which a contingency table was the correct answer. 

The results also indicate the improvements that can be gained through better prompts (i.e. doubling in accuracy for GPT 4). 

The lesson is two-fold. Just because LLMs excel at some tasks doesn't mean they will excel at others. Second, good prompting strategies pay off. 

For us in the niche R world there is also another lesson. The LLMs should be good at helping us implement analyses (ie write the R code). However, they are less reliable as statisticians who can guide us on the scientific question of what type of analysis to do. 