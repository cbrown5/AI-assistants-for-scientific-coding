# AI powered analysis workflows 

This section about advice for workflows

![](resources/figure2-brown-spillias.png)

## Designing an analysis

### Desinging an analysis with chatbots 

#### The jagged frontier of LLM progress

LLMs were created to write text. But it soon became apparent that they excel at writing programming code in many different languages. 

Since then AI companies have been optimising their training and development for coding and logic. 

There are a series of standardized tests that are used to compare quality of LLMs. Common evaluation tests are the SWE benchmark which looks at the ability of LLMs to autonomously create bug fixes. Current models get about [50% resolution on this benchmark](https://www.swebench.com/). 

Their progress on math and logic is a bit more controversial. It seems like some of the math benchmarks (like AIME annual tests for top 5% highschool students) [are saturated as LLMs are scoring close to 100% on these tests.](https://epoch.ai/frontiermath/the-benchmark). So newer tests of unsolved maths problems are being developed. 

However, others are finding that the ability of [LLMs on math and logic are overstated](https://garymarcus.substack.com/p/reports-of-llms-mastering-math-have), perhaps because the LLMs have been trained on the questions and the answers. Its also clear that AI companies have a strong financial incentive to find ways (real and otherwise) of improving on the benchmarks. Are the moment there is tough competition to be 'industry leaders' and grab market share with impressive results on benchmarks. 

Either way, it does seem that the current areas of progress are programming, math and logic. 

Evaluations on statistics and the R software are less common. 

The limited evaluations of LLMs on their ability to identify the correct statistical procedure are less impressive than other benchmarks. [An evaluation (published 2025) of several models, including GPT-4 as the most up-to-date model](https://arxiv.org/abs/2406.07815), found accuracy at suggesting the correct statistical test of between 8% and 90%. 

In general LLMs were good at choosing descriptive statistics (accuracy of up to 90% for GPT-4). Whereas when choosing inferential tests accuracy was much less impressive - GPT-4 scored between 20% and 43% accuracy on questions for which a contingency table was the correct answer. 

The results also indicate the improvements that can be gained through better prompts (i.e. doubling in accuracy for GPT 4). 

The lesson is two-fold. Just because LLMs excel at some tasks doesn't mean they will excel at others. Second, good prompting strategies pay off. 

For us in the niche R world there is also another lesson. The LLMs should be good at helping us implement analyses (ie write the R code). However, they are less reliable as statisticians who can guide us on the scientific question of what type of analysis to do. 

### How to prompt for better statistical advice

The limited number of evaluations of LLMs for statistics have found the biggest improvements for prompts that:

- Include domain knowledge in the prompt
- Include data or summary data in the prompt
- Combine domain knowledge with CoT (but CoT on its own doesn't help)

In addition, larger and more up-to-date models tend to be better. e.g. try Claude 4.0 over GPT-mini.  

#### What LLMs don't do that real statisticians do... 

If you consult a human statistician they'll usually ask you lots of questions. LLMs, in contrast, will tend to just give you an answer, whether or not they have enough context. 

Say you asked me the same question you had in your LLM prompt like "how do see if fish are related to coral". There's no way I'd jump in with an answer with so little information. But the LLM will. 

So be aware of this shortcoming and come to prompting pre-prepared with the context it will need to give you a better answer. 


```{r echo=FALSE, message=FALSE,warning=FALSE}
library(ggplot2)
library(tidyr)
library(dplyr)
# Create sample data
time <- seq(1, 10)

# Human consultant data - starts with questions, transitions to more answers with a curved pattern
human_questions <- c(8, 7.5, 6.7, 5.6, 4.4, 3.2, 2.1, 1.4, 1.1, 1)
human_answers <- c(1, 1.8, 2.7, 3.9, 5.2, 6.4, 7.3, 7.8, 8, 8.2)

# AI assistant data - consistently gives more answers than asks questions (with slight wobble)
ai_questions <- c(2.2, 1.8, 2.1, 2.3, 1.9, 2.2, 1.8, 2.1, 1.9, 2.2)
ai_answers <- c(7.1, 7.3, 6.8, 7.2, 6.9, 7.4, 7, 7.2, 6.8, 7.1)

# Combine data
df <- data.frame(
  time = rep(time, 4),
  number = c(human_questions, human_answers, ai_questions, ai_answers),
  type = rep(c(rep("Questions", 10), rep("Answers", 10)), 2),
  source = c(rep("Human consultant", 20), rep("AI assistant", 20))
)

# Create plot
ggplot(df, aes(x = time, y = number, color = type, group = type)) +
  geom_line(size = 1.2) +
  facet_wrap(~ source, ncol = 2) +
  labs(x = "Time in conversation", 
       y = "Number of \n questions and answers",
       color = "") +
  theme_bw(base_size = 14) +
  scale_color_manual(values = c("Questions" = "#E69F00", "Answers" = "#009E73")) +
  theme(
    legend.position = "top",
    strip.text = element_text(size = 14, face = "bold"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks = element_blank(),
    strip.background = element_rect(fill = "white")
  )

```

**Figure 1 From Brown and Spillias in review** Comparison of how an experienced human statistical consultant would structure a conversation compared to a typical prompt chain with an AI assistant (figure 1). The human consultant will usually ask more questions than provide answers at the start of a conversation, then switch to providing more answers once they understand the context of the study. An AI assistant will tend to be constant in the number of questions it asks, unless explictly prompted to ask questions rather than provide answers. This means it provides answers without first gathering appropriate context. 

#### Guidelines for prompting for statistical advice

**Attach domain knowledge** Try to find quality written advice from recognized researchers to include in your prompts. 

**Always provide context on the data** For instance, the model will give better advice for the prompt above if we tell it that `pres.topa` is integer counts (it will probably then recommend poisson GLM straight away). Likewise, if your replicates are different sites, tell that to the model so it has the opportunity to recommend approaches that are appropriate for spatial analysis. 

**Attach data to your prompts** You can attach the whole dataset if its in plain text (e.g. csv). Or write a `summary()` and/or `head()` to file and attach that. 

**Combine the above approaches with Chain of Thought** Just add 'use Chain of Thought reasoning' to your prompt. Its that easy. 

**Double-up on chain of thought with self evaluation** After the initial suggest try prompts like "are you sure?", "Take a deep breath, count to ten and think deeply", "Evaluate the quality of the options on a 1-5 scale". 

::: {.tip}
**Tip:** Make a library of reference material for your prompting. If you see vignettes, blogs, or supplemental sections of papers that explain an analysis well, save them as text files to use in prompts. 
:::

### With web tools

## 