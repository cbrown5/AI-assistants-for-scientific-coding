[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI assistants for Scientific Coding",
    "section": "",
    "text": "0.1 Summary\nIf you are doing data analysis you are probably using language models (e.g. ChatGPT) to help you write code, but are you using them in the most effective way? Language models have different biases to humans and so make different types of errors. This book will cover how to use language models to learn scientific computing and conduct reliable environmental analyses. The book is reference material for a 1-day workshop I teach.\nI will cover:\nThe content I’ll teach is suitable for anyone using computing coding (e.g. R, Python) to do data analysis.\nExamples will be in marine conservation science using the R language, but the methods are general to any field. The AI software is also general to any programming language and we won’t be doing much actual coding (the AI does that!) so participants can follow along in other languages if they prefer. To follow the practical applications you will need to have some experience in scientific computing (e.g. R or Python).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "AI assistants for Scientific Coding",
    "section": "",
    "text": "How to use different software tools from the simple interfaces like ChatGPT to advanced tools that can run and test code by themselves and keep going until the analysis is complete (and even written up).\nVibe coding and how future analysis workflows will change dramatically from today\nBest practice prompting techniques that can dramatically improve model performance for complex data analysis\nApplying language models to common environmental applications such as GLMs and multivariate statistics\nIssues including environmental impacts, copyright and ethics\nI’ll also make space for an interactive discussion of people’s concerns about AI, but also the opportunities.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "index.html#citation-for-book",
    "href": "index.html#citation-for-book",
    "title": "AI assistants for Scientific Coding",
    "section": "0.2 Citation for book",
    "text": "0.2 Citation for book\nPlease consider citing my accompanying article if you are using the advice in this book, currently in pre-print form:\nCitation: Brown & Spillias (2025). Prompting large language models for quality ecological statistics. Pre-print. https://doi.org/10.32942/X2CS80\nGenerative AI software for coding is changing fast. So this book will keep changing as I update it to reflect new developments. It does not make sense to publish it as a fixed book. The general guidelines for prompting, which have a longer shelf-life are captured in the article. So please cite that to support my work.\n\n0.2.0.1 Who should read this book?\nThe book is for: anyone who currently uses R, from intermittent users to experienced professionals. The workshop is not suitable for those that need an introduction to R and I’ll assume students know at least what R does and are able to do tasks like read in data and create plots.\nImportant This book isn’t for people who need an introduction to R or Python. I’ll assume students know at least how to do tasks like read in data and create plots in Python or R. To use these AI tools effectively you absolutely have to understand how scientific computing works first. If you need an introduction to R then I recommend you learn it without AI first.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "index.html#about-chris",
    "href": "index.html#about-chris",
    "title": "AI assistants for Scientific Coding",
    "section": "0.3 About Chris",
    "text": "0.3 About Chris\nI’m an Associate Professor of Fisheries Science at University of Tasmania and an Australian Research Council Future Fellow. I specialise in data analysis and modelling, skills I use to better inform environmental decision makers. R takes me many places and I’ve worked with marine ecosystems from tuna fisheries to mangrove forests. I’m an experienced teacher of R. I have taught R to 100s people over the years, from the basics to sophisticated modelling and for everyone from undergraduates to my own supervisors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "index.html#software-youll-need-for-this-workshop",
    "href": "index.html#software-youll-need-for-this-workshop",
    "title": "AI assistants for Scientific Coding",
    "section": "0.4 Software you’ll need for this workshop",
    "text": "0.4 Software you’ll need for this workshop\nSave some time for setting up the software, there is a bit to it. You may also need IT help if your computer is locked down. See the Chapter 3 on for more detailed instructions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "index.html#book-overview",
    "href": "index.html#book-overview",
    "title": "AI assistants for Scientific Coding",
    "section": "0.5 Book overview",
    "text": "0.5 Book overview\nNote the book is still under construction. Most sections are complete, but a few sections just have bullet points that I refer to when teaching the workshop. It’s also a fast moving space, so the content needs constant updating.\n\nIntroduction — Introduction to LLMs for coding: Strengths, weaknesses and common failure modes of language models when used for programming and data analysis.\nSet-up and tools: Step‑by‑step software installation, recommended environments (R, Python, editors), and brief troubleshooting tips.\nCalling LLMs via an API: Accessing LLMs programmatically from R or Python, prompt structure and examples.\nGitHub Copilot for R and coding assistants: How to use Copilot and other assistant features for planning, coding and editing.\nGeneral prompting advice: Best practices for effective prompts, problem decomposition and chain-of-thought techniques.\nAI-powered analysis workflows: Organising workflows, stages of analysis and worked examples using an example dataset.\nAdvanced LLM agents: Agents, automation, safety considerations and example agent workflows.\nProject set-up for AI agents (Specification sheets): Templates and guidance for organising projects to work well with agents and assistants.\nResearch applications of LLMs: Programmatic literature review, pdf processing, web-searching and other research-focused examples.\nWriting documents with Quarto and AI assistants: Workflows for writing papers, reproducible figures and integrating AI into Quarto.\nCost and security: Practical considerations on cost, API security and agent safety.\nEthics and copyright: Environmental impacts, IP concerns and ethical responsibilities when using LLMs.\nAppendix — Code and data: Code snippets, data access links and supporting scripts used in the book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "AI assistants for Scientific Coding",
    "section": "0.6 Data",
    "text": "0.6 Data\nWe’ll load all data files directly via URL in the workshop notes. So no need to download any data now. Details on data attribution are below.\n\n0.6.1 Benthic cover surveys and fish habitat\nIn this course we’ll be analyzing benthic cover and fish survey data. These data were collected by divers doing standardized surveys on the reefs of Kia, Solomon Islands. These data were first publshed by Hamilton et al. 2017 who showed that logging of forests is causing sedimentation and impact habitats of an important fishery species.\nIn a follow-up study Brown and Hamilton 2018 developed a Bayesian model that estimates the size of the footprint of pollution from logging on reefs.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "02-introduction.html",
    "href": "02-introduction.html",
    "title": "2  Introduction to LLMs for coding",
    "section": "",
    "text": "2.1 The jagged frontier of LLM progress\nLLMs were created to write text. But it soon became apparent that they excel at writing programming code in many different languages.\nSince then AI companies have been optimising their training and development for coding and logic.\nThere are a series of standardized tests that are used to compare quality of LLMs. Common evaluation tests are the SWE benchmark which looks at the ability of LLMs to autonomously create bug fixes. Current models get about 50% resolution on this benchmark.\nTheir progress on math and logic is a bit more controversial. It seems like some of the math benchmarks (like AIME annual tests for top 5% highschool students) are saturated as LLMs are scoring close to 100% on these tests.. So newer tests of unsolved maths problems are being developed.\nHowever, others are finding that the ability of LLMs on math and logic are overstated, perhaps because the LLMs have been trained on the questions and the answers. Its also clear that AI companies have a strong financial incentive to find ways (real and otherwise) of improving on the benchmarks. Are the moment there is tough competition to be ‘industry leaders’ and grab market share with impressive results on benchmarks.\nEither way, it does seem that the current areas of progress are programming, math and logic.\nEvaluations on statistics and the R software are less common.\nThe limited evaluations of LLMs on their ability to identify the correct statistical procedure are less impressive than other benchmarks. An evaluation (published 2025) of several models, including GPT-4 as the most up-to-date model, found accuracy at suggesting the correct statistical test of between 8% and 90%.\nIn general LLMs were good at choosing descriptive statistics (accuracy of up to 90% for GPT-4). Whereas when choosing inferential tests accuracy was much less impressive - GPT-4 scored between 20% and 43% accuracy on questions for which a contingency table was the correct answer.\nThe results also indicate the improvements that can be gained through better prompts (i.e. doubling in accuracy for GPT 4).\nThe lesson is two-fold. Just because LLMs excel at some tasks doesn’t mean they will excel at others. Second, good prompting strategies pay off.\nFor us in the niche R world there is also another lesson. The LLMs should be good at helping us implement analyses (ie write the R code). However, they are less reliable as statisticians who can guide us on the scientific question of what type of analysis to do.\nIn this book we’ll walk through some of the ways you can use LLMs to help you do statistics, data analysis and R coding (and a bit of python too). There are a multitude of software and R package options. So many its bewildering. In the next chapter I’ll give you a few of the easiest set-up optoins available.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to LLMs for coding</span>"
    ]
  },
  {
    "objectID": "03-set-up.html",
    "href": "03-set-up.html",
    "title": "3  Software you’ll need for this book and how to set it up",
    "section": "",
    "text": "3.1 R packages\nIf you are using R then we will make use of: install.packages(c(\"vegan\", \"ellmer\",\"tidyverse\"). For Python users you can follow along most examples without these, this isn’t an R training course, its an course on using AI to assist with coding.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Software you'll need for this book and how to set it up</span>"
    ]
  },
  {
    "objectID": "03-set-up.html#options-for-ai-software",
    "href": "03-set-up.html#options-for-ai-software",
    "title": "3  Software you’ll need for this book and how to set it up",
    "section": "3.2 Options for AI software",
    "text": "3.2 Options for AI software\n\n3.2.1 Preferred option for R users\nWhat I will be using, and what I prefer you to use is:\n\nThe R program from the VS Code Integrated Development Environment (IDE)\nGithub Copilot\nR Ellmer package, for which you’ll need an API key to one of the LLM providers\n\nThe challenges with the above options are that it can sometimes be difficult to connect R and VScode. Comprehensive instructions are below. You may need IT help, especially if your computer is locked down!\nIf you chose this option you will need to follow instructions in this chapter for ‘Getting an API key’ (Section 3.4) (required for ellmer), ‘Ellmer set-up’ (Section 3.7), VS code setup (Section 3.5) and ‘Github Copilot set-up’ (Section 3.6).\n\n\n3.2.2 Option 2 for R users\nUse Rstudio with R. You will need ellmer and gander packages. See instructions below for ‘Getting an API key’ (Section 3.4), ‘Ellmer set-up’ (Section 3.7) and ‘Gander: Rstudio friendly alternative to copilot’ (Section 3.8).\nRstudio also has basic Github Copilot capablities. To use these (not essential for the workshop) get a copilot account (Section 3.6) then in Rstudio go to ‘Tools’ -&gt; ‘Global Options’ -&gt; ‘Copilot’ and enable it (follow instructions for entering password to setup).\n\n\n3.2.3 Option 3 for R users: Positron assistant\nPositron is a new IDE from Posit (the creators of RStudio) that is designed for data science. It includes a built-in AI assistant that can help you with coding tasks.\nPositron is very similar to VScode, in fact the editor was originally a copy of VScode’s source code. It is designed for R and Python, so for some people it may be easier to set-up than VScode.\nI’ve put it as option 3 for now as it is still on version 0.xxx (as of 2025-10-10), so there are still likely to be many changes to this software.\nPositron’s assistant provides similar functionality to GitHub Copilot, including:\n\nCode completion and suggestions\nNatural language to code generation\nCode explanation and documentation\n\nTo use Positron with its AI assistant:\n\nDownload and install Positron from https://positron.posit.co/\nFollow the instructions in the Positron assistant documentation to set up the AI assistant\nYou’ll need to configure your AI providers.\n\nAs of writing (2025-10-10) it supports Github Copilot for code completion and chat and Anthropic for chat. So your providers are more limited than other options here.\n\n\n3.2.4 Python users\nI’m not a Python programmer, however, all the principles I’ll teach also apply to Python. You can easily follow on with the examples in Python.\nYou will need to have VSCode with Github Copilot. So I recommend you get the VScode software, then follow these instructions for Python set-up.\nThen follow the instructions for ‘Github Copilot set-up’ (Section 3.6).\nI also recommend you get an API key with OpenRouter (Section 3.4).\nYou don’t need an ellmer equivalent because most of the LLM providers already provide Python code on their webpages.\n\n\n3.2.5 Options 4 +\nThere are innumerable AI coding assistants now available. Feel free to BYO if you are using one that you are already comfortable with. However, I strongly recommend using a tool with IDE integration (meaning it can edit your R/Python scripts directly). See Section 3.9.\nAs a fall-back, you can follow this course via ChatGPT or Copilot, but it will lots of cutting and pasting. Not the integrated experience I want you to have.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Software you'll need for this book and how to set it up</span>"
    ]
  },
  {
    "objectID": "03-set-up.html#software-options-summary",
    "href": "03-set-up.html#software-options-summary",
    "title": "3  Software you’ll need for this book and how to set it up",
    "section": "3.3 Software options summary",
    "text": "3.3 Software options summary\nHere are some of the software options I’ve looked at. Or if you’ve picked your option from the above list, just jump ahead to the required sections.\n\n\n\n\n\n\n\n\n\nOption\nBest for\nPros\nCons\n\n\n\n\nVScode with Github Copilot\nCoders of all abilities who have a fixed budget or just want to try IDE integration\n- Subscription based- Capable free tier- Editor integration (auto-complete style)- Agent mode- Multiple LLMs available- Easy to use\n- VSCode can be hard to set-up for R users- Less flexibility and customization than other agents- The Github Copilot Agent is less automated than other agent software- Limited choice of LLMs (unless you BYO API key)\n\n\nEllmer R package\nR users who want to create their own chat bots, or integrate LLMs into their code and shiny apps. For example, to extract data from a large corpus of papers.\n- Works anywhere R works- Unlimited flexibility- BYO API key, can interact with any LLM- Automate API calls to LLMs\n- Only a basic prompt interface provided (you need to write it)\n\n\nGander R package\nModerately experienced R users who don’t want to leave RStudio.\n- Works with Rstudio- Can see ‘inside of’ R objects, so knows your variable names- Can customize how it works to optimize performance and cost- BYO API key, can interact with any LLM\n- Requires some experience with the R program- Not as deeply integrated with Rstudio as Github Copilot is with VScode (doesn’t have as many features as copilot)\n\n\nAider\nExperienced python programmers\n- Python based AI agent- Interact with the agent via Python code- Highly flexible, can use as an everyday coding assistant or to create bespoke agents- Can edit files as well as run in agent mode\n- Not a straightforward prompt interface- Requires Python coding experience\n\n\nVSCode with Roo Code\nExperienced programmers who want to fully automate coding workflows\n- Fully automated agents- Multiple modes for different behaviours, e.g. architect versus code- Orchestrate mode that can delegate to sub-agents and complete very complex tasks- BYO API key- System messages fully customizable for advanced scientific applications of agents- One of the best performing and most popular agents\n- VSCode can be hard to set-up for R users- Can get more expensive- Does not do inline code editing ‘as you type’ like copilot- Not suitable for novice programmers\n\n\nVSCode with Cline\nSame as Roo Code. The user base for Cline is slightly larger.\n- Fully automated agents- Multiple modes for different behaviours- BYO API key- System messages customizable- Large user base\n- Less options for customizing the system prompt compared to Roo Code- Not suitable for novice programmers\n\n\nPositron with Roo Code or Cline\nR users who don’t want to use VScode. Positron is a fork (copy) of VScode managed by Posit, the group who run RStudio.\n- Possibly a bit easier to set-up R than with VScode- Pros and cons are same as VScode with Roo Code or Cline\n- New software- Limited features and support\n\n\nPositron Assistant\nR users who don’t want to use VScode. Positron is a fork (copy) of VScode managed by Posit, the group who run RStudio.\n- Possibly a bit easier to set-up R than with VScode- Pros R friendly developer. Cons on version 0 so software will likely change\n- New software- Limited features to date\n\n\n\nThere are many other (10s, 100s?) of coding assistants now available. Some prominent examples are Claude code, Cursor and Windsurf. I haven’t tried these, I understand they are similar to Github Copilot. Last time I checked none of them worked with Rstudio.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Software you'll need for this book and how to set it up</span>"
    ]
  },
  {
    "objectID": "03-set-up.html#sec-apikeys",
    "href": "03-set-up.html#sec-apikeys",
    "title": "3  Software you’ll need for this book and how to set it up",
    "section": "3.4 Getting an API key",
    "text": "3.4 Getting an API key\nAn API key is like a password that allows the AI assistant (e.g. roo code) to send your prompt to a large language model. Your key should be kept private. Usually you’ll have to buy some credits. These allow you to send prompts to the LLM. You’ll be paying per prompt.\nNow you need to choose your large language model provider. I’m currently using OpenRouter and Anthropic, which have a diversity of models for generating text, code and reading images. Do some web searching to find out the latest info on providers and models.\nYou choose depends on what you want to do and your budget. Some providers offer a free tier. You’ll need to web search for the latest info on this.\nFor this workshop I strongly recommend you get an OpenRouter API key. This will give you the most flexibility to try different models and to find models that have free options.\nOnce you’ve chosen a provider, create an account and follow their instructions for creating an API key. You will probably also need to buy some credit to use the model.\nThe one caveat is that OpenRouter may not have access the the full capabilities of all LLMs. For example, when Sonnet 3.7 came out you could get vision capabilities via an Anthropic API key but not with an OpenRouter API key.\nImportant the sign-up for getting an API key is often through a different webpage to the sign-up you may be using for subscription based AI tools (e.g. Claude Code or ChatGPT). Here are a couple of key links:\n\nSign-up here to get an API key for OpenRouter to access LLMs from 100s of providers\nSign-up here to get an API key for Anthropic’s LLMs\nSign-up here to get an API key for OpenAI’s LLMs",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Software you'll need for this book and how to set it up</span>"
    ]
  },
  {
    "objectID": "03-set-up.html#sec-vscodesetup",
    "href": "03-set-up.html#sec-vscodesetup",
    "title": "3  Software you’ll need for this book and how to set it up",
    "section": "3.5 VSCode set-up for R users",
    "text": "3.5 VSCode set-up for R users\nVScode has many extensions that let you create and run entire workflows via using prompts to a large language model. Its not widely used in the R community yet, but I expect it will be soon. You can create your entire R project, interpret the results and write a draft of your findings without writing any R code.\nMost of these tools are not available (as of writing) in RStudio, or have only limited functionality. So you need to use a different IDE (Integrated Development Environment) to run your R code. Here I’ll explain how to set-up VSCode (a popular IDE) so you can use Cline.\n\n3.5.1 Software requirements\nTo set up VScode for R and Cline, you’ll need:\n\nR programming language\nVScode text editor\nR extension for VScode\nCline AI assistant extension for VScode\n\nNote that if you computer is controlled centrally by an IT department, you may need to request admin access to install software, or email IT and ask for them to come and help you.\n\n\n3.5.2 Install R\n\nGo to the official R project website: https://www.r-project.org/\nClick the “download R” link in the Getting Started section\nChoose a CRAN mirror close to your location\nDownload the appropriate R installer for your operating system\nRun the installer and follow the prompts to complete installation\n\n\n\n3.5.3 R packages\n\nOpen R or RStudio\nInstall language server install.packages(\"languageserver\")\nInstall httpgd install.packages(\"httpgd\") (this helps improve plots in VScode). NOTE that httpgd seems to often be removed from CRAN, then come back again, I’m not sure why… If you are having trouble you can try install from a different repo, see instructions here: https://community.r-multiverse.org/httpgd\n\n\n\n3.5.4 Install VScode\n\nGo to the official VScode website: https://code.visualstudio.com/\nClick the big blue “Download” button\nDownload the appropriate VScode installer for your operating system\nRun the installer and follow the prompts\nLaunch VScode once installation is complete\n\n\n\n3.5.5 Install R extension\n\nOpen VScode\nOpen the Extensions view in VScode (click the boxes on left hand side)\nSearch for “R” in the extensions marketplace\nSelect the “R” extension published by REditorSupport\nClick the “Install” button\nRestart VScode after installation if prompted\n\nMore info on vscode and R here\n\n\n3.5.6 Connect R and VScode\n\nOpen a new terminal in VScode (Terminal &gt; New Terminal)\nCheck that R is installed by running: R --version\nType R to open the R console in the terminal\nNow open any R script in VS code (File &gt; Open)\nRun some R code to check that VS code can connect to R in the terminal. Use the shortcut Ctrl+Enter/Cmd+Enter or press the play button in the top right of the script editor.\n\nIf R is not found then open extensions (left hand side, boxes icon), filter by ‘enabled’ then click the R extension. Now click the cog icon in the R extension and select ‘settings’ from the dropdown. Search for ‘rpath’. Check that it has the correct path to R on your computer. Also update ‘rterm’ with the same path. You can find the path by opening a terminal and typing which R (on mac) or in a windows terminal where R.\nOne way to fine out where R is installed on your computer is to open Rstudio, go to ‘Tools’ -&gt; ‘General’ then under the field ‘R version’ it will have the path to your R version that may look like this: ‘C:.0’\nCopy that path from Rstudio. Then go back to the VScode settings and paste it into the ‘rpath’ and ‘rterm’ settings. You will also need to add the R.exe on the end like this: ‘C:/Users/myname/AppData/Progams/R/R-4.5.0/R.exe’\nNow open an R script and run it (e.g. press cmd/cntrl-enter). If everything is connected R will open and it will run the script. If its not working you’ll get an error pop-up. Go back and check the steps above.\nIf you copied it on windows you may also need to reverse the slashes, like I have above.\nWhile you have the extension settings open search for ‘httgp’ and make sure Plot: Use Httpgd is enabled.\n\n\n3.5.7 Issues and tips for VScode with R\nThis is just a list of issues I’ve had and how I’ve solved them.\nPlotting If your R plots look weird (like tiny font), make sure httpgp is enabled. Go back to steps above and see how to do that.\nViewing data There are various extensions for viewing csv and excel files. It is worth looking into these so that when you do View(dat) in R you get a nice table. Some also allow editing.\nGetting help to install software My computer is somewhat locked down by IT, so getting this set-up was a bit fiddly and required a few requests to IT to install software.\nR markdown There are options in the R extension settings for how to knit markdown. You may need to configure these if you want to knit markdown docs from VScode. If you are having trouble knitting markdown it may mean that the path to pandoc is not set correctly. There is some helpful instructions here\nR terminal crashes If I run too much R code at once (like selecting a big block then running) the terminal tends to crash. Initially I see a little highlighted box saying ‘PTY HOST’. Then I need to close all the terminals (with the bin icon) and start again. Try radian if this is a problem. You can also code run line-by-line or source whole scripts from the terminal (which works fine). I tried debugging this by increasing the buffer but to on avail.\nShortcut keys (on osx) cmd-/ to comment uncomment lines. cmd-shift-p to open the command palette, cmd-b to open the file explorer, cmd-enter to run lines or selection of R code, cmd-shift-c to open terminal in new window, cntrl-shift-` to open a new terminal in vs code.\n\n\n3.5.8 Installing radian (optional)\nRadian is a terminal editor that is a bit nicer than the base R one. It does autocomplete in the terminal (like Rstudio does in the console), colours code/brackets etc… and allows multi-line editing in the terminal.\nTo set this up, install radian (you need python to do this). More instructions here.\nThen go to the terminal and find the path where radian is installed (e.g. which radian on mac or where radian on windows).\nNow open your settings in VScode (cmd-,) and search for ‘rterm’ (stands for ‘R Terminal’, don’t change the rpath which we set just before). Add the path to radian to the rterm setting. Also search for the setting ‘R: Bracketed Paste’ and make sure it is enabled.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Software you'll need for this book and how to set it up</span>"
    ]
  },
  {
    "objectID": "03-set-up.html#sec-githubcopilot",
    "href": "03-set-up.html#sec-githubcopilot",
    "title": "3  Software you’ll need for this book and how to set it up",
    "section": "3.6 Github Copilot set-up",
    "text": "3.6 Github Copilot set-up\nOnce you have VSCode just follow the simple steps here. Note if you already have a Github account make sure you have your login information handy.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Software you'll need for this book and how to set it up</span>"
    ]
  },
  {
    "objectID": "03-set-up.html#sec-ellmer",
    "href": "03-set-up.html#sec-ellmer",
    "title": "3  Software you’ll need for this book and how to set it up",
    "section": "3.7 Ellmer set-up",
    "text": "3.7 Ellmer set-up\nFirst, you need to get an API key from the provider, see step above on ‘Getting an API key’.\nThen, you need to add the key to your .Renviron file:\nusethis::edit_r_environ()\nThen type in your key like this:\nOPENROUTER_API_KEY=“xxxxxx”\nIf you are using open AI it would be like this:\nOPENAI_API_KEY=“xxxxxx”\ni.e. use the appropriate name for the provider you are using.\nThen restart R. ellmer will automatically find your key so long as you use the recommended environment variable names. See ?ellmer::chat_openrouter (or chat_xxx where xxx is whatever provider you are using).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Software you'll need for this book and how to set it up</span>"
    ]
  },
  {
    "objectID": "03-set-up.html#sec-gander",
    "href": "03-set-up.html#sec-gander",
    "title": "3  Software you’ll need for this book and how to set it up",
    "section": "3.8 Gander: Rstudio friendly alternative to copilot",
    "text": "3.8 Gander: Rstudio friendly alternative to copilot\nGander lets you select code in Rstudio and edit it directly through calls to an LLM.\nFollow the steps to set-up Ellmer first.\nThen you can install.packages(gander)\nNow, you need to do two steps before Gander will work. First you need to add your default LLM to the .Rprofile so Gander knows what LLM to use type: usethis::edit_r_profile()\nNow set the option for your LLM, e.g. \noptions(.gander_chat = ellmer::chat_anthropic()) or options(.gander_chat = ellmer::chat_openai())\nNow add a keyboard short-cut to use Gander. Go to the ‘Tools’ menu at the top, click ‘Add-ins’, ‘Browse Add-ins’, then click the ‘Keyboard shortcuts’ button. Find the row for Gander (probably at the bottom), click in the second column for ‘Shortcut’ and type your keyboard shortcut. I used cmd-i (or cntrl-i). Then click ‘Cancel’ once you’ve set the shortcut key (‘Cancel’ as we don’t want to run Gander right now).\nIf for some reason the shortcut key won’t you can also gander::gander_addin() at anytime from R.\nNow to check it works, open an R project and select a line of R code. Press your shortcut keys. A box should pop up. Type a prompt (like ‘make this code more fun’) and hit enter to test it does something.\nI recommend reading the gander reference as there are ways you can customize it to know your data but also save on tokens.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Software you'll need for this book and how to set it up</span>"
    ]
  },
  {
    "objectID": "03-set-up.html#sec-otheroptions",
    "href": "03-set-up.html#sec-otheroptions",
    "title": "3  Software you’ll need for this book and how to set it up",
    "section": "3.9 Installing other gen AI VSCode extensions",
    "text": "3.9 Installing other gen AI VSCode extensions\ne.g. if you want to try Roo Code or Cline.\n\nOpen the Extensions view in VScode (Ctrl+Shift+X)\nSearch for the genAI assistant of your choice. I’m use Roo Code currently. Cline is another popular choice.\nSelect the extension\nClick the “Install” button\nThe extension icon (e.g. a Roo if using Roo Code) should appear in the VScode sidebar (left side).\n\nYou’ll have to navigate the extensions menus to input your API key. It won’t work without an API key that gives you access to an LLM. e.g. for Roo Code:\n\nClick on the extension icon (e.g. a roo for roo code or robot for cline) on the left hand side\nClick the cog (if the settings don’t open automatically)\nSelect your API provider and cut and paste the API key into the box.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Software you'll need for this book and how to set it up</span>"
    ]
  },
  {
    "objectID": "04-llm-from-api.html",
    "href": "04-llm-from-api.html",
    "title": "4  Calling LLMs via an Application Programming Interface",
    "section": "",
    "text": "4.1 What is an Application Programming Interface?\nAn Application Programming Interface (API) lets different software programs communicate with each other.\nLarge Language Models often require high powered computers to run. It is possible to have one on your computer (called local hosting), but here we will look at using LLMs hosted with ‘Providers’, these are servers remote to our computer.\nAn API allows you to send text prompts to an LLM provider (like OpenAI’s GPT models or Anthropic’s Claude) over the internet and receive the model’s response back to your code. This is different from using a chat interface like ChatGPT’s website, where you’re interacting through a user interface designed for humans.\nIn fact, locally hosted LLMs also use an API to manage requests from your computer to the LLM software.\nHow API calls work: 1. Your code sends a request to the API endpoint (a web address) 2. The request includes your prompt, model choice, and parameters 3. The LLM service processes your request 4. The service sends back a structured response containing the model’s output 5. Your code can then use this response in further analysis or processing\nMost LLM APIs use a similar structure - you provide messages (like system prompts and user questions), specify which model to use, and set parameters to control the response. The API then returns the model’s generated text along with metadata about the request.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calling LLMs via an Application Programming Interface</span>"
    ]
  },
  {
    "objectID": "04-llm-from-api.html#setup-authorisation",
    "href": "04-llm-from-api.html#setup-authorisation",
    "title": "4  Calling LLMs via an Application Programming Interface",
    "section": "4.2 Setup authorisation",
    "text": "4.2 Setup authorisation\nGet your API key, see Chapter 3 for how to get an API key and connect Ellmer to an LLM provider.\nIf you are using Python, save your API key in a .env file in your project directory, like this:\n`OPENROUTER_API_KEY=my-key-here",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calling LLMs via an Application Programming Interface</span>"
    ]
  },
  {
    "objectID": "04-llm-from-api.html#understanding-how-llms-work",
    "href": "04-llm-from-api.html#understanding-how-llms-work",
    "title": "4  Calling LLMs via an Application Programming Interface",
    "section": "4.3 Understanding how LLMs work",
    "text": "4.3 Understanding how LLMs work\nLarge Language Models (LLMs) operate by predicting the next token in a sequence, one token at a time. To understand how this works in practice, we’ll call an API directly through computer code.\nBy accessing the API in this way we get as close to the raw LLM as we are able. Later on we will use ‘coding assistants’ (e.g. copilot) which put another layer of software between you and the LLM.\n\n4.3.1 Calling LLMs via an API\nTry to get it to complete a sentence:\n\nR - EllmerRPython\n\n\n\nlibrary(ellmer)\n\n# Initialize a chat with Claude\nchat &lt;- chat_openrouter(\n  system_prompt = \"You are a helpful assistant.\",\n  model = \"anthropic/claude-3.5-haiku\",\n  api_args = list(max_tokens = 50)\n)\nchat$chat(\"Ecologists like to eat \")\n\n\n\n\nlibrary(jsonlite)\nlibrary(httr)\nopenrouter_api_key &lt;- Sys.getenv(\"OPENROUTER_API_KEY\")\nresponse &lt;- POST(\n    url = \"https://openrouter.ai/api/v1/chat/completions\",\n    add_headers(\n      \"Content-Type\" = \"application/json\",\n      \"Authorization\" = paste(\"Bearer\", openrouter_api_key)\n    ),\n    body = toJSON(list(\n      model = \"anthropic/claude-3.5-haiku\",\n      messages = list(\n        list(\n          role = \"system\",\n          content = \"You are a helpful assistant.\"\n        ),  \n        list(\n          role = \"user\",\n          content = \"Ecologists like to eat \"\n        )\n      )\n    ), auto_unbox = TRUE),\n    encode = \"raw\"\n  )\nr3 &lt;- fromJSON(content(response, \"text\"))\nr3$choices$message$content\n\n\n\n\nimport requests\nimport json\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\napi_key = os.getenv(\"OPENROUTER_API_KEY\")\nmodel = \"anthropic/claude-3.5-haiku\"\nmessage = \"Ecologists like to eat \"\n\nresponse = requests.post(\n  url=\"https://openrouter.ai/api/v1/chat/completions\",\n  headers={\n    \"Authorization\": f\"Bearer {api_key}\",\n    \"Content-Type\": \"application/json\"\n  },\n  data=json.dumps({\n    \"model\": model,\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\",\n      },\n      {\n        \"role\": \"user\",\n        \"content\": message,\n      }\n    ]\n  })\n)\ndata = response.json()\ncontent = data['choices'][0]['message']['content']\ncontent\n\n\n\n\nNotice that the model doesn’t do what we intend, which is complete the sentence. LLMs have a built in command to be an assistant. Let’s use the ‘system prompt’ to provide it with strong directions.\n\nTip: The system prompt sets the overall context for a chat. It is meant to be a stronger directive than the user prompt. In most chat interfaces (e.g. copilot) you are interacting with the user prompt. The provider has provided the system prompt, here’s the system prompt for the chat interface version of anthropic (Claude)\n\n\nR - EllmerRPython\n\n\n\nchat &lt;- chat_openrouter(\n  system_prompt = \"Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides\",\n  model = \"anthropic/claude-3.5-haiku\",\n  api_args = list(max_tokens = 50)\n)\nchat$chat(\"Ecologists like to eat \")\n\n\n\n\nlibrary(jsonlite)\nlibrary(httr)\nopenrouter_api_key &lt;- Sys.getenv(\"OPENROUTER_API_KEY\")\nresponse &lt;- POST(\n    url = \"https://openrouter.ai/api/v1/chat/completions\",\n    add_headers(\n      \"Content-Type\" = \"application/json\",\n      \"Authorization\" = paste(\"Bearer\", openrouter_api_key)\n    ),\n    body = toJSON(list(\n      model = \"anthropic/claude-3.5-haiku\",\n      messages = list(\n        list(\n          role = \"system\",\n          content = \"Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides\"\n        ),  \n        list(\n          role = \"user\",\n          content = \"Ecologists like to eat \"\n        )\n      ),\n      max_tokens = 50\n    ), auto_unbox = TRUE),\n    encode = \"raw\"\n  )\nr3 &lt;- fromJSON(content(response, \"text\"))\nr3$choices$message$content\n\n\n\n\nimport requests\nimport json\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\napi_key = os.getenv(\"OPENROUTER_API_KEY\")\nmodel = \"anthropic/claude-3.5-haiku\"\nmessage = \"Ecologists like to eat \"\nsystem_prompt = \"Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides\"\n\nresponse = requests.post(\n  url=\"https://openrouter.ai/api/v1/chat/completions\",\n  headers={\n    \"Authorization\": f\"Bearer {api_key}\",\n    \"Content-Type\": \"application/json\"\n  },\n  data=json.dumps({\n    \"model\": model,\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": system_prompt,\n      },\n      {\n        \"role\": \"user\",\n        \"content\": message,\n      },\n    \"max_tokens\": 50\n    ]\n  })\n)\ndata = response.json()\ncontent = data['choices'][0]['message']['content']\n\n\n\n\n\nTip: It is generally more effective to tell the LLM what to do rather than what not to do (just like people!).\n\n\n\n4.3.2 Controlling randomness of the responess\nThe “temperature” parameter controls the randomness of token predictions. Lower temperatures (closer to 0) make the model more deterministic, while higher temperatures (closer to 2) make it more creative and unpredictable.\nEven at 0 there is still a small amount of randomness. We can also try to make a response more deterministic with the ‘top_k’ parameter. Top K forces the model to choose among a smaller set of tokens, setting it to 1 will give the most predictable results\nLet’s compare responses with different temperatures and top K settings:\n\nR - EllmerRPython\n\n\n\n# Create chats with different temperature settings\nchat_temp &lt;- chat_openrouter(\n          system_prompt = \"Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides\",\n        model = \"anthropic/claude-3.5-haiku\",\n        api_args = list(max_tokens = 50, temperature = 0, top_k = 1)\n    )\n\nchat_temp$chat(\"Marine ecologists like to eat \")\n\n\n\n\nlibrary(jsonlite)\nlibrary(httr)\nopenrouter_api_key &lt;- Sys.getenv(\"OPENROUTER_API_KEY\")\nresponse &lt;- POST(\n    url = \"https://openrouter.ai/api/v1/chat/completions\",\n    add_headers(\n      \"Content-Type\" = \"application/json\",\n      \"Authorization\" = paste(\"Bearer\", openrouter_api_key)\n    ),\n    body = toJSON(list(\n      model = \"anthropic/claude-3.5-haiku\",\n      messages = list(\n        list(\n          role = \"system\",\n          content = \"Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides\"\n        ),  \n        list(\n          role = \"user\",\n          content = \"Marine ecologists like to eat \"\n        )\n      ),\n      max_tokens = 50,\n      temperature = 0,\n      top_k = 1\n    ), auto_unbox = TRUE),\n    encode = \"raw\"\n  )\nr3 &lt;- fromJSON(content(response, \"text\"))\nr3$choices$message$content\n\n\n\n\nimport requests\nimport json\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\napi_key = os.getenv(\"OPENROUTER_API_KEY\")\nmodel = \"anthropic/claude-3.5-haiku\"\nmessage = \"Marine ecologists like to eat \"\nsystem_prompt = \"Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides\"\n\nresponse = requests.post(\n  url=\"https://openrouter.ai/api/v1/chat/completions\",\n  headers={\n    \"Authorization\": f\"Bearer {api_key}\",\n    \"Content-Type\": \"application/json\"\n  },\n  data=json.dumps({\n    \"model\": model,\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": system_prompt,\n      },\n      {\n        \"role\": \"user\",\n        \"content\": message,\n      }\n    ],\n    \"max_tokens\": 50,\n    \"temperature\": 0,\n    \"top_k\": 1\n  })\n)\ndata = response.json()\ncontent = data['choices'][0]['message']['content']\ncontent\n\n\n\n\nNow try again but set the temperature parameter higher, say to 2 and remove the top_k line, or set it to a high number like 1000\nAt low temperatures and top K values, you’ll notice the model consistently produces similar “safe” completions that focus on the most probable next tokens. As temperature increases, the responses become more varied and potentially more creative, but possibly less coherent.\n\nThere are quite a few different parameters you can play with to control how an LLM responds. The names and meanings of these are all documented in the Openrouter docs.\n\n\n\n4.3.3 Navigating Openrouter\nI created a lot of the code for this tutorial by browsing the Openrouter page to see what is possible.\nThere are two key pages.\nThe Models page lists all the models available. If you have some idea of what you want you can filter this list. Click the page icon to copy the string for the model id. You can use this to replace the string in model = \"anthropic/claude-3.5-haiku\" if you want to use a different model.\nClick the model’s name to get more information about it.\nA single model may be available through multiple providers (servers), these will also be listed here. Openrouter automatically picks the provider for you. Their docs tell you how to fix the provider if you want to default to a specific choice (e.g. if you are concerned about some providers for data security of ethical reasons, more on this later).\nClick a provider then the drop-down arrow to see what parameter choices are available for this model (note not all providers support all parameters for a model).\nThe second important page is the docs page. This has all the information on using the openrouter API. There are lost of code examples, usually in Python. You can use an AI assistant to translate these to R if you are using R.\nFor example, there is a long list of parameters that various LLMs support. If you want to try any of these just add them using the same syntax as we did for the temperature parameter. Note that not all LLMs support all parameters. To find out what parameters a model supports go to the model’s provider page and expand a specific provider to see a list of Supported Parameters.\n\n4.3.3.1 API errors.\nSometimes you won’t get a response instead you’ll get an HTTP error. After you send your response you may get an answer something like Error: 402.\nAll the error codes are listed here. Some common errors codes:\n400 There’s an error in your code.\n401 API key is invalid (did you copy and paste it correctly? Does it need “” around it?)\n402 A 402 means you’ve run out of API credits and need to put more money on your openrouter account against that API key.\n408 Time out error, your request might be too big for the speed of your internet connection.\n429 Wow you are really a power user! You’ve been rate limited, meaning you’re calling the LLM too much, too quickly. Just wait a bit and try again (or add pauses in your code to slow it down if you are doing automated repeat requests).\n\n\n\n4.3.4 Comparing model complexity\nDifferent models have different capabilities based on their size, training data, and architecture.\nFor example anthropic/claude-3.5-haiku has many fewer parameters than anthropic/claude-sonnet-4. This means that the latter model is more complex and can handle more nuanced tasks. However, haiku is significantly cheaper to run. Haiku is 80c per million input tokens vs $3 for Sonnet. Output tokens are $4 vs $15\nTask for you: use the Openrouter models page to find a different model to try.\nHint try some of the Chinese models like Kimi instead of the American models.\n\n\n4.3.5 Understanding context windows\nLLMs have a limited “context window” - the amount of text they can consider when generating a response. This affects their ability to maintain coherence over long conversations. For most LLMs this is about 100-200K tokens, which includes input and output. However, Google’s models have up to 1 million + tokens.\nWe’ll come back to the context window when we explore more advanced tools with longer prompts. These simple prompts don’t come close to using up the context window.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calling LLMs via an Application Programming Interface</span>"
    ]
  },
  {
    "objectID": "04-llm-from-api.html#diy-stats-bot",
    "href": "04-llm-from-api.html#diy-stats-bot",
    "title": "4  Calling LLMs via an Application Programming Interface",
    "section": "4.4 DIY stats bot",
    "text": "4.4 DIY stats bot\nLet’s put together what we’ve learnt so far and built our own chatbot. I’ve provided you with a detailed system prompt that implements a chat bot that specialises in helping with statistics. First, we read the bot markdown file from github, then we can use it in our chat session.\n\nR - EllmerRPython\n\n\n\nstats_bot &lt;- readr::read_file(url(\"https://raw.githubusercontent.com/cbrown5/AI-assistants-for-scientific-coding/refs/heads/main/resources/DIY-stats-bot-system.md\"))\n\nchat_stats &lt;- chat_openrouter(\n  system_prompt = stats_bot,\n  model = \"anthropic/claude-sonnet-4\",\n  api_args = list(max_tokens = 5000)\n)\n\nchat$chat(\"Who are you?\")\n\n\n\n\nlibrary(jsonlite)\nlibrary(httr)\nopenrouter_api_key &lt;- Sys.getenv(\"OPENROUTER_API_KEY\")\nstats_bot &lt;- readLines(\"https://raw.githubusercontent.com/cbrown5/AI-assistants-for-scientific-coding/refs/heads/main/resources/DIY-stats-bot-system.md\")\nresponse &lt;- POST(\n    url = \"https://openrouter.ai/api/v1/chat/completions\",\n    add_headers(\n      \"Content-Type\" = \"application/json\",\n      \"Authorization\" = paste(\"Bearer\", openrouter_api_key)\n    ),\n    body = toJSON(list(\n      model = \"anthropic/claude-sonnet-4\",\n      messages = list(\n        list(\n          role = \"system\",\n          content = paste(stats_bot, collapse=\"\\n\")\n        ),\n        list(\n          role = \"user\",\n          content = \"Who are you?\"\n        )\n      ),\n      max_tokens = 5000\n    ), auto_unbox = TRUE),\n    encode = \"raw\"\n  )\nr3 &lt;- fromJSON(content(response, \"text\"))\nr3$choices$message$content\n\n\n\n\nimport requests\nimport json\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\napi_key = os.getenv(\"OPENROUTER_API_KEY\")\nmodel = \"anthropic/claude-sonnet-4\"\nimport requests\nstats_bot = requests.get(\"https://raw.githubusercontent.com/cbrown5/AI-assistants-for-scientific-coding/refs/heads/main/resources/DIY-stats-bot-system.md\").text\n\nresponse = requests.post(\n  url=\"https://openrouter.ai/api/v1/chat/completions\",\n  headers={\n    \"Authorization\": f\"Bearer {api_key}\",\n    \"Content-Type\": \"application/json\"\n  },\n  data=json.dumps({\n    \"model\": model,\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": stats_bot,\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Who are you?\",\n      }\n    ],\n    \"max_tokens\": 5000\n  })\n)\ndata = response.json()\ncontent = data['choices'][0]['message']['content']\n\n\n\n\nTry asking it some different statistical questions and note how it responds.\n\nTip: How many of you started using “DIY-stats-bot-system.md” without first reading it? Did you find the easter egg in my prompt? For security you should ALWAYS read prompts before you start running them through LLM chats. We’ll see later that LLMs can be given ‘tools’ which allow them to run code on your computer. Its easy to see how a malicious prompt could mis-use these tools. We’ll cover security later.\n\n\n4.4.1 Multi-turn conversation\nYou might want to have a conversation with your chat bot to help you problem solve. This is easy with Ellmer and a bit fiddly with python or base R. I’m only going to cover Ellmer here.\n\nR - EllmerRPython\n\n\n\nstats_bot &lt;- readr::read_file(url(\"https://raw.githubusercontent.com/cbrown5/AI-assistants-for-scientific-coding/refs/heads/main/resources/DIY-stats-bot-system.md\"))\n\nchat_stats &lt;- chat_openrouter(\n  system_prompt = stats_bot,\n  model = \"anthropic/claude-sonnet-4\",\n  api_args = list(max_tokens = 5000)\n)\n\nlive_console(chat_stats)\n# live_browser(chat_stats)\n\nlive_console let’s you chat in the terminal. live_browser opens up a browser window that is hosted locally on your computer. It feels the most like chatGPT.\n\n\nI suggest using ellmer for multi-turn conversations. If Ellmer doesn’t work, don’t worry we’ll do more on this with github copilot in a moment.\n\n\nThere are some packages available to help with multi-turn conversation. For example the chat-cli. These take some time to set-up so we won’t go over them here, read the docs for more information.\n\n\n\n\nTip: LLMs performance is better if you put everything in an initial single questions, rather than a multi-turn conversation. Responses to a problem split into multiple questions are less accurate than responses to problem that is put up front in the initial prompt. This rule applies across many domains from logic to coding. I like to think of it as mansplaining, the LLMs have (sub-concioulsy?) been designed to prefer being mansplained over having a conversation. We’ll look more at this later.\n\n\n\n4.4.2 Improving the stats bot\nMake a local copy of the stats bot system prompt and try editing it. You can use your coding skills to write the stats bot to file, or just go here and copy the text: https://raw.githubusercontent.com/cbrown5/AI-assistants-for-scientific-coding/refs/heads/main/resources/DIY-stats-bot-system.md.\nTry different commands within it and see how your chat bot responds (you’ll have to open a new chat object each time).\nHere’s some ideas.\n\nTry making a chat bot that is a verhment Bayesian that abhors frequentist statistics.\n\nYou could provide it with more mode-specific instructions. For instance, try to get the chatbot to suggest appropriate figures for verifying statistical models.\nTry different temperatures.\nAdd your own easter egg.\n\n\nTip: Adjectives, CAPITALS, *markdown* formatting can all help create emphasis so that your model more closely follows your commands. I used ‘abhors’ and ‘verhment’ above on purpose.\n\n\n\n4.4.3 Tools\nTools like Copilot Agent mode then go a step further and send the results of step 5 back to the LLM, which then interprets the results and the loop continues (sometimes with and sometimes without direct user approval).\nIf you want to go further with making your own tools, then I suggest you check out ellmer package. It supports tool creation in a structured way. For instance, I made a tool that allows an LLM to download and save ocean data to your computer.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calling LLMs via an Application Programming Interface</span>"
    ]
  },
  {
    "objectID": "04-llm-from-api.html#reflection-on-prompting-fundamentals",
    "href": "04-llm-from-api.html#reflection-on-prompting-fundamentals",
    "title": "4  Calling LLMs via an Application Programming Interface",
    "section": "4.5 Reflection on prompting fundamentals",
    "text": "4.5 Reflection on prompting fundamentals\nTo recap, the basic workflow an agent follows is:\n\nSet-up a system prompt with detailed instructions for how the LLM should format responses\nUser asks a question that is sent to the LLM\nLLM responds and sends response back to user\nSoftware on user’s computer attempts to parse and act on the response according to pre-determined rules\nUser’s computers enacts the commands in the response and provides results to user\n\nThe key things I hoped you learnt from this lesson are: Basic LLM jargon, including tokens, temperature, API access and different LLM models.\nNow you understand the basics, let’s get into Github Copilot.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calling LLMs via an Application Programming Interface</span>"
    ]
  },
  {
    "objectID": "05-github-copilot.html",
    "href": "05-github-copilot.html",
    "title": "5  Github copilot for R",
    "section": "",
    "text": "5.1 What to do if you are not using R with VScode\nIf you are using Python in VScode, then just use the same dataset I have below, but write in a .py script. Copilot can help you if you get stuck with anything.\nIf you are using R in Rstudio. You can enable copilot with inline completions from Rstudio (if you have github copilot account). See Chapter 3, Option 2 for how to do that.\nOnce we get to the Inline Code Generation section you can use gander as described in Chapter 3.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Github copilot for R</span>"
    ]
  },
  {
    "objectID": "05-github-copilot.html#inline-code-editing",
    "href": "05-github-copilot.html#inline-code-editing",
    "title": "5  Github copilot for R",
    "section": "5.2 Inline code editing",
    "text": "5.2 Inline code editing\nThis chapter explores techniques for using GitHub Copilot’s inline code editing capabilities to enhance your R programming workflow.\n\n5.2.1 1. Code completion\nThis is the only option supported in Rstudio (last time I checked).\nAssuming you have github copilot set-up you just need to start a new R script (remember to keep it organized and give it a useful name) and start typing. You’ll see suggested code completions appear in grey. Hit tab to complete them.\nLet’s read in the benthic site data and fish counts:\n\nlibrary(tidyverse)\nlibrary(readr)\n\ndat &lt;- read_csv(url(\"https://raw.githubusercontent.com/cbrown5/example-ecological-data/refs/heads/main/data/benthic-reefs-and-fish/fish-coral-cover-sites.csv\"))\n\nhead(dat)\nsummary(dat)\n\nMore details on this data are available here.\nNow try create a ggplot of secchi (a measure of water clarity, higher values mean clearer water) and pres.topa (count of topa, the bumphead parrotfish). Start typing gg and see what happens.\nYou should get a recommendation for a ggplot. But it won’t know the variable names.\n\nTip: Sometimes GC gets stuck in a loop and keeps recommending the same line. To break it out of the loop try typing something new.\n\n\n\n5.2.2 2. Using comments\nThe code completion is using your script and all open scripts in VScode to predict your next line of code. It won’t know the variable names unless you’ve provided that. One way is to include them in the readme.md file and have that open, another is to use comments in the active script (which tends to work more reliably), e.g.\n# Make a point plot of secchi against pres.topa\ngg...\nShould get you the write ggplot. Using variable names in your prompts is more precise and will help the LLM guess the right names.\nYou could also try putting key variable names in comments at the top of your script.\nAnother way to use autocomplete is not to write R at all, just to write comments and fix the R code. Try templating a series of plots like:\n# Make a point plot of secchi against pres.topa with a stat_smooth\n\n# Plot logged (two categories) and pres.topa as a boxplot\n\n# Plot CB_cover (branching coral cover) against secchi\n\nTip: Remember one way to improve prompts is to be more specific. So using the correct variable names helps.\n\nNow go back through and click under each line to get the suggestions.\nThis strategy is great in data wrangling workflows. As a simple example try make this grouped summary using comments only:\n\ndat %&gt;%\n    group_by(logged) %&gt;%\n    summarize(mean_topa = mean(pres.topa), \n                mean_CB = mean(CB_cover))\n\nTo make this I might write this series of comments:\n    #group dat by logged \n    #summarize pres.topa and CB_cover\nIf the variable names are documented above you can ofter be lazier and less precise with variable names here.\n\n\n5.2.3 3. Code completion settings\nClick the octocat in the bottom right corner of VScode to fine-tune the settings. You can enable/disable code completions (sometimes they are annoying e.g. when writing a workshop!).\nYou can also enable ‘next edit suggestions’. These are useful if editing an exisiting file. e.g. if you misspelt ‘sechi’ then updated it in one place, it will suggest updates through the script. Hit tab to move through these.\nThe box will also tell you if indexing is available. Indexing allows AI to search your code faster.\n\n\n5.2.4 4. Inline code generation\nIn VScode you can also access an inline chat box with cmd/cntrl-i. This chat can chat as well as edit code.\nYou can click anywhere and active this. I find it most useful though to select a section of code and then hit cmd/cntrl-i.\nThis is most useful to - Add new code - Explain code - Fix bugs - Add tests\nTry select some of your code (e.g. a ggplot) and ask it to explain what the code does.\nNow try select one of your plots and ask for some style changes (e.g. theme, colours, axes label sizes etc…).\nNow add a bug into one of your plots. See if the inline chatbox can fix the bug.\n\n5.2.4.1 Prompt shortcuts\nUse the / to bring up a list of prompt shortcuts. The most useful in R are /explain, /fix, /tests. Try select some code then use these to see what happens.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Github copilot for R</span>"
    ]
  },
  {
    "objectID": "05-github-copilot.html#ask-mode",
    "href": "05-github-copilot.html#ask-mode",
    "title": "5  Github copilot for R",
    "section": "5.3 Ask mode",
    "text": "5.3 Ask mode\nAsk mode is a chatbot that has access to context from your active project. It is great to help you plan analysis and implementation, using context from your project. It is also handy to help explain code.\nIn VScode click the ‘octocat’ symbol that should be at the top towards the right. This will open the chat window.\nThe chat panel will appear down the bottom of this new sidebar. Confirm that the chatbot is currently set to ‘Ask’ mode.\nYour current file will automatically be included as context for the prompt. You can drag and drop any other files here as well.\nStart by asking the chatbot for guidance on a statistical analysis. We are interested in how the abundance of Topa relates to coral cover. For instance you could ask:\nHow can I test the relationship between pres.topa and CB_cover?\nEvaluate the quality of its response and we will discuss.\n\n5.3.1 Improving our initial prompt by attaching data\nRecall our initial prompt was:\nHow can I statistically test the relationship between pres.topa and CB_cover?\nTry some of the strategies above (make a new prompt by clicking the + button) and compare the quality of advice.\nFor instance, you can save a data summary like this:\n\nwrite_csv(head(dat), \"resources/head-site-level-data.csv\")\n\nThen drag and drop it into the ask window and add something like:\nHow can I statistically test the relationship between pres.topa and CB_cover? Here are the first 6 rows of data\n\n\n5.3.2 Improving our initial prompt by attaching domain knowledge\nYou can further improve the response by attaching a trusted resource. e.g. save this webpage on count models for ecology to your computer. Then you can attach the html file.\nHowever, html files are full of code as well as the text we want. It can be more efficient for the LLM if you convert the html to markdown. Here’s a handy browser operated tool that will convert an arbitrary webpage to markdown:\nhttps://tools.simonwillison.net/jina-reader\nJust paste in the url above (https://environmentalcomputing.net/statistics/glms/glm-2/) to get a markdown version. Copy that to a local file in your project and attach it to ask mode.\nBe sure to check the markdown to make sure its safe and doesn’t contain malicious instructions.\nAnother option is to use a websearch tool. You can install one with github copilot. However, I’m not recommending this at this point, due to cyber-security concerns, we’ll disucss this further later on.\nThat worked well for me. I then followed up with:\nGreat. Evaluate the robustness of each suggestoin on a 1-10 scale\nAnd it gave me a nice summary suggesting to try overdispersion models first (which is a good suggestion).\nThe absolute best practice would be to give the assistant all the context for your study and observational design. Let’s see how doing that can work in our favour when planning implementation.\n\n\n5.3.3 Planning implementation\nAnother way to use Ask mode is for help in implementing an analysis. Many of our workflows are complex and involve multiple data wrangling steps.\nTo get the best out of GC I recommend creating a detailed README.md file with project context. Let’s try that and use it to plan our project.\nSave the README.md that his here to a local file. (Remember that we are going to be using this as a prompt, so read it first).\nNow you can attach it (or open it then click new chat). Given all the context you’ve provided you can just write something simple like:\nHelp me plan R code to implement this analysis. \nOr\nHelp me plan the workflow and scripts to implement this analysis\nI did this. It suggested both code (that looked approximatley correct) and the directory structure, sticking to my guideline in the readme about being modular.\nYou should iterative with Ask mode to if there are any refinements you want.\nLet’s move onto edit mode to see how to put this plan to action.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Github copilot for R</span>"
    ]
  },
  {
    "objectID": "05-github-copilot.html#edit-mode-creating-code-from-text",
    "href": "05-github-copilot.html#edit-mode-creating-code-from-text",
    "title": "5  Github copilot for R",
    "section": "5.4 Edit mode: Creating code from text",
    "text": "5.4 Edit mode: Creating code from text\nEdit mode will edit files for you. The best way to learn how is to just see it in action.\nOpen the Chat panel and click the ‘Ask’ button, then select ‘Edit’.\nNow go back to your R file with the plots in it. In edit mode suggest a new plot such as:\nCreate a pairs plot for all continuous variables \n\nTip: Sometimes you can’t go back once copilot has made edits to a file. So its good practice to use git and commmit changes before and after editing.\n\nTry creating some other plots or analyses using edit mode.\n\n5.4.1 Adding documentation with edit mode\nEdit mode can write text as well as code. It can be handy to use for documentation for instance.\nOpen the README.md. Then type this prompt:\nDocument what this script does in the readme. \nDrag and drop your script into the chat window as well.\nClick ‘Keep’ if you like what it did. Or you can suggest improvements. Alternatively, accept it for now and then edit it afterwards.\nAnother way to use edit mode is for translation. Try getting it to translate part of your code into another programming language, or part of your text into another language (Spanish and French are the best options at the moment).\n\n5.4.1.1 Why so much code?\nCopilot is designed as a programming assistant. We don’t know its system message, but given the main market for this software is professional programmers, we can guess it has a strong emphasis on programming robust code.\nYou might notice that copilot tend to ‘over-engineer’ your R scripts. For instance, it has a tendancy to make an if statement to check if each new package needs installing, before loading it.\nIf you don’t like this style you can add a statement to the readme asking it to keep implementation simple.\n\n\n\n5.4.2 Workflows and tips for edit mode\nRemember its an assistant, its not doing the project for you. So you need to make sure it stays on track. Left unattended (if you just accept, accept, accept without reading) it can go down rabbit holes. Sometimes it creates superfluous analyses or even incorret statistics.\nSo here’s how I recommend you use it:\n\nUse git for version control so you can go back in to older versions.\nRead the suggested edits before accepting\nNEVER edit the file while copilot is working! To edit files it uses string matching to locate the position to insert the edits. If you change the file it may not find the correct place to insert the new code.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Github copilot for R</span>"
    ]
  },
  {
    "objectID": "05-github-copilot.html#agent-mode-automated-workflows",
    "href": "05-github-copilot.html#agent-mode-automated-workflows",
    "title": "5  Github copilot for R",
    "section": "5.5 Agent mode: Automated workflows",
    "text": "5.5 Agent mode: Automated workflows\nAgents are LLMs that have tools that allow them to work autonomously. In effect they review the results of tool use (such as writing code and running code), then respond to those results.\nIn Copilot’s chat window you can set it to ‘Agent’ mode to enable these features.\nAfter each tool use copilot will ask you to confirm the changes and the next action. At that point you can review its changes, make edits, or continue chatting to suggest refinements.\n\nImage: Agent mode from https://code.visualstudio.com\nAgent mode has access to the terminal, so it will be using the terminal application to run scripts it creates. We’ll demonstrate in class so you can understand what its doing.\n\n5.5.1 Exploring agent mode\nLet’s explore Agent mode’s features through some analysis.\nSwitch the Modes to agent mode. Make sure you have the readme file in your project directory. This has all the context our agent will need.\nIt will help if you have the readme file open when you run the agent, so open that file now (then it will refer to that file first).\nNow prompt agent mode with something like:\nI want to do a regression of pres.topa on cb_cover. Create all the scripts I need to do this, as well as run the scripts to make plots of the predicted relationship between the variables\nThen follow along with the conversation, adjusting or correcting it as need be.\nSee how it goes and we can discuss in class. It will be interesting to compare results as they will all be different.\n\nTip: There’s no ‘optimal’ prompt, only better prompts. Sometimes the best way to write is the way you are most comfortable writing. You’ll get more out of your brain that way and copilot will end up performing the same.\n\nHere’s an alternative way to phrase that prompt:\nArrr matey, I be wantin' to run a regression of pres.topa on cb_cover. Hoist all the scripts I need for this voyage, and chart the plots of the predicted relationship between these variables!\n(Ok so that last prompt definitely doesn’t follow the guidelines of being super clear, but I was bored and it seemed to work ok)\n\n\n5.5.2 Writing up the project?\nYou can keep going from here if you like and get agent mode to write up the results it found as an Rmd file. It will use the tables it generates to (hopefully) make accurate interpretations. Copilot Pro also has vision capabilities. This means it will be able to interpret the figures it creates as well. We’ll see that in action when we look at Roo Code in a bit.\nIf you do that, as always, don’t take anything for granted. Make sure you check everything and understand the results yourself.\n\n\n5.5.3 Agent mode errors\nAgent mode will try to debug its errors, but can get stuck in a loop sometimes.\nOne common error is that it will try to run Rscript from the terminal in order to run full R scripts. But not everyone’s computer is set-up with an Rscript command. To do this you will need to web search how to put Rscript on your PATH (which means it can be found from the terminal). How you do this depends on your computer.\nThis is an advanced IT set-up feature that I won’t cover here, but for one tip see the custom instructions section below.\n\n\n5.5.4 Summary\nAgent mode can really accelerate your workflow development. But there are some risks. It can also go off track or write excessive amounts of code (over-engineering). Best practices for using Agent mode include:\n\nSeparate science questions (what stats) from implementation stats (what code)\nUnderstand the stats you want to do, don’t just rely on copilot to get it right\nChecking what it does at is does it, so you can keep it on track\nGiving strong guidelines e.g. through a project readme file.\nKeeping the readme updated to guide copilot\nReport AI use and how it was used in your publications",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Github copilot for R</span>"
    ]
  },
  {
    "objectID": "05-github-copilot.html#llm-choice",
    "href": "05-github-copilot.html#llm-choice",
    "title": "5  Github copilot for R",
    "section": "5.6 LLM choice",
    "text": "5.6 LLM choice\nGithub copilot let’s you choose different LLMs. What you can see will depend on what type of plan you have. Some LLMs have a limited number of ‘Premium Requests’ per month (again depending on your account tier) (e.g. at time of writing Pro accounts got 300 premium requests per month.)\nClick the Model choice menu to see what’s available (bottom of chat window, probably says ‘GPT-4.1’ right now).\nThe 0x means you get unlimited requests, whereas a 1x means it will use one full premium request.\nDifferent models have different performances for different tasks.\nIn general there isn’t a lot of formal evaluations of R coding or statistics yet. Whereas, there are many available for Python.\nThe two most commonly used LLMs for data science are Claude Sonnet series (currently 4.0 available) and GPT 4.X series (currently 4.1 available).\nIn general I’ve found the Claude Sonnet models are the best for R code and most reliable for tool use (e.g. Edit or Agent mode).\nGPT 4.1 is also pretty good, but in my experience there is a higher rate of errors. For instance, in Edit and Agent mode reasonably often it will delete text or code you wanted to keep (so keep an eye on what its doing). GPT 4.1 also tends to be a bit ‘lazier’ than Sonnet 4.0 in that it will tend to explain what to do, rather than explaning and implementing the suggestion.\nPlay around with the same prompt, but different models, so you can compare performance.\n\n5.6.1 Bring Your Own API Key (BYOK)\nYou can add your own API key to github copilot. This lets you access other models on a pay per use basis. To add an API key click the model menu then select ‘Manage models’ and follow instructions.\nYou will also need an OpenAI API key if you want to use vision, that is, attach images as context.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Github copilot for R</span>"
    ]
  },
  {
    "objectID": "05-github-copilot.html#tools-and-mcp",
    "href": "05-github-copilot.html#tools-and-mcp",
    "title": "5  Github copilot for R",
    "section": "5.7 Tools and MCP",
    "text": "5.7 Tools and MCP\nAdvanced users may want to install Model Context Protocol Servers or tools. These are accessible via the cog in the chat window.\nI won’t cover tools and MCP in detail in this course. But these give Github Copilot additional abilites, such as web searching, or access to specific online databases or reference manuals (e.g. for code styles).\nThere are some security concerns to consider if using tools, see later chapters on this topic.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Github copilot for R</span>"
    ]
  },
  {
    "objectID": "05-github-copilot.html#other-options",
    "href": "05-github-copilot.html#other-options",
    "title": "5  Github copilot for R",
    "section": "5.8 Other options",
    "text": "5.8 Other options\nGithub Copilot is a complex software and I recommend browsing the docs for a comprehensive guide. But there are a few other key features.\nAll of these are found by clicking the octocat on the bottom right of your VScode window.\n\n5.8.1 Indexing\nFirst, it will automatically index the scripts and documents in your active directory. This means a version is created that the AI agent can quickly search. This speeds up recommendations and makes them more accurate.\nHowever, indexing may also be a data privacy concern. While your index shouldn’t be shared with anyone else, it does mean that as soon as you open VSCode github copilot is reading some of the files (hard to tell exactly what) to create its index in the cloud.\nTo turn this off click the octocat and find the option for indexing to disable it.\n\n\n5.8.2 Configure code completions\nYou can enable/disable code completions for a certain file type or all file types. Sometimes they get in the way or are distracting, like when you are writing qmd documents.\n\n\n5.8.3 Next edit suggestions\nThis option is turned off by default. If turned on it will suggest changes anywhere in your document, rather than just at the cursor. For instance, if you update a variable name at the top, it will find and suggest updating all instances of that variable name. You use tab to scroll through the different ‘next edit’ suggestions when it makes those.\n\n\n5.8.4 Premium requests bar\nThis menu is also where you find how many premium requests you’ve used in this month.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Github copilot for R</span>"
    ]
  },
  {
    "objectID": "05-github-copilot.html#customizing-github-copilot-mode-and-instructions",
    "href": "05-github-copilot.html#customizing-github-copilot-mode-and-instructions",
    "title": "5  Github copilot for R",
    "section": "5.9 Customizing Github Copilot mode and instructions",
    "text": "5.9 Customizing Github Copilot mode and instructions\nThere’s a lot of customization options in copilot now. To see some of these, open the chat window and click the cog symbol. Then pick one of the options below\n\n5.9.1 Custom modes\nAfter the cog click ‘Modes’, then a pop-up will appear at the top of your VScode window. Click the plus to create a new mode. Follow the prompts. Try paste in our text from the stats bot from before.\nOnce that is saved in the .github directory (so only local to this project) you can click the Modes selector (bottom left of chat window, might say ‘Ask’, ‘Edit’ or ‘Agent’) and choose your new mode. Try having a conversation with it.\nYou can also give custom modes access to tools, so they can edit or act like agents. If you’re an advanced user you might want to play around with these options to make modes for different tasks (like data exploration).\n\n\n5.9.2 Custom intstructions\nFor heavy Agent users you may want to set-up custom instructions. These are just instructions attached to every prompt you run in this project. To create these press the cog button again, click ‘Instructions’ and follow the steps to create a new instruction as we did above for the modes.\nSome ideas: you could set preference for ggplot2 or a specific coding style.\nYou can also add instructions to tell the agent how to use Rscript to avoid terminal errors See here for instructions. One way around Rscript related errors is to find the path to the Rscript executable on your computer (e.g. Rscript.exe on windows) and then create a custom instruction that says something like:\nTo use Rscript to run R scripts give the full path like this: \n\"C:/Program Files/R/bin/Rscript.exe\" \"my_script.R\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Github copilot for R</span>"
    ]
  },
  {
    "objectID": "06-general-prompting-advice.html",
    "href": "06-general-prompting-advice.html",
    "title": "6  General principles for effective use of LLMs and AI assistants",
    "section": "",
    "text": "6.1 Break your problem down into smaller parts\nWhat statistical problems might I run into if I want to build a statistical model that predicts fish abundance from coral cover?\nThere’s many examples available of how LLMs can reason effectively if they are encouraged to work through a problem step-by-step, whereas they guess (some would say hallucinate) the answer if just asked the question straight up.\nThere are many structured prompting strategies that work by splitting a problem into smaller parts (Chen et al. 2025). We won’t cover these here, they are most beneficial if you are writing code that talks to LLMs programmatically, to iteratively solve a problem (more on that later). But we can learn from the principle general principle of breaking a problem into parts.\nHere’s a stats motivated example for you to try. Underwater cameras and deep learning image analysis are used to count fish on surveys, but when fish are in high densities the algorithms can overcount the number of fish (Connolly et al. 2021).\nLet’s see if the LLM can figure out the logic of adjusting for the overcounting bias.\nStraight-up question:\nThe correct reasoning is: 2.5*5.5 = 13.75 predicted fish. Then we adjust for the overcounting bias: 13.75^(1/1.1) = 10.83 which is the true number.\nOptions for breaking it down into parts\nIf you have an option for a ‘thinking’ or ‘reasoning’ model available, try the first prompt again, but use a reasoning model. These use a different computation process that does more reasoning.\nI tested this with Sonnet and GPT 4.1 models in copilot ask mode. Without the step-by-step instruction they sometimes got it write and sometimes got it wrong. Often the wrong answers were because they applied the bias in the wrong direction (ie 13.75^1.1). With a thinking model it got it correct more often.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>General principles for effective use of LLMs and AI assistants</span>"
    ]
  },
  {
    "objectID": "06-general-prompting-advice.html#break-your-problem-down-into-smaller-parts",
    "href": "06-general-prompting-advice.html#break-your-problem-down-into-smaller-parts",
    "title": "6  General principles for effective use of LLMs and AI assistants",
    "section": "",
    "text": "Prompt\n\n\n\nI have a regression that finds snapper abundance increases by an average of 2.5 fish per 10% increase in seagrass cover, with an intercept of zero. But the snapper count data were biased upwards at high density, with a power relationship that has an exponent of 1.1. What is the expected abundance at a seagrass cover of 55%?\n\n\n\n\n\n\n\n\n\n\nPrompt\n\n\n\nThink step-by-step to solve this problem: I have a regression that finds snapper abundance increases by an average of 2.5 fish per 10% increase in seagrass cover, with an intercept of zero. But the snapper count data were biased upwards at high density, with a power relationship that has an exponent of 1.1. What is the expected abundance at a seagrass cover of 55%?\n\n\n\n\n\n\n\n\nPrompt\n\n\n\nI have a regression that finds snapper abundance increases by an average of 2.5 fish per 10% increase in seagrass cover, with an intercept of zero. But the snapper count data were biased upwards at high density, with a power relationship that has an exponent of 1.1. What is the expected abundance at a seagrass cover of 55%? Use chain-of-thought reasoning to answer\n\n\n\n\n\n\n\n\nPrompt\n\n\n\nThink like the White Stripes in their song Little Acorns to solve this problem: Think step-by-step to solve this problem: I have a regression that finds snapper abundance increases by an average of 2.5 fish per 10% increase in seagrass cover, with an intercept of zero. But the snapper count data were biased upwards at high density, with a power relationship that has an exponent of 1.1. What is the expected abundance at a seagrass cover of 55%?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>General principles for effective use of LLMs and AI assistants</span>"
    ]
  },
  {
    "objectID": "06-general-prompting-advice.html#be-clear-specific-and-give-lots-of-details",
    "href": "06-general-prompting-advice.html#be-clear-specific-and-give-lots-of-details",
    "title": "6  General principles for effective use of LLMs and AI assistants",
    "section": "6.2 Be clear, specific and give lots of details",
    "text": "6.2 Be clear, specific and give lots of details\nThere’s a trade-off between spending mental energy on writing a prompt and writing a prompt that gets you an accurate answer from the LLM. In general our brains are trying to minimize the amount of writing we do, so we don’t write enough or write clearly enough to get good performance out of the LLM.\nSome commentators have noted that the over-use of the term ‘hallucination’. You often find if you ask the same question again, but with clearer language and more details you’ll get an accurate answer. So is an inaccurate answer really a hallucination (making something up) or is it just the LLM answering based on insufficient information?\nLet’s look at a couple of examples.\nClear your script of any plotting code and variable names (if using VScode) and try getting a GC to create a plot of two variables in our dataframe. Try this prompt, you can add it as a comment to use inline code completions, just ask it in ‘ask mode’ (or equivalent) or bring up the inline chat window (cmd/cntrl-i)\n\n\n\n\n\n\nPrompt\n\n\n\nPlot branching coral cover against distance to logponds.\n\n\nDid the code work without human intervention? Without the context the AI won’t know the variable names. A better prompt would reference the variable names:\n\n\n\n\n\n\nPrompt\n\n\n\nPlot cb_cover against dist_to_logging_km\n\n\nor even better\n\n\n\n\n\n\nPrompt\n\n\n\nMake a geom_point with cb_cover on the y-axis and dist_to_logging_km on the x-axis\n\n\nNow let’s try a more complex example. If we wan’t to get advice on models for analyzing how the number of fish relates to coral cover we could ask:\n\n\n\n\n\n\nPrompt\n\n\n\nHow could I test the relationship between coral cover and fish?\n\n\n(If trying this in Ask mode make sure you have no data or scripts attached, which may add the extra context it needs).\nSometimes if you do the above the LLM won’t even realize you are asking for a statistical ‘test’ and may give you an answer for computer programming tests.\nSlight better:\n\n\n\n\n\n\nPrompt\n\n\n\nHow could I test if there is a statistically significant relationship between coral cover and fish?\n\n\nBut our data are count data, so the statistically best-practice answer would be a response that specifically accounts for the fact that fish are abundance counts.\nEven better would be to be specific about the data types:\n\n\n\n\n\n\nPrompt\n\n\n\nHow could I test if there is a statistically significant relationship between coral cover and juvenile fish. Fish abundance is my response variable. Fish abundance was measured by counting fish on surveys of standard length at 50 different sites. Coral cover was measured as percent cover at each site?\n\n\nTry each of these a few times and tally up how often you get a good recommendation for each prompt style. Make sure you start a new chat window each time. I would consider a good recommendation one that appropriately models the count data, e.g. a poisson or negative binomial GLM. Coral cover is the predictor variable so we don’t need to do any special transforms on that (sometimes the LLMs will recommend you transform it also).\nNote that if you are using a chat interface, rather than API calls, your interface may have a ‘memory’ which means each new prompt is not independent of previous prompts. Check the documents for your software to be sure and turn off memory if possible. If using VSCode you can open a new window from the ‘File’ menu and then open the GC chat directly without opening a project folder (so it has no context).\nWe could make this prompt even better if we prompt to consider issues that may arise:\n\n\n\n\n\n\nPrompt\n\n\n\nHow could I test if there is a statistically significant relationship between coral cover and juvenile fish. Fish abundance is my response variable. Fish abundance was measured by counting fish on surveys of standard length at 50 different sites. Coral cover was measured as percent cover at each site? Reason step-by-step and identify any statistical issues that may arise with this analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>General principles for effective use of LLMs and AI assistants</span>"
    ]
  },
  {
    "objectID": "06-general-prompting-advice.html#put-everything-up-front-rather-than-engaging-in-conversation",
    "href": "06-general-prompting-advice.html#put-everything-up-front-rather-than-engaging-in-conversation",
    "title": "6  General principles for effective use of LLMs and AI assistants",
    "section": "6.3 Put everything up front, rather than engaging in conversation",
    "text": "6.3 Put everything up front, rather than engaging in conversation\nLLMs perform better if you explain to them in detail what you want straight-up, rather than trying to engage in conversation. Tests show that answers are less accurate if you present the same question over multiple turns versus giving all the information up front.\nThe problem with multi-turn conversation is what’s called ‘context poisoning’. If the LLM doesn’t have all the information it will still try to answer and it may answer wrongly (what some people inaccurately call hallucinations). Once that incorrect answer is in the context window (chat thread) it is hard to get the LLM to forget it. The best thing to do is start the chat thread over again and write a clearer prompt to start with.\nTry the above example again, but break it into parts and get the LLM to answer each part before progressing.\n\n\n\n\n\n\nPrompt\n\n\n\n\nHow could I test if there is a statistically significant relationship between coral cover and juvenile fish.\nFish abundance is my response variable.\nFish abundance was measured by counting fish on surveys of standard length at 50 different sites.\nCoral cover was measured as percent cover at each site\n\n\n\nCompare the final answers to the answer you got when you put all of that in one initial single prompt. Note any instances of context poisoning.\nThe problem gets worse in bigger projects, because as you fill up more of the context window small problems can be harder to detect and eliminate from your context window. For example, an incorrect line of code in a superseded script make make it into copilot’s index may repeatedly haunt you until you get rid of it and the index is updated.\nAnd that’s another reason to use git to manage versions, rather than keeping multiple versions of the same script hanging around (haunting?) your project directory.\nIts ok to engage the assistant in conversation and use it as a problem solving partner if you’re not sure of how to do something. But you should be aiming to come up with a plan. Once you have the plan, then write a really good clear prompt to get the final advice.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>General principles for effective use of LLMs and AI assistants</span>"
    ]
  },
  {
    "objectID": "06-general-prompting-advice.html#overall-advice-over-explain-what-you-want-and-say-it-all-up-front",
    "href": "06-general-prompting-advice.html#overall-advice-over-explain-what-you-want-and-say-it-all-up-front",
    "title": "6  General principles for effective use of LLMs and AI assistants",
    "section": "6.4 Overall advice: Over-explain what you want and say it all up front",
    "text": "6.4 Overall advice: Over-explain what you want and say it all up front\nOverall the above steps amount to: Over-explaining what you want and saying it all up front. This probably means more effort on your behalf than you were hoping.\nYou can prompt an agent with a command like ‘Create an academic paper about fish and coral using this data’, but the answer won’t be very good and will be full of inaccuracies.\nIf you were to write (optionally with AI assistance) detailed instructions on your project’s aims, the details of the data collection and the analysis you want to do, you could get a long way towards an accurate first draft of an academic paper. We’ll look at the idea of expanding prompts into specification sheets later on.\n\n\n\n\nChen, Banghao, Zhaofeng Zhang, Nicolas Langrené, and Shengxin Zhu. 2025. “Unleashing the Potential of Prompt Engineering for Large Language Models.” Patterns 6 (6). https://doi.org/10.1016/j.patter.2025.101260.\n\n\nConnolly, R. M., D. V. Fairclough, E. L. Jinks, E. M. Ditria, G. Jackson, S. Lopez-Marcano, A. D. Olds, and K. I. Jinks. 2021. “Improved Accuracy for Automated Counting of a Fish in Baited Underwater Videos for Stock Assessment.” Frontiers in Marine Science 8: 658135. https://doi.org/10.3389/fmars.2021.658135.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>General principles for effective use of LLMs and AI assistants</span>"
    ]
  },
  {
    "objectID": "07-ai-powered-analysis-workflows.html",
    "href": "07-ai-powered-analysis-workflows.html",
    "title": "7  AI powered analysis workflows",
    "section": "",
    "text": "7.1 Recommended data analysis workflow\nLLM prompting is most effectively designed if you have a good idea of your goal and the workflow or process that needs to be followed to achieve that goal.\nRemember the basic advice was to break problems into smaller parts. So for data analysis I suggest thinking about different stages in your workflow. Here’s one suggestion:\nLLMs perform differently across these components. They excel at code generation and implementation planning but are less reliable for selecting appropriate statistical approaches or interpreting complex results.\nLLMs can be used across all of these steps, but we recommend that each step is treated separately. This encourages informed decision making and avoids making decisions on the fly. For instance, it is better to design the statistical analysis prior to setting an agent up to automate the implementation of that analysis.\nThe separation of workflow steps also helps prevent overreliance on LLMs for statistical decisions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI powered analysis workflows</span>"
    ]
  },
  {
    "objectID": "07-ai-powered-analysis-workflows.html#recommended-data-analysis-workflow",
    "href": "07-ai-powered-analysis-workflows.html#recommended-data-analysis-workflow",
    "title": "7  AI powered analysis workflows",
    "section": "",
    "text": "Select statistical approach: Determining appropriate statistical methods for research questions\nPlan implementation: Designing the analytical workflow and code structure\nWrite code: Writing the actual code to implement analyses\nGuidance on Interpretation: Understanding and reporting results",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI powered analysis workflows</span>"
    ]
  },
  {
    "objectID": "07-ai-powered-analysis-workflows.html#stages-of-analysis",
    "href": "07-ai-powered-analysis-workflows.html#stages-of-analysis",
    "title": "7  AI powered analysis workflows",
    "section": "7.2 Stages of analysis",
    "text": "7.2 Stages of analysis\nNow let’s work through applying that general advice to specific the stages of analysis I identified.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI powered analysis workflows</span>"
    ]
  },
  {
    "objectID": "07-ai-powered-analysis-workflows.html#selecting-a-statistical-method",
    "href": "07-ai-powered-analysis-workflows.html#selecting-a-statistical-method",
    "title": "7  AI powered analysis workflows",
    "section": "7.3 Selecting a statistical method",
    "text": "7.3 Selecting a statistical method\nIts good practice to identify the analysis you want to do, before thinking about how you write the code for that. Once you have an approach, follow-up with identifying what packages can implement that approach.\nThe limited number of evaluations of LLMs for statistics have found the biggest improvements for prompts that:\n\nInclude domain knowledge in the prompt\nInclude data or summary data in the prompt\nCombine domain knowledge with CoT (but CoT on its own doesn’t help)\n\nIn addition, larger and more up-to-date models tend to be better. e.g. try Claude 4.0 or GPT 5.\n\nTip: LLMs will tend to suggest the most obvious statistical analyses. If you want to innovate creative new types of analyses you need to work a bit harder. One way to do this is to mix up your prompts to try and get cross-disciplinary pollination. For instance, you could ask it: “Suggest methods I could use for this analysis, taking inspiriation from different disciplines such as medicine, psychology and climate research”.\n\n\n7.3.1 Giving context\nAs we saw above its a good idea to give the assistant context on your data, data collection methodology and aims. I recommend writing a markdown document with all of this information in it. Here’s an example for the analysis of my benthic project. The markdown is easy to attach to any request you make to an assistant, or copy and paste into web forms.\nYou can of course use the assistant to create the markdown. Try this in agent mode:\n\n\n\n\n\n\nPrompt\n\n\n\nWrite a summary of the meta-data for this database in a new markdown file [attach the csv database]\n\n\nThen you can edit the summary to ensure it is accurate.\n\n\n7.3.2 Attaching data or data summaries\nAttaching the data is proven to improve the accuracy of statistical recommendations.\nAttaching data works a lot like giving context. The LLM can see what the data looks like and can often guess variable types from the data field names or data values.\nLarge datasets can be hard to attach, because they fill up the context window. GC agent mode will automatically shorten large data frames if you attach them.\nOr ask the agent to write a script to create a data summary. For instance try this in agent mode:\n\n\n\n\n\n\nPrompt\n\n\n\nWrite a script that creates smaller version of the benthic data that has only the first six rows of data\n\n\n\nTip: If you consult a human statistician they’ll usually ask you lots of questions. LLMs, in contrastwill tend to just give you an answer, whether or not they have enough context. Say you asked me the same question you had in your LLM prompt like “how do see if fish are related to coral”. There’s no way I’d jump in with an answer with so little information. But the LLM will. So be aware of this shortcoming and come to prompting pre-prepared with the context it will need to give you a better answer.\n\n\n\n7.3.3 Attach domain knowledge\nDomain knowledge is proven to improve the accuracy of statistical recommendations.\nDomain knowledge on the research question and relevant statistics can really help refine your approach. If you have a paper that you are emulating, convert it to markdown so you can attach that as context to the prompt (e.g. from a url or pdf).\nOther good sources of domain knowledge are stats tutorial blogs, package documentation and the R package vignettes. So keep a list of these handy for when you need them.\nGoogle’s LLM notebook is another popular tool for this. Create a notebook, attach your sources and then ask for a summary. You can then feed this into your project specific AI assistant as domain knowledge.\n\n\n7.3.4 Web search\nAI web search tools can help you create customized tutorials for your stats problem. You can then attach these as context when discussing your project with the assistant. See the ?sec-deepresearch for how to use API calls to do this. Many AI web platforms also have web search features, including chatGPT and copilot. Google is in the process of adding this option to its search engine.\nYou’ll get the best results if reasoning/thinking modes are enabled. As these are more energy expensive to run they are often only available for paid subscriptions. Make sure you read the reasoning information as well, as its important to understand if the answer is of high quality or not.\nRemember that web search AI doesn’t like long prompts, so keep your prompts short and specific.\nHere’s a prompt you can try in chatGPT or Copilot (e.g. in Teams):\n\n\n\n\n\n\nPrompt\n\n\n\nResearch the peer-reviewed academic literature to find the most robust methods for analysing ecological count data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI powered analysis workflows</span>"
    ]
  },
  {
    "objectID": "07-ai-powered-analysis-workflows.html#plan-implementation-and-project-structure",
    "href": "07-ai-powered-analysis-workflows.html#plan-implementation-and-project-structure",
    "title": "7  AI powered analysis workflows",
    "section": "7.4 Plan implementation and project structure",
    "text": "7.4 Plan implementation and project structure\nRepeat after me:\n\nI will keep my data analysis code neat and organized. I will keep my data analysis code neat and organized. I will keep my data analysis code neat and organized.\n\nIf you’re like most of us, you probably code as you go and don’t spend a lot of time organizing what you’ve created. You may end up with a script that’s 1000s of lines long and spans data import, wrangling, initial visuals, analysis, verification and plotting.\nThe worst cases I’ve seen are scripts that don’t even evaluate in order, like students who tell me ‘run lines 60-70, then go back to line 20 and it should work…’\nHave you ever tried to go back into one of your old projects to fix something and found it a massive mental struggle to work out what you did? Well that is a problem for the AI assistant too. You’re filling its context window up with irrelevant junk and its going to have a hard time giving you good advice.\nOrganized code is good for future you, its good for other scientists using your code, its good for repeatability AND its good for getting better help from AI assistants.\nIts more work to do, but the good news is that the time cost is more than made up with the time savings from using the AI. The AI can even help you be more organized.\nSo my next step is to plan what you do, before you do it. This means thinking about the high level steps you’ll have to go through to complete your analysis.\n\n7.4.1 Directory structure\nThere’s no single way to organize your project, the main thing is to be organized. I keep a folder on my computer with a template. Inside that folder I have these files and directories:\nglm-test-case/\n├──  readme.md           # This readme file with project documentation\n├──  .gitignore           # Standard gitignore for R projects\n├──  LICENSE\n├── data/                    # Data files and intermediate datasets\n├── initial-prompt.md       # Initial project prompt and requirements\n├── outputs/                # Generated output files\n│   └── plots/              # Diagnostic plots and visualization outputs (PNG files)\n└── scripts/                # R scripts for data analysis and modeling\nYou might want to add more nesting if it’s a complex project, such as project with both spatial and non-spatial databases.\n\n\n7.4.2 Readme file for the assistant (and you)\nHere’s an example from my readme file, which gives instructions for the agent for how to structure and navigate the project.\nIt is helpful to be specific about:\n\nFolder structure\nVariable names and meta-data\nCode organisation\nCode style\n\nHere’s an example from one of my readme files that has the above elements.\n\n\n\n\n\n\nPrompt\n\n\n\n7.5 Instructions for AI agent\nThe agent will produce a report that answers the above questions. The report will include a description of the data, the methods used for analysis, and the results of the analysis. The code will be written as R scripts.\nEach script should be modular and save intermediate results as datafiles and figures. The final report must be written in Rmarkdown format. The figures will be imported using markdown syntax, e.g. ![](outputs/plots/figure1.png). Don’t use R code for figures in the markdown report. Summary tables should be imported from .csv files and created using the knitr::kable() function in Rmarkdown. The report must include the following sections:\n\nStudy aims\nData methodology\nAnalysis methodology\nResults\n\nModel selection and verification\nModel fit statistics\nPlots of predicted fish abundance (log-link scale) based on the final model, with confidence intervals\nRelevant statistics (r2, p-values, etc.)\n\n\nThe agent is must produce diagnostic plots and a separate report on the model diagnostics.\n\n7.5.1 Tech context\n\nWe will use the R program\ntidyverse packages for data manipulation\nggplot2 for data visualization\nuse theme_set(theme_classic()) for plots\nUse the MASS package for the negative binomial model, however don’t load it globally with library(MASS), instead use MASS::glm.nb() to avoid namespace conflicts.\nUse visreg package for plotting model effects and confidence intervals, e.g. visreg::visreg(m2, \"CB_cover\", \"soft_cover\", gg=TRUE, scale = 'linear')\n\nKeep your scripts short and modular to facilitate debugging. Don’t complete all of the steps below in one script. Finish scripts where it makes sense and save intermediate datasets.\nWhen using Rscript to run R scripts in terminal put quotes around the file, e.g. Rscript \"1_model.R\"\n\n\n7.5.2 Workflow\n\nCreate a todo list and keep track of progress\nData processing including standardizing coral variables by number of points\nModel selection and verification, produce diagnostic plots\nModel diagnostic plots markdown report\nCreate plots of predictions from the final model\nWrite report in markdown format\n\n\n\n7.5.3 Directory structure\nglm-test-case/\n├── data/                    # Processed data files and intermediate datasets\n├── fish-coral.csv          # Raw data file with fish and coral cover measurements\n├── glm-readme.md           # This readme file with project documentation\n├── initial-prompt.md       # Initial project prompt and requirements\n├── outputs/                # Generated output files\n│   └── plots/              # Diagnostic plots and visualization outputs (PNG files)\n└── scripts/                # R scripts for data analysis and modeling\nPut the .rmd reports in the top-level directory.\n\n\n\nHave a go at writing your own readme file, then using an assistant to complete the analysis. You will also want to include meta-data and project aims. You can pick your own project aims, but one example might be a project that aims to test if fish abundance depends on coral cover.\nOnce you have your readme, you can attach it (or open it then click new chat). Given all the context you’ve provided you can just write something simple like:\n\n\n\n\n\n\nPrompt\n\n\n\nHelp me plan R code to implement this analysis.\n\n\nOr\n\n\n\n\n\n\nPrompt\n\n\n\nHelp me plan the workflow and scripts to implement this analysis",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI powered analysis workflows</span>"
    ]
  },
  {
    "objectID": "07-ai-powered-analysis-workflows.html#instructions-for-ai-agent",
    "href": "07-ai-powered-analysis-workflows.html#instructions-for-ai-agent",
    "title": "7  AI powered analysis workflows",
    "section": "7.5 Instructions for AI agent",
    "text": "7.5 Instructions for AI agent\nThe agent will produce a report that answers the above questions. The report will include a description of the data, the methods used for analysis, and the results of the analysis. The code will be written as R scripts.\nEach script should be modular and save intermediate results as datafiles and figures. The final report must be written in Rmarkdown format. The figures will be imported using markdown syntax, e.g. ![](outputs/plots/figure1.png). Don’t use R code for figures in the markdown report. Summary tables should be imported from .csv files and created using the knitr::kable() function in Rmarkdown. The report must include the following sections:\n\nStudy aims\nData methodology\nAnalysis methodology\nResults\n\nModel selection and verification\nModel fit statistics\nPlots of predicted fish abundance (log-link scale) based on the final model, with confidence intervals\nRelevant statistics (r2, p-values, etc.)\n\n\nThe agent is must produce diagnostic plots and a separate report on the model diagnostics.\n\n7.5.1 Tech context\n\nWe will use the R program\ntidyverse packages for data manipulation\nggplot2 for data visualization\nuse theme_set(theme_classic()) for plots\nUse the MASS package for the negative binomial model, however don’t load it globally with library(MASS), instead use MASS::glm.nb() to avoid namespace conflicts.\nUse visreg package for plotting model effects and confidence intervals, e.g. visreg::visreg(m2, \"CB_cover\", \"soft_cover\", gg=TRUE, scale = 'linear')\n\nKeep your scripts short and modular to facilitate debugging. Don’t complete all of the steps below in one script. Finish scripts where it makes sense and save intermediate datasets.\nWhen using Rscript to run R scripts in terminal put quotes around the file, e.g. Rscript \"1_model.R\"\n\n\n7.5.2 Workflow\n\nCreate a todo list and keep track of progress\nData processing including standardizing coral variables by number of points\nModel selection and verification, produce diagnostic plots\nModel diagnostic plots markdown report\nCreate plots of predictions from the final model\nWrite report in markdown format\n\n\n\n7.5.3 Directory structure\nglm-test-case/\n├── data/                    # Processed data files and intermediate datasets\n├── fish-coral.csv          # Raw data file with fish and coral cover measurements\n├── glm-readme.md           # This readme file with project documentation\n├── initial-prompt.md       # Initial project prompt and requirements\n├── outputs/                # Generated output files\n│   └── plots/              # Diagnostic plots and visualization outputs (PNG files)\n└── scripts/                # R scripts for data analysis and modeling\nPut the .rmd reports in the top-level directory.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI powered analysis workflows</span>"
    ]
  },
  {
    "objectID": "07-ai-powered-analysis-workflows.html#writing-the-code",
    "href": "07-ai-powered-analysis-workflows.html#writing-the-code",
    "title": "7  AI powered analysis workflows",
    "section": "7.6 Writing the code",
    "text": "7.6 Writing the code\nOnce you’re happy with the plan, you can get copilot to implement it. There’s roughly three ways to do this, with increasing levels of automation\n\nStart creating the code yourself, using inline editing and copilot edit to help write bits of the code\nInstruct an AI agent to complete the tasks in the readme, manually checking in and editing the code as you go. See the Agents chapter for more on AI Agents.\nFull ‘vibe-coding’ which means you ask an AI agent to complete the task and just accept all of its suggestions.\n\nVibe coding isn’t recommended for scientific projects, more on that later.\n\nTip: We are using the readme.md is copilot’s memory. This means the assitant always has the context it needs across different chat sessions (where it would otherwise forget). So its important to keep the readme updated. Its also useful to help you remember if you come back to the project some months or years later.\n\nA few more tips for this stage of the analysis.\nAs you go attach datasets or domain knowledge as neccessary, particularly if the assistant can’t access that information from your readme.\nYou can get the assistant to write tests for your datasets. Such as checking joins worked properly and you didn’t duplicate or lose data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI powered analysis workflows</span>"
    ]
  },
  {
    "objectID": "07-ai-powered-analysis-workflows.html#suggested-workflows",
    "href": "07-ai-powered-analysis-workflows.html#suggested-workflows",
    "title": "7  AI powered analysis workflows",
    "section": "7.7 Suggested workflows",
    "text": "7.7 Suggested workflows\n\n7.7.1 Suggested workflow for new analyses\nHere’s a workflow I’ve found works well if I’m doing an analysis that is new to means\n\nRead the literature to identify the appropriate analysis for the research question and data.\nOnce I’ve narrowed down the options I look for useful domain knowledge: vignettes, manuals or blogs that have suitable R examples.\nStart a new folder, setting up the directory and readme as descriped in this workshop.\nUse copilot to implement the analysis, attaching data summaries and the domain knowledge to get the best prompts.\n\n\n7.7.1.1 Suggested workflow for analyses I know well\nMuch the same as above, just less planning and you don’t need to search the literature because you know what you want to do. If you save useful domain knowledge when you see it you will also have the documents on hand to support the assistant.\n\n\n\n7.7.2 Iterating\nEcological modelling is a creative art with rules. The rules are the scientific method, the medium is math, logic and code. You can view AI as part of this creative scientific process, rather than a replacement for the human.\nUse an LLM assistant to create multiple complete versions of your project. You can then compare them to get ideas. You can also continuously refine the prompt to get what you want as you go.\nAdd it as a tool in your belt, not a replacement. Talking to colleagues, reading, sitting on a board looking at the ocean, having showers etc… are all still important.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI powered analysis workflows</span>"
    ]
  },
  {
    "objectID": "08-advanced-llm-agents.html",
    "href": "08-advanced-llm-agents.html",
    "title": "8  Advanced LLM agents",
    "section": "",
    "text": "8.1 Software options\nWe’ve already seen the Github Copilot interactive agent. Below we’ll look at two other options: Roo Code and Github Copilot CLI (which can be run programmatically).\nThere are also a number of agents available that run in R. I haven’t mentioned them here (yet) as they are very early stage. But check out this resource for an up-to-date overview.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced LLM agents</span>"
    ]
  },
  {
    "objectID": "08-advanced-llm-agents.html#software-options",
    "href": "08-advanced-llm-agents.html#software-options",
    "title": "8  Advanced LLM agents",
    "section": "",
    "text": "8.1.1 Roo code\nRoo Code is an LLM agent that you can run from a VSCode or Positron extension. I like this one because of its customization options, such as allowing you to create your own modes and fully modify the system prompt.\nYou ‘BYOK’ (‘bring your own key’) with Roo Code, meaning you provide the API key for LLM access. It then uses that key to make requests on your behalf.\nRoo code is more complex and expensive to use than Copilot (you are paying per token and it uses a lot of tokens), but allows significant amounts of customization to make bespoke agents that can help with the scientific process.\nSome of the customization options include:\n\nAPI access\nModel options\nCustomizing system message\nContext window management\nCost\nVision capabilities",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced LLM agents</span>"
    ]
  },
  {
    "objectID": "08-advanced-llm-agents.html#github-copilot-cli",
    "href": "08-advanced-llm-agents.html#github-copilot-cli",
    "title": "8  Advanced LLM agents",
    "section": "8.2 Github Copilot CLI",
    "text": "8.2 Github Copilot CLI\nGithub Copilot CLI (Command Line Interface) recently became available. It is an AI agent, meaning it can use tools autonomously in a loop.\nThe advantages of this one over Roo Code are a monthly subscription fee rather than API fees (note its not available in free copilot licenses) and ability to run it programmatically. Downsides are that it is not as customizable.\nIt has two modes:\n\nAn interactive mode that is like a terminal version of the Copilot agent that runs in the chat window\nA programmatic mode that can be run with shell scripts.\n\nThe Copilot CLI is available with paid versions of github copilot.\nThe programmatic mode allows you to write scripts that call github copilot agents. This means you could run replicate agents on the same problem, then gather there results for analysis.\nOnce you’ve setup the copilot CLI, its very easy to run it from R:\n\ncopilot_cmd &lt;- \"copilot -p 'Set-up this project directory with a readme.md file, and directories for outputs, scripts, plots.' --allow-all-tools\"\n\nsystem(copilot_cmd)\n\nThis will run the agent autonomously in the current working directory of your R session.\nNow I wouldn’t recommend using --allow-all-tools like this however. There are important security considerations. Like prompt injection attacks where an Agent goes on the web and gets tricked into doing something bad to your computer when it reads some malicious content. Likewise, agents can just stuff up and just delete or overwrite a bunch of files you wanted to keep.\nYou do need to allow some tools however, otherwise you might as well run the agent in interactive mode (because you’ll have to manually approve every tool use). This defeats the time-saving goal of running agents in a loop.\nHere’s my current set-up:\ncopilot -p 'A prompt here' --allow-all-tools --deny-tool 'shell(cd)' --deny-tool 'shell(git)' --deny-tool 'shell(pwd)' --deny-tool 'fetch' --deny-tool 'extensions' --deny-tool 'websearch' --deny-tool 'githubRepo'\"\nI allow all tools, then prevent tools related to changing working directory, accessing the web or viewing directory context.\nThere is still some risk here. e.g., if you want it to run Rscripts you might want to include in your prompt something like ‘Use Rscript my-script.R to run R scripts from the terminal.’ The R scripts could include anything (including connecting to the web).\nHere’s an example in action. I use sprintf to format the terminal command with the prompt, tools and sub-directory path.\ncopilot_prompt &lt;- \"Set-up this project directory with a readme.md file, and directories for outputs, scripts, plots. Then create example data to illustrate a poisson GLM. Make prediction plots. Use `Rscript 'my-script.R'` to run R files. \"\n\ncopilot_tools &lt;- \"--allow-all-tools --deny-tool 'shell(cd)' --deny-tool 'shell(git)' --deny-tool 'shell(pwd)' --deny-tool 'fetch' --deny-tool 'extensions' --deny-tool 'websearch' --deny-tool 'githubRepo'\"\n\nsubdir_path &lt;- \"dir1\"\n\n copilot_cmd &lt;- sprintf(\n    \"cd '%s' && copilot -p '%s' %s\",\n    subdir_path,\n    copilot_prompt,\n    copilot_tools\n  )\n\nsystem(copilot_cmd)\n\nFrom here it would be easy to create a loop over different subdirectory paths, and run a separate agent in each one. The advantage of cding into each path before the agent opens is that the agent can’t then see context from other sub-directories. So you get independent agent runs.\nYou could use this to see how different prompts perform, or do complex prompt strategies like ‘tree of thought’. In tree of thought you ask an agent to create the same thing many times, it will do it slightly differently each time. Then you aggregate the results and pick the most common or most accurate one.\nMy one gripe is that Github have provided us with woeful documentation of what the tool names are. I’m guessing the tools are what you can see if you open the chat window, click ‘Agent’ mode, then click the tool symbol.\nI’d love to have some default tool sets that simplify the tool permissions. For instance a tool set that locks down all web access.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced LLM agents</span>"
    ]
  },
  {
    "objectID": "09-specification-sheets.html",
    "href": "09-specification-sheets.html",
    "title": "9  Project set-up for AI agents",
    "section": "",
    "text": "9.1 Project organization\nIts helpful to set-up your projects in an organized and modularised way. In my experience most R users write most of their analysis in one long script. Don’t do this. It will be hard for ‘future you’ to navigate. If its hard for a human to navigate, it will also be hard for the assistant. Here’s how I set-up my projects.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Project set-up for AI agents</span>"
    ]
  },
  {
    "objectID": "09-specification-sheets.html#project-organization",
    "href": "09-specification-sheets.html#project-organization",
    "title": "9  Project set-up for AI agents",
    "section": "",
    "text": "9.1.1 General guidance\n\nCreate a new folder for each new project.\nOptional but recommended: Initiliaze a git repo in that folder (I use the ‘Source Control’ extension to VS Code).\nSet-up folders and files in an organized way\nIdeally put the data in this folder also. However, large datasets or sensitive data can be kept in other folders.\nKeep scripts short and modularized (e.g one for data analysis, one for modelling).\n\nOnce you have your folder you can make it an Rstudio project (if using Rstudio) or just use ‘open folder’ in vscode. If want to link multiple folders in then use VScode workspaces.\nIf you are not using git (version control), then I recommend you learn. LLM code editing tools can cause you to lose older versions. So best to back them up with proper use of git.\n\n\n9.1.2 Project directory structure example\nHere’s an example of a project directory structure. You don’t have to use this structure, there are no hard rules about prompting LLMs and ultimately this will be a very long prompt. the important thing is to be organized.\nmy-project/\n├── README.md \n├── .gitignore\n├── Scripts/ # R code\n│   ├── 01_data-prep.R\n│   ├── 02_data-analysis.R\n│   └── 03_plots.R\n├── Shared/       \n│   ├── Outputs/\n│   │   ├── Figures/\n│   │   ├── data-prep/\n│   │   └── model-objects/\n│   ├── Data/\n│   └── Manuscripts/   \n└── Private/",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Project set-up for AI agents</span>"
    ]
  },
  {
    "objectID": "09-specification-sheets.html#the-readme.md-file",
    "href": "09-specification-sheets.html#the-readme.md-file",
    "title": "9  Project set-up for AI agents",
    "section": "9.2 The README.md file",
    "text": "9.2 The README.md file\nThe README.md is the memory for the project. If you use github it will also be the landing page for your repo, which is handy.\nRemember you are writing this for you and the LLMs. So think of it like a prompt.\nHere’s an example of some of the information you might want to include in your readme.\n# PROJECT TITLE\n\n## Summary\n\n## Aims\n\n## Data methodology\n\n## Analysis methodology\n\n## Tech context\n- We will use the R program\n- tidyverse packages for data manipulation\n- ggplot2 for data visualization\n\nKeep your scripts short and modular to facilitate debugging. Don't complete all of the steps below in one script. Finish scripts where it makes sense and save intermediate datasets. \n\n## Steps\nAs you go tick of the steps below. \n\n[ ] Wrangle data\n[ ] Fit regression\n[ ] Plot verification\n[ ] ... \n\n## Data \n\nInclude meta-data here and file paths. \n\n## Directory structure \n\nmy-project/\n├── README.md \n├── .gitignore\n├── Scripts/ # R code\n│   ├── 01_data-prep.R\n│   ├── 02_data-analysis.R\n│   └── 03_plots.R\n├── Shared/       \n│   ├── Outputs/\n│   │   ├── Figures/\n│   │   ├── data-prep/\n│   │   └── model-objects/\n│   ├── Data/\n│   └── Manuscripts/   \n└── Private/",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Project set-up for AI agents</span>"
    ]
  },
  {
    "objectID": "09-specification-sheets.html#example-data",
    "href": "09-specification-sheets.html#example-data",
    "title": "9  Project set-up for AI agents",
    "section": "9.3 Example data",
    "text": "9.3 Example data\nFor this chapter we’ll work with some ecological data on benthic marine habitats and fish.\n\n9.3.1 Case-study: Bumphead parrotfish, ‘Topa’ in Solomon Islands\nBumphead parrotfish (Bolbometopon muricatum) are an enignmatic tropical fish species. Adults of these species are characterized by a large bump on their forehead that males use to display and fight during breeding. Sex determination for this species is unknown, but it is likely that an individual has the potential to develop into either a male or female at maturity.\nAdults travel in schools and consume algae by biting off chunks of coral and in the process they literally poo out clean sand. Because of their large size, schooling habit and late age at maturity they are susceptible to overfishing, and many populations are in decline.\nTheir lifecycle is characterized by migration from lagoonal reef as juveniles (see image below) to reef flat and exposed reef habitats as adults. Early stage juveniles are carnivorous and feed on zooplankton, and then transform into herbivores at a young age.\n\nImage: Lifecycle of bumphead parrotfish. Image by E. Stump and sourced from Hamilton et al. 2017.\nUntil the mid 2010s the habitat for settling postlarvae and juveniles was a mystery. However, the pattern of migrating from inshore to offshore over their known lifecycle suggests that the earliest benthic lifestages (‘recruits’) stages may occur on nearshore reef habitats.\nNearshore reef habitats are susceptible to degradation from poor water quality, raising concerns that this species may also be in decline because of pollution. But the gap in data from the earliest lifestages hinders further exploration of this issue.\nIn this course we’ll be analyzing the first survey that revealed the habitat preferences of early juveniles stages of bumphead parrotfish. These data were analyzed by Hamilton et al. 2017 and Brown and Hamilton 2018.\nIn the 2010s Rick Hamilton (The Nature Conservancy) lead a series of surveys in the nearshore reef habitats of Kia province, Solomon Islands. The aim was to look for the recruitment habitat for juvenile bumphead parrotfish. These surveys were motivated by concern from local communities in Kia that topa (the local name for bumpheads) are in decline.\nIn the surveys, divers swam standardized transects and searched for juvenile bumphead in nearshore habitats, often along the edge of mangroves. All together they surveyed 49 sites across Kia.\nThese surveys were made all the more challenging by the occurrence of crocodiles in mangrove habitat in the region. So these data are incredibly valuable.\nLogging in the Kia region has caused water quality issues that may impact nearshore coral habitats. During logging, logs are transported from the land onto barges at ‘log ponds’. A log pond is an area of mangroves that is bulldozed to enable transfer of logs to barges. As you can imagine, logponds are very muddy. This damage creates significant sediment runoff which can smother and kill coral habitats.\nRick and the team surveyed reefs near logponds and in areas that had no logging. They only ever found bumphead recruits hiding in branching coral species.\nIn this course we will first ask if the occurrence of bumphead recruits is related to the cover of branching coral species. We will then develop a statistical model to analyse the relationship between pollution from logponds and bumphead recruits, and use this model to predict pollution impacts to bumpheads across the Kia region.\nThe data and code for the original analyses are available at my github site. In this course we will use simplified versions of the original data. We’re grateful to Rick Hamilton for providing the data for this course.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Project set-up for AI agents</span>"
    ]
  },
  {
    "objectID": "09-specification-sheets.html#example-spec-sheet",
    "href": "09-specification-sheets.html#example-spec-sheet",
    "title": "9  Project set-up for AI agents",
    "section": "9.4 Example spec sheet",
    "text": "9.4 Example spec sheet\n# Analysis of fish dependence on coral habitat\n\n## Introduction\nThis project will ask how abundance of fish juveniles depends on coral cover. The fish we are interested in is *Bolbometopon muricatum*, the bumphead parrotfish. Also known as 'topa' in the local language of our study region. We will analyze survey data from 49 sites, that includes benthic cover surveys and surveys fish abundance at the same locations. We are studying its juvenile habitat. \n\n## Aims of the analysis \n\n1. Does fish abundance depend on branching coral cover? \n2. What is the direction and strength of the relationship between fish abundance and branching coral cover? \n3. Does fish abundance depend on soft coral cover? \n4. What is the direction and strength of the relationship between fish abundance and branching coral cover? \n\n## Data methodology\n\nThe data was collected with the point intersect transect method. Divers swam along transects. There were several transects per site.  Along each transect they dropped points and recorded the type of benthic organism (in categories) on that point. Percentage cover for one organism type can then be calculated as the number of points with that organism divided by the total number of points on that transect. In our data we have percent cover of branching corals and percent cover of soft corals. \nTransects were averaged to give a single value for each site. \nAt each site divers also counted the number of juvenile 'topa' along dive transects of the same length. \n\n## Analysis methodology \n\nWe will use generalized linear models to analyze the relationship between topa and the two coral cover types. \nTopa abundance is probably over-dispersed, so we we will need to use a negative binomial family. We will use R and the MASS package: \n\nMASS::glm.nb(pres.topa ~ CB_cover*soft_cover, data = fish_coral_cover_sites)\n\nTo obtain a final model we should model selection, starting with a full model then working towards simpler models. We will use likelihood ratio tests to compare models. For example the first test would be: \n\nm1 &lt;- MASS::glm.nb(pres.topa ~ CB_cover*soft_cover, data = fish_coral_cover_sites) m2 &lt;- MASS::glm.nb(pres.topa ~ CB_cover + soft_cover, data = fish_coral_cover_sites) anova(m1, m2, test = “Chisq”)\n\nThen proceed with m2 if the interaction term is not significant. Elsewise to the next proceed with m1.\n\nOn completion of the model selection, we will do model diagnostics. This will include checking residuals and the dispersion parameter. Save these as png files. \n\nWrite a diagnosticis report in an Rmarkdown file. \n\n\n## Instructions for the agent\n\nThe agent will produce a report that answers the above questions. The report will include a description of the data, the methods used for analysis, and the results of the analysis. The code will be written as R scripts. \n\nEach script should be modular and save intermediate results as datafiles and figures. The final report must be written in Rmarkdown format. The figures will be imported using markdown syntax, e.g. `![](outputs/plots/figure1.png)`. Don't use R code for figures in the markdown report. \nSummary tables should be imported from .csv files and created using the `knitr::kable()` function in Rmarkdown. \nThe report must include the following sections:\n\n- Study aims\n- Data methodology\n- Analysis methodology\n- Results\n  - Model selection and verification\n  - Model fit statistics\n  - Plots of predicted fish abundance (log-link scale) based on the final model, with confidence intervals\n  - Relevant statistics (r2, p-values, etc.)\n\nThe agent is must produce diagnostic plots and a separate report on the model diagnostics. \n\n### Tech context\n- We will use the R program\n- tidyverse packages for data manipulation\n- ggplot2 for data visualization\n- use `theme_set(theme_classic())` for plots\n- Use the `MASS` package for the negative binomial model, however don't load it globally with `library(MASS)`, instead use `MASS::glm.nb()` to avoid namespace conflicts.\n- Use `visreg` package for plotting model effects and confidence intervals, e.g. `visreg::visreg(m2, \"CB_cover\", \"soft_cover\", gg=TRUE, scale = 'linear')`\n\nKeep your scripts short and modular to facilitate debugging. Don't complete all of the steps below in one script. Finish scripts where it makes sense and save intermediate datasets. \n\nWhen using Rscript to run R scripts in terminal put quotes around the file, e.g. `Rscript \"1_model.R\"`\n\n### Workflow \n\n1. Create a todo list and keep track of progress\n2. Data processing including standardizing coral variables by number of points\n3. Model selection and verification, produce diagnostic plots\n4. Model diagnostic plots markdown report \n5. Create plots of predictions from the final model\n6. Write report in markdown format\n\n### Directory structure \n\n/```\nglm-test-case/\n├── data/                    # Processed data files and intermediate datasets\n├── fish-coral.csv          # Raw data file with fish and coral cover measurements\n├── glm-readme.md           # This readme file with project documentation\n├── initial-prompt.md       # Initial project prompt and requirements\n├── outputs/                # Generated output files\n│   └── plots/              # Diagnostic plots and visualization outputs (PNG files)\n└── scripts/                # R scripts for data analysis and modeling\n/```\n\nPut the .rmd reports in the top-level directory. \n\n## Meta data \n\n### fish-coral.csv\n\nLocation: `data/fish-coral.csv`\n\nVariables\n- site: Unique site IDs, use to join to benthic_cover.csv\n- reef.ID: Unique reef ID\n- pres.topa: number of Topa counted (local name for Bolbometopon)\n- pres.habili: number of Habili counted (local name for Cheilinus) \n- secchi: Horizontal secchi depth (m), higher values mean the water is less turbid\n- flow: Factor indicating if tidal flow was \"Strong\" or \"Mild\" at the site\n- logged: Factor indicating if the site was in a region with logging \"Logged\" or without logging \"Not logged\"\n- coordx: X coordinate in UTM zone 57S\n- coordy: Y coordinate in UTM zone 57S\n- CB_cover: Number of PIT points for branching coral cover\n- soft_cover: Number of PIT points for soft coral cover\n- n_pts: Number of PIT points at this site (for normalizing cover to get per cent cover)\n- dist_to_logging_km: Linear distance to nearest log pond (site where logging occurs) in kilometres.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Project set-up for AI agents</span>"
    ]
  },
  {
    "objectID": "10-research-applications-LLMs.html",
    "href": "10-research-applications-LLMs.html",
    "title": "10  Research applications of LLMs",
    "section": "",
    "text": "10.1 Automating literature reviews\nNow let’s see if we can use ellmer to clean up some text from a pdf and summarize it. ellmer has some handy functions for processing pdfs to text, so they can then be fed into prompts.\nI’m going to attempt to summarize my recent paper on turtle fishing.\nx &lt;- content_pdf_url(\"https://conbio.onlinelibrary.wiley.com/doi/epdf/10.1111/conl.13056\")\nThis fails with a 403 error. This means the server is blocking the request, it probably guesses (correctly) that I’m calling the pdf programmatically: it thinks I’m a bot (which this tutorial kind of is creating).\nWe can also try with a file on our hard drive, we just have to manually download the pdf.\nmypdf &lt;- content_pdf_file(\"pdf-examples/Brown_etal2024 national scale turtle mortality.pdf\")\nThat works, now let’s use it within a chat. First set-up our chat:\nchat &lt;- chat_anthropic(\n  system_prompt = \"You are a research assistant who specializes in extracting structured data from scientific papers.\",\n  model = \"claude-3-5-haiku-20241022\", \n  max_tokens = 1000\n)\nNow, we can use ellmer’s functions for specifying structured data. Many LLMs can be used to generate data in the JSON format (they were specifically trained with that in mind).\nellmer handles the conversion from JSON to R objects that are easier for us R users to understand.\nYou use the type_object then type_number, type_string etc.. to specify the types of data. Read more in the ellmer package vignettes\npaper_stats &lt;- type_object(\n  sample_size = type_number(\"Sample size of the study\"),\n  year_of_study = type_number(\"Year data was collected\"),\n  method = type_string(\"Summary of statistical method, one paragraph max\")\n)\nFinally, we send the request for a summary to the provider:\nturtle_study &lt;- chat$extract_data(mypdf, type = paper_stats)\nThe turtle_study object will contain the structured data from the pdf. I think (the ellmer documentation is a bit sparse on implementation details) ellmer is converting a JSON that comes from the LLM to a friendly R list.\nclass(turtle_study)\n#list\nAnd:\nIt works, but like any structured lit review you need to be careful what questions you ask. Even more so with an LLM as you are not reading the paper and understanding the context.\nIn this case the sample size its given us is the estimated number of turtles caught. This was a model output, not a sample size. In fact this paper has several methods with different sample sizes. So some work would be needed to fine-tune the prompt, especially if you are batch processing many papers.\nYou should also experiment with models, I used Claude haiku because its cheap, but Claude sonnet would probably be more accurate.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Research applications of LLMs</span>"
    ]
  },
  {
    "objectID": "10-research-applications-LLMs.html#sec-lit-reviews",
    "href": "10-research-applications-LLMs.html#sec-lit-reviews",
    "title": "10  Research applications of LLMs",
    "section": "",
    "text": "turtle_study$sample_size\n#11935\nturtle_study$year_of_study\n#2018\nturtle_study$method\n#The study estimated national-scale turtle catches for two fisheries in the Solomon Islands \n#- a small-scale reef fishery and a tuna longline fishery - using community surveys and \n#electronic monitoring. The researchers used nonparametric bootstrapping to scale up \n#catch data and calculate national-level estimates with confidence intervals.\n\n\n\n\n\n10.1.1 Batch processing prompts\nLet’s try this with a batch of papers (here I’ll just use two). For this example I’ll just use two abstracts, which I’ve obtained as plain text. The first is from another study on turtle catch in Madagascar. The second is from my study above.\nWhat we’ll do is create a function that reads in the text, then passes it to the LLM, using the request for structured data from above.\n\n# Function to process abstracts with ellmer\nprocess_abstract &lt;- function(file_path, chat) {\n  # Read in the text file\n  abstract_text &lt;- readLines(file_path, warn = FALSE)\n  \n  # Extract data from the abstract\n  result &lt;- chat$extract_data(abstract_text, type = paper_stats)\n  \n  return(result)\n}\n\nNow set-up our chat and data request\n\n# Create chat object if not already created\nchat &lt;- chat_anthropic(\n      system_prompt = \"You are a research assistant who specializes in extracting structured data from scientific papers.\",\n      model = \"claude-3-5-haiku-20241022\", \n      max_tokens = 1000\n)\n\nThere’s a risk that the LLM will hallucinate data if it can’t find an answer. To try to prevent this we can set an option , required = FALSE. Then the LLM should return ‘NULL’ if it can’t find the data.\n\n# Define the structured data format\npaper_stats &lt;- type_object(\n    sample_size = type_number(\"Number of surveys conducted to estimate turtle catch\", required = FALSE),\n    turtles_caught = type_number(\"Estimate for number of turtles caught\", required = FALSE),\n    year_of_study = type_number(\"Year data was collected\", required = FALSE),\n    region = type_string(\"Country or geographic region of the study\", required = FALSE)\n  )\n\nNow we can batch process the abstracts and get the structured data\n\nabstract_files &lt;- list.files(path = \"pdf-examples\", pattern = \"\\\\.txt$\", full.names = TRUE)\nresults &lt;- lapply(abstract_files, function(file) process_abstract(file, chat))\nnames(results) &lt;- basename(abstract_files)\n\n# Display results\nprint(results)\n\nIn my first take without the required = FALSE I got some fake results. It hallucinated that the Humber study was conducted in 2023 (it was published in 2010!) and that there were 2 villages surveyed in my study. The problem was that you can’t get that data from the abstracts. So the model is hallucinating a response.\nUnfortunately, with required = FALSE it still hallucinated answers. I then tried Claude Sonnet (a more powerful reasoning model) and it correctly put NULL for my study’s sample size, but still got the year wrong for the Humber study.\nI think this could work, but some work on the prompts would be needed.\n\n10.1.1.1 Reflections\n\n10.1.1.1.1 Cost uncertainty\nThis should be cheap. It cost &lt;1c to make this post with all the testing. So in theory you could do 100s of methods sections for &lt;100USD. However, if you are testing back and forwards a lot or using full papers the cost could add up. It will be hard to estimate this until people get more experience.\n\n\n10.1.1.1.2 Obtaining the papers and dealing with unstructued text in PDFs or HTML\nA big challenge will be getting the text into a format that the LLM can use. Then there are issues like obtaining the text. Downloading pdfs is time consuming and data intensive. Trying to read text data from webpages can also be hard, due to paywalls and rate limits (you might get blocked for making reqeat requests).\nFor instance, in a past study we did where we did simple ‘bag of words analysis’ we either downloaded the pdfs manually, or set timers to delay web hits and avoid getting blocked.\nHTML format would be ideal, because the tags mean the sections of the paper, and the figures already semi-structured.\nThe ellmer pdf utility function seems to work ok for getting text from pdfs. I’m guessing it could be improved though, e.g. to remove wastefull (=$) text like page headers.\nSome other handy tools I use are from AI researcher Simon Willison, who made a PDF reader that runs in your browser and Jina reader for extracting plain text from URLs. Both of these run in your browser, so no data is uploaded to any servers.\n\n\n10.1.1.1.3 Prompting\nNeed to experiment with this to get it right. It might also be good to repeat prompt the same text to triangulate accurate results.\n\n\n10.1.1.1.4 Validation\nYou’ll definitely want to manually check the output and report accuracy statistics in your study. So maybe your review has 1000 papers, you’ll want to manually check 100 of them to see how accurate the LLM was.\n\n\n10.1.1.1.5 You’ll still need to read a lot of papers to write a good lit review\nA lit review is more than the systematic data. I still believe you need to read a lot of papers in order to understand the literature and make a useful synthesis. If you just use AI you’re vulnerable to the ‘illusion of understanding’.\n\n\n\n\n10.1.2 Conclusion\nThis tool will be best for well defined tasks and consistently written papers. For instance, an ideal use case would be reviewing 500 ocean acidification papers that all used similar experimental designs and terminology. You’ll then be able to get consistent answers to prompts about sample size etc…\nAnother good use case would be to extract model types from species distribution model papers.\nHarder tasks will be where the papers are from diverse disciplines, or use inconsistent terminology, or methods. My study was a good example of that, there were about 5 different sample sizes reported. So in this example we’d need first to think clearly about what sample size you wanted to extract before writing the prompt.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Research applications of LLMs</span>"
    ]
  },
  {
    "objectID": "10-research-applications-LLMs.html#sec-websearch",
    "href": "10-research-applications-LLMs.html#sec-websearch",
    "title": "10  Research applications of LLMs",
    "section": "10.2 Deep research",
    "text": "10.2 Deep research\nAs of writing ellmer gives you incomplete input parameters and responses if you are using Open Router’s API. One gap is that it doesn’t properly handle requests and responses to web search enabled LLMs. That’s a shame, because Open Router lets you access and experiment with a broad range of web search LLMs.\nSo what I’ve done here is written my own R functions that make requests to Open Router and properly handle the request and response. First, you’ll need to get my functions off of github: https://github.com/cbrown5/web-search-ai. Then you can try the web search models.\nMy functions make the call to Open Router (you will need to have saved your API key as an environment variable). They then return save the results as qmd, Rmd or markdown. What happens with a web search request is that you get back the response (that ellmer seems to handle ok), but you also get back ‘annotations’ which are the URLs, and the reasoning (if using a reasoning model). ellmer didn’t give you annotations or reasoning at time of writing.\nAnnotations are obviously essential, because you need to know the sources. The reasoning is also good to read because it explains how the LLM decided to do the web search.\nWord of warning, these web searches use a lot of tokens, because the model will be reasoning through a lot of information.\n\nlibrary(httr)\nlibrary(jsonlite)\n\nsource(\"perplexity-search-functions.R\")\n\nopenrouter_api_key &lt;- Sys.getenv(\"OPENROUTER_API_KEY\")\n\n# Example of standard web search query\n\nuser_message &lt;- \"What types of biases occur in fisheries stock models?\"\n\nsystem_message &lt;- \"You are a helpful AI assistant. \n        Rules: \n        1 Include the DOI in your report of any paper you reference.   \n        2. Produce reports that are less than 10000 words.\"\n\nresponse &lt;- call_openrouter_api(\n  openrouter_api_key,\n  model = \"perplexity/sonar-deep-research\",\n  system_message = system_message,\n  user_message,\n  search_context_size = \"medium\"\n  #Options \"low\"  \"medium\", \"high\"\n)\n\n# Example usage:\nsave_response_as_qmd(response, \"results/results.qmd\")\n\nNow you can open that qmd file (or just call it .md for markdown) and read it, or render it to get an html/pdf/word doc.\nAnother warning, the save_response_as_qmd makes the links clickable. Be careful and inspect before clicking, these are links that are provided by an LLM, you can’t vouch for their safety.\n\n10.2.1 Making an R tutorial\nYou can make really nice R tutorials using web search LLMs, like:\nuser_message &lt;- \"How can I relate multiple ecological response variables for benthic cover to an environmental gradient in the R program?\"\n\nsystem_message &lt;- \"You are a helpful AI agent who creates statistical analysis tutorials in R. \n        Rules: \n        1. Include text and examples of code in your responses. \n        2. Produce reports that are less than 10000 words.\"\n\n\n\n\n10.2.2 Other search models\nOpen Router lets you append :online to the end of the model string as a shortcut to getting a websearch. Here are some other options at time of writing:\nmodel = \"perplexity/sonar\" for quick and cheap searches.\nmodel = \"gpt-5:online\" to use one of OpenAI’s search enabled models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Research applications of LLMs</span>"
    ]
  },
  {
    "objectID": "11-writing-documents.html",
    "href": "11-writing-documents.html",
    "title": "11  Writing documents with Quarto and AI assistants",
    "section": "",
    "text": "You can use quarto to write scientific papers. There are several reasons to use quarto rather than a word processor:\n\nEasier to manage document style\nEasier to manage references\nWorkflows that auto-update figures/tables when R code is re-run\nGenerative AI integration that is customizable.\n\nPoint 3 is great, no more cut and pasting figures into word documents!\nPoint 4 is the big one. For instance, I’m developing my own ‘writing mentor’ prompts for large language models. Using quarto lets me implement writing advice specific to science direclty into my manuscripts.\nQuarto is ‘What You See is What you Make’, meaning that you write special syntax for formatting. Once you are used to it, this is way easier way to manage styles than word.\nAnother option is Rmarkdown, which is very similar to quarto, just with fewer features.\nThe downside is getting your (non-coding) collaborators to edit files in quarto. This is the biggest bottleneck to my use of quarto/markdown. Currently I send them word documents then have to manually integrate the feedback. Or I work in quarto until the near final stages, accepting comments only, then get them to edit the final manuscript.\nFor instance, I wrote most of this paper in markdown but had to go to word editing towards the end so I could get edits from my collaborator (this was all before AI assistants). Once you’ve progressed it in word, its hard to go back to markdown.\nInstructions below are high level. There are quite a few pieces of software you need to do this, so I’ve linked to tutorials for each below.\n\n11.0.1 1. Download and install an IDE\nDownload and install VScode as per Section 3.5.\nI’m using VScode because of its AI assistant integration. But you could also use positron if you have issues with VScode or want to use a Posit product rather than a Microsoft product.\n\n\n11.0.2 2. Get git and github\nInstall git on your computer. Optionally, get a github account and connect to that. Git does version control. Github lets you share that online. If your collaborators are github users then you can also share edits on documents this way.\nGit is also essential if you are using AI assistants. Sometimes they majorly stuff up your documents, such as over-writing content you want to keep. So keeping back-ups with git is essential.\n\n\n11.0.3 3. VScode extensions\nInstall these VScode extentions (or equivalents if you are using positron, note that many vscode extensions are also compatable with Positron)\n\nQuarto extension.\n\nOpen VSCode and click the four boxes ‘extension’ icon on the LHS then search and install the Quarto extension.\n\n\n11.0.4 4. Steps for AI integration\nQuarto in VScode is great on its own. But if you want to use AI assistants there’s a few more steps.\nIf you are using quarto or markdown its possible to get large-language models to help with many paper writing tasks (including the writing). This is a specialized area though and I’ve only given basic technical instructions here. Actually getting it to work well is another topic altogether and something I’m still developing…\nGet an API key with an LLM provider see Section 3.4.\nGet an AI assistant extension for your IDE. Here I’ll assume you are using the Roo Code extension for VSCode Section 3.9.\nThe reason I like Roocode for this is the ability to to create custom modes, so you can create a mode specific to scientific writing.\nRead the documents/watch the tutorials and learn how to use Roo Code\nYou can now create a custom mode, e.g. a ‘scientific writing mode’ in Roo code. As of writing this requires clicking the mode selection button at the bottom of the Roo Code Pane, then click the Cog, then the + button to make a new mode. Then you need to write a ‘Role Definition’ and ‘Custom instructions’. For tools I just use ‘Read Files’, ‘Edit Files’ and unclick the others (will save you money and tokens).\nThis is the hard part that needs a lot of thought:\nIn the custom instructions you should write detailed instructions on how to help an author with scientific writing. For instance, you might want to put some very strong instructions about not making up references. You might also put instructions about your particular writing style preferences. I’m working on a template, but am not yet ready to share it.\nSee Roo code documentation for more advice on custom modes.\nYou can also create custom modes in VSCode. Open the chat window and click the cog to access the menu for creating custom modes.\n\n\n11.0.5 5. Using quarto\nTake a tutorial and learn how to use Quarto.\nFor academic paper writing the key things to understand from the Quarto tutorial are:\n\nHow to knit as word or pdf (pdf requires extra software installations)\nFormatting, headings, bold etc…\nYAML frontmatter for styles, linking a bibliography and bibliography style\nHow to insert images and/or code.\n\nNote on AI integration once you are using quarto and Roo Code you can simply ask Roo Code to do things in your document (like outline a paper template) by referencing the file (e.g. (myfile.qmd?)) in the prompt box.\nWhether this works well for you is another questions. Prompting well requires a lot of thought and practice. Its not simply going to write a paper for you. You have to give the AI assistant detailed, specific, instructions and lots of context.\nNote in VSCode as of writing you need to set your quarto window to ‘source’ mode. The AI assistants generally won’t work if its set to ‘visual’ mode. I recommend memorising the short-cut key to easily flick between these two modes (cmd-shft-F4 on my computer).\n\n\n11.0.6 6. YAML front matter\nThe YAML controls how your qmd document is rendered. Here’s an example of mine:\n---\ntitle: \"The paper's title\"\nformat: docx\neditor: visual\nbibliography: mybib.bib\ncsl: myjournal.csl\nexecute: \n  echo: false\n  message: false\n  warning: false\n---\nThis goes at the top of your document. A few key points.\nformat controls document type to render this as, here a word doc.\neditor controls how it is viewed in vscode. Options are editor: visual and editor: source. Visual looks more like a word doc, source looks more like markdown. You’ll have to save and re-open the document for this to change.\nbibliography links to a bibtex file where your references are stored.\ncsl links to a style guide for the bibliography.\nMore on styles and references below.\nexecute is controlling how R code is run and if the R code appears in the document.\n\n\n11.0.7 7. Rendering as a document\nUse the short-cut key ‘cmd-shift-K’/‘cntrl-shft-k’ (mac/windows) to preview your document. It will also create a rendered version in your current directory.\nIts helpful to set: format: html when you are writing the document, then you get a live preview in vscode. Use format: docx when you want a word document.\nIts worth also learning the short-cut `cmd-shft-p’/‘cntrl-shft-p’, this brings up searchable actions for all extensions in vscode. The one you want is ‘Quarto: preview’ which does the same as the shortcut above.\nI tend to have minimal R code in my quarto manuscript. Or none at all (just reference .png files for figures). This keeps rendering quick. Also your document can get unweildy if there is a lot of text mixed in with R code.\n\n\n11.0.8 8. Word counts\nThere are various word count extensions for vscode qmd and md documents.\n\n\n11.0.9 9. Document styles\nGetting a word document to follow a particular style is a bit fiddly. You need to set-up a template word document with styles the include that as a reference in your YAML.\nSee instructions here.\n\n\n11.0.10 10. Reference manager integration\nQuarto integrates with many different reference managers. There’s a good guide here.\nIn brief you create a .bib file that has your references in it. This is then linked in the YAML. The manual way to manage this is just to create a .bib file and paste bibtext entries directly into it (available on most journal’s pages as a citation format, as well as google scholar).\ne.g. the bibtext for R looks like this:\n@Manual{Rlanguage,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2024},\n    url = {https://www.R-project.org/},\n  }\nThen in quarto you just type @ and a dropdown of all your references will appear. @Manual{Rlanguage, the Rlanguage bit is the CiteKey that will appear in the dropdown. So @Rlanguage will insert that reference into the bibliography and the citation at that place in the document.\nYou can streamline the process of gathering and managing references with a reference manager.\nMy workflow in Zotero is as follows:\n\nOpen Zotero on my computer\nGo to journal webpage for paper\nUse zotero plugin to my browser to grab the citation and save it to a library\nGo to my quarto document in VScode\ntype @ and a drop down of all references in all libraries on zotero appears. Pick the one I want.\nClick the OK button which saves that reference into my local .bib file.\n\nFor some reason (that does not seem to be documented in any quarto tutorials anywhere!) it will find any reference I have anywhere in zotero and then save that bibtex entry to my local .bib file, so it is now accessible for use in my quarto doc. This only works if I have zotero open and use editor: visual in the YAML.\nThere are many other options however.\n\n\n11.0.11 11. Optional AI integration for reference management\nYou can get AI assistants to help with referencing if you keep your notes on papers linked to your references. For instance, you could keep your notes on references in the bibtex field for notes. Alternatively you could create another quarto/markdown document that has a header for each citation tag along with its notes in a structured way:\n## Rlanguage \n\n### What it is\n\nThe R software for scientific computing. \n\n### Usage\n\nCitation for the R software. Use this at least once in every paper where i've used R for statistics\n\n## edgar2023continent\n\n### What is it\n\nKey paper that shows Australia is losing its marine biodiversity. \n\n### Usage\n\nCite this as evidence that Australia is losing coastal marine biodiversity and as evidence that climate change is causing marine biodiversity loss\nIt doesn’t matter how you do this, so long as you follow a consistent structure. I’ve used the CiteKey as the main header for each reference entry. Then I’ve put in markdown sections about each paper and why I might wnat to cite it. Then you can get Roo Code to help with inserting references.\nNote that if you are using the .bib directly just be careful not to plagiarise! Roo Code might insert excerpts from the abstracts/titles directly into your written document, which is a no-no for publishing.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Writing documents with Quarto and AI assistants</span>"
    ]
  },
  {
    "objectID": "12-cost-security.html",
    "href": "12-cost-security.html",
    "title": "12  Cost and security",
    "section": "",
    "text": "12.1 Cost considerations\nAI companies are running at a loss and its quite likely that costs will go up in future. The aim right now is to get us all dependent on the technology, so that we have to keep paying in future (another reason I think its improtant our own countries develop these capaibilites, and that we also need to strive to be capable to work in AI free ways as well. )",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cost and security</span>"
    ]
  },
  {
    "objectID": "12-cost-security.html#cost-considerations",
    "href": "12-cost-security.html#cost-considerations",
    "title": "12  Cost and security",
    "section": "",
    "text": "Managers and lab heads need to consider cost and impact on research budget\ne.g. Copilot subscription free for students\nTools like Roo Code can be more expensive (pay per use as using API).\nStill, many of the tasks they can do are less than a hiring a person to do that (though they definitely need expert human oversight)\ne.g. I estimated that processing 6000 abstracts to extract data for a lit review might cost about USD300 (including cost of developing prompts)\nThere are strategies you can use for optimizing token usage if you really want to optimize costs\nYou will need to balance cost with capabilities. Cheaper models are often less proficent.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cost and security</span>"
    ]
  },
  {
    "objectID": "12-cost-security.html#api-security",
    "href": "12-cost-security.html#api-security",
    "title": "12  Cost and security",
    "section": "12.2 API security",
    "text": "12.2 API security\n\nManaging API keys and credentials\nSanitizing inputs to remove sensitive information\nLocal vs. cloud-based LLM solutions\nAuditing and monitoring LLM interactions",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cost and security</span>"
    ]
  },
  {
    "objectID": "12-cost-security.html#agent-security",
    "href": "12-cost-security.html#agent-security",
    "title": "12  Cost and security",
    "section": "12.3 Agent security",
    "text": "12.3 Agent security\n\nCan run code on your computer\nBe careful what it is doing\nRead prompts before running them\n\nNote that malicious people can hide text in webpages and pdfs and other content you might be uploading as context, e.g. using white font or tiny font. The LLM will still see this text and may act on it.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cost and security</span>"
    ]
  },
  {
    "objectID": "12-cost-security.html#lethal-trifecta-for-prompt-injection-attacks",
    "href": "12-cost-security.html#lethal-trifecta-for-prompt-injection-attacks",
    "title": "12  Cost and security",
    "section": "12.4 Lethal trifecta for prompt injection attacks",
    "text": "12.4 Lethal trifecta for prompt injection attacks\nNever allow the agent to do all these three things at the same time:\n\nRead unverified material from the web (even if its material you downloaded earlier)\nHave access to sensitive data (which includes your personal data, API keys and your research data)\nUpload information to the web (which includes creating webpages or pushing commits to github)\n\nThe Lethal Trifecta for prompt injection attacks (coined by Simon Williamson) is access to private data, ability to communicate externally and exposure to untrusted content.\nWhat can happen is that if your agent can read untrusted sources, those sources may contain malicious prompts. These prompts could convince the agent to do things like send or post your personal data to the hacker or create malicious code that runs on your computer.\nYou want to be sure your agents can’t do these three things at once. Remember sensitive data includes your name, username, phone number, email, API keys as well as sensitive research data.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cost and security</span>"
    ]
  },
  {
    "objectID": "13-ethics-copyright.html",
    "href": "13-ethics-copyright.html",
    "title": "13  Ethics and copyright",
    "section": "",
    "text": "13.1 Who’s responsible for mistakes?\nYou are. You own the content you create with generative AI and you are also responsible for it.\n‘Vibe coding’ - coding without looking at the code can be useful. Its great for prototyping project ideas or make simple low risk applications. Its less useful in serious science applications (like research you might publish) because its hard to verify whether its accurate. Ultimately you will be accountable for flawed research you create with an AI Agent.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ethics and copyright</span>"
    ]
  },
  {
    "objectID": "13-ethics-copyright.html#impacts-on-learning",
    "href": "13-ethics-copyright.html#impacts-on-learning",
    "title": "13  Ethics and copyright",
    "section": "13.2 Impacts on learning",
    "text": "13.2 Impacts on learning\nDoes AI make our brains lazy? One study found less engagement and deep thinking for students who had access to chatGPT for writing an essay compared to students who just had web searches or had no internet connectivity.\nI think the upshot is using it deliberatley and being careful not to replace your own creativity.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ethics and copyright</span>"
    ]
  },
  {
    "objectID": "13-ethics-copyright.html#sustainability",
    "href": "13-ethics-copyright.html#sustainability",
    "title": "13  Ethics and copyright",
    "section": "13.3 Sustainability",
    "text": "13.3 Sustainability\nTraining LLMs costs millions of dollars, much of this cost is energy use. Further, the data centres for training and running LLMs need water for cooling. Asking a finished LLM questions uses much less energy, but cumulatively across the globe it adds up to a lot. Here are a few informative statistics I found online:\nFrom Forbes:\n\n“ChatGPT’s daily power usage is nearly equal to 180,000 U.S. households, each using about twenty-nine kilowatts.”\nMicrosoft emissions have risen 30% since 2020 due to data centers\nAI prompts use 10x more energy than a traditional google search\n\nTo put it in context I did some calculations on my personal usage. I estimate the prompting I do through copilot each year will cost about 2.32 kg of C02 and about 1000 litres of water. (this is lower bound, as I also using LLMs for other tasks).\nTo put that in context, flying the 1.5 hours from Halifax to Montreal is about 172kg of emissions, driving 15 minutes is about 3 kg. So I’m using approximately 10 less than a short flight, or the same as driving to work once. 1000L is equivalent to taking about 22 5-minute showers.\nOf course, the carbon cost is global, whereas the water cost is localised (Probably to US data centres, so by using this resource I’m really just making the water problem worse for Americans. )\nSo its not a huge increase in my personal energy use. But cumulatively across the globe it is a lot.\nMore generally, humanities energy use is growing exponential. Despite renewables and so on, ultimately our planet won’t be able to sustain this energy drawdown. LLMs are part of that trend of growing energy use. At some point we need to start using less energy, or the biosphere will become depleted and return to a ‘moon like rock’ in one study’s words.\nHere’s my personal belief.\nIf we’re smart humanity will use this technology to find ways to make our use of the planet more sustainable and ultimately save water and energy. Just like we should have been using fossil fuels to develop a transition to lasting sustainanle energy use. So you can guess how likely that is to happen…\nIts the reason I’m teaching this course. I don’t personally think that LLMs make our lives better, or humanity more sustainable. They just raise the bar on the rate of progress.\nYou can bet industries are using this technology to improve their productivity (= greater environmental impacts). I believe as environmental scientists we need to try to keep up. Ultimately we need progress on local to planetary sustainability (environmental scientists) to outpace the development of the industries that are environmentally unsustainable.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ethics and copyright</span>"
    ]
  },
  {
    "objectID": "13-ethics-copyright.html#model-biases",
    "href": "13-ethics-copyright.html#model-biases",
    "title": "13  Ethics and copyright",
    "section": "13.4 Model biases",
    "text": "13.4 Model biases\nThis is a big one. I recommend everyone read this perspective on the ‘Illusion of Understanding’\nIts important that we don’t become too reliant on AI for our work. That’s why I’m teaching and promoting thoughtful use.\nSome key points:\n\nWe need to maintain and grow research fields that aren’t convenient to do with AI, not just grow the stuff that’s easy with AI\nWe need to push ourselves as individuals to not ‘be lazy’ and rely on AI too much. There is still great value in human learning. This requires mental energy, for instance, you will know something better if you write it yourself rather than write it with AI.\nWe need to be aware of biases in the content AI generates\n\nFor statistics these biases are likely to be a preference for well-known methods developed by Western science. So you should still read the literature broadly and avoid using AI, or prompt it in different ways, if you truly want to create novel statitistics (as opposed to using it to do statistics on a study that is otherwise novel data etc…)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ethics and copyright</span>"
    ]
  },
  {
    "objectID": "13-ethics-copyright.html#rising-inequality",
    "href": "13-ethics-copyright.html#rising-inequality",
    "title": "13  Ethics and copyright",
    "section": "13.5 Rising inequality",
    "text": "13.5 Rising inequality\nAI development is currently concentrated in the USA and profits for LLM use go to American companies. (USA is itself a country with massive inequality issues!). So the extent to LLMs replaces labour will redirect income and taxes from jobs in countries to American companies.\nIt is likely that the current low cost of LLM use will not continue. Companies are running at a loss in order to gain market share. So be careful how dependent you become on the LLMs and what that budget is replacing in your research budgets.\nI personally beleive that our own countries should be developing our own LLM products and resources. Even if they are not ‘industry leading’ they can still be highly effective for specific tasks. There are open-source models available that can fill this role.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ethics and copyright</span>"
    ]
  },
  {
    "objectID": "13-ethics-copyright.html#copyright",
    "href": "13-ethics-copyright.html#copyright",
    "title": "13  Ethics and copyright",
    "section": "13.6 Copyright",
    "text": "13.6 Copyright\nMany LLMs have been trained on pirated books. The extent to which this is recognized by law is still in court.\nFor me personally its frustrating that I spent years developing a statistics blog (which was open-access, but I appreciated attribution), but now that information has been mined by LLMs. Thus AI companies are profiting from our collective knowledge.\nIt is an even worse situation for authors who’s livelihoods and careers depend on their copyrighted works.\nCopilot does in theory block itself from writing code that might be copyrighted. However, the efficacy of this system is unclear (it seems to just be a command in the system prompt). So be careful. Here are some recommendations for individuals\n\nIn general you own works you create with an LLM.\nThis also means you have the liability for any works you create (not normally an issue in environmental sciences).\ne.g. you couldn’t blame the LLM if you had to retract a paper due to incorrect statistics.\nYou should acknolwedge LLM use in academic publications, and what you used it for.\nAlways look for original sources references, e.g. don’t ‘cite’ the LLM for use of a GLM, use a textbook or reputable source (Zuur’s books are good for this!)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ethics and copyright</span>"
    ]
  },
  {
    "objectID": "13-ethics-copyright.html#managing-data-privacy",
    "href": "13-ethics-copyright.html#managing-data-privacy",
    "title": "13  Ethics and copyright",
    "section": "13.7 Managing data privacy",
    "text": "13.7 Managing data privacy\nAny prompt you send to an LLM provider is going to the server of an AI company (e.g. Google). So its important to be mindful of what information you are including in your prompts.\nThe data you send (including text data) will be covered by the privacy policy of the LLM provider. Some services claim to keep your data private (e.g. the Copilot subscription my University has). Public services will tend to retain the right to use any data you enter as prompts.\nThis means if you put your best research ideas into chatGPT, its possible that it will repeat them later to another user who asks similar questions. So be mindful of what you are writing.\nBefore using an LLM to help with data analysis, be sure you understand the IP and ethical considerations involved with that data. For instance, if you have human survey data you may not be allowed to send that to a foreign server, or reveal any information to an LLM.\nIn that case you have three options.\n\n13.7.0.1 Option 1: Locally hosted LLM\nUse a locally hosted LLM. We won’t cover setting these up in this workshop. Locally hosted LLMs run on your computer. They can be suitable for simpler tasks and if you have a reasonably powerful GPU. Downsides are they do not have the performance of the industry leading LLMs and response times can be slower.\n\n\n13.7.0.2 Option 2: Keep data seperate from code development.\nUse the LLM to help generate code to analyse the data, but do not give the LLM the data or the results. I would recommend keeping the data in a different directory altogether (ie not your project directory), so that LLM agents don’t inadvertently access the raw data. You also want to be sure that the LLM isn’t returning results of data analysis to itself (and therefore you reveal private information to the LLM).\nIt can be helpful to generate some simulated data to use for code development, so there is no risk of violating privacy.\n\n\n13.7.0.3 Option 3: Ignore sensitive folders\nSome LLM agents can be directed to ignore specific folders. e.g. You could add a command to ignore a folder to copilot custom instructions, Roo Code has a .rooignore file for this.\nHowever, remember prompts are not 100% precise (unlike real code), so there’s still the chance the LLM will go in those folders. So be careful, if its really sensitive keep it elsewhere on your computer, and always check its actions before you approve them.\n\n\n13.7.0.4 Option 4: Service with no data retention policy\nOpenrouter has a ‘no data retention’ option you can click in the profile, it will filter providers to those with NDR rules.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ethics and copyright</span>"
    ]
  },
  {
    "objectID": "13-ethics-copyright.html#supplement-calculations-of-personal-environmental-impact-from-using-llms",
    "href": "13-ethics-copyright.html#supplement-calculations-of-personal-environmental-impact-from-using-llms",
    "title": "13  Ethics and copyright",
    "section": "13.8 Supplement: Calculations of personal environmental impact from using LLMs",
    "text": "13.8 Supplement: Calculations of personal environmental impact from using LLMs\nA ChatGPT request uses 2.9 watt-hour. So say that’s similar cost for coding applicatoins (probably more due to the additional context we are loading with every prompt). Then looking at my chat history I had 14 conversations in the last week (not counting in-line editing). Average was 3x requests per conversation, so in a year that equals: 2.9 * 14 * 3 * 52 = 6.33 kW-hours In USA energy cost on Average is 367 grams C02 per kW-hour. (https://www.eia.gov/tools/faqs/faq.php?id=74&t=11) So my conservative estimated yearly usage for coding: 6.33 x 367 = 2.32 kg C02 For comparison flying the 1.5 hours from Halifax to Montreal is about 172kg of emissions. So my personal annual emissions for coding are perhaps about 10x than a short plane flight. Water is used for cooling in data centres: “A single ChatGPT conversation uses about fifty centilitres of water, equivalent to one plastic bottle.” Based on calculations above, this equates to about 1000L per year. That’s equivalent to about 22 x 5-minute showers.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ethics and copyright</span>"
    ]
  },
  {
    "objectID": "14-appendix.html",
    "href": "14-appendix.html",
    "title": "14  Code for fish and benthic analysis",
    "section": "",
    "text": "14.1 Read in the data from an online source\nlibrary(readr)\n\n# URLs for datasets\nbenthic_cover_url &lt;- \"https://raw.githubusercontent.com/cbrown5/example-ecological-data/refs/heads/main/data/benthic-reefs-and-fish/benthic_cover.csv\"\nbenthic_variables_url &lt;- \"https://raw.githubusercontent.com/cbrown5/example-ecological-data/refs/heads/main/data/benthic-reefs-and-fish/benthic_variables.csv\"\nfish_coral_cover_sites_url &lt;- \"https://raw.githubusercontent.com/cbrown5/example-ecological-data/refs/heads/main/data/benthic-reefs-and-fish/fish-coral-cover-sites.csv\"\n\n# Local file paths\nbenthic_cover_path &lt;- \"data/benthic_cover.csv\"\nbenthic_variables_path &lt;- \"data/benthic_variables.csv\"\nfish_coral_cover_sites_path &lt;- \"data/fish-coral-cover-sites.csv\"\n\n# Download and save datasets\nbenthic_cover &lt;- read_csv(benthic_cover_url)\nwrite_csv(benthic_cover, benthic_cover_path)\n\nbenthic_variables &lt;- read_csv(benthic_variables_url)\nwrite_csv(benthic_variables, benthic_variables_path)\n\nfish_coral_cover_sites &lt;- read_csv(fish_coral_cover_sites_url)\nwrite_csv(fish_coral_cover_sites, fish_coral_cover_sites_path)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Code for fish and benthic analysis</span>"
    ]
  },
  {
    "objectID": "14-appendix.html#convert-the-benthic-data-to-a-wide-format-data-frame",
    "href": "14-appendix.html#convert-the-benthic-data-to-a-wide-format-data-frame",
    "title": "14  Code for fish and benthic analysis",
    "section": "14.2 Convert the benthic data to a wide format data frame",
    "text": "14.2 Convert the benthic data to a wide format data frame\n\n# Load required libraries\nlibrary(tidyverse)\n\n# Load benthic cover data\nbenthic &lt;- read_csv(\"data/benthic_cover.csv\")\n\n# Calculate proportional cover per transect\nbenthic &lt;- benthic %&gt;%\n    mutate(prop_cover = cover / n.pts)\n\n# Average proportional cover by site and habitat type\nbenthic_site &lt;- benthic %&gt;%\n    group_by(site, code) %&gt;%\n    summarise(mean_prop_cover = mean(prop_cover, na.rm = TRUE), .groups = \"drop\")\n\n# Pivot to wide format: one row per site, columns for each habitat type\nbenthic_wide &lt;- benthic_site %&gt;%\n    pivot_wider(names_from = code, values_from = mean_prop_cover)\n\n# View result\nprint(head(benthic_wide))\nreadr::write_csv(benthic_wide, file = \"data/benthic_wide.csv\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Code for fish and benthic analysis</span>"
    ]
  },
  {
    "objectID": "14-appendix.html#join-the-benthic-and-fish-datasets",
    "href": "14-appendix.html#join-the-benthic-and-fish-datasets",
    "title": "14  Code for fish and benthic analysis",
    "section": "14.3 Join the benthic and fish datasets",
    "text": "14.3 Join the benthic and fish datasets\n\nlibrary(tidyverse)\nlibrary(readr)\nfish &lt;- read_csv(\"data/fish-coral-cover-sites.csv\")\nbenthic_wide &lt;- read_csv(\"data/benthic_wide.csv\")\n\ndat &lt;- left_join(fish, benthic_wide, by = \"site\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Code for fish and benthic analysis</span>"
    ]
  }
]